{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9Gc3WmO+93pxIxJljPqQV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hp7dX5oRfZKR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755589388441,"user_tz":-120,"elapsed":24688,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"3dc19a74-5e28-42a6-d4db-afebaae4b3a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#步骤 1：将所有 CSV 文件合并成一个文件\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","source":["#En Chinois"],"metadata":{"id":"N3UgZd1tHmyZ"}},{"cell_type":"markdown","source":["#méthode 1 avec stanza"],"metadata":{"id":"Gp4FiOlG9CvT"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","from pathlib import Path\n","import pandas as pd\n","from collections import Counter\n","\n","# ========= 配置文件路径 =========\n","path_refs = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt\")\n","path_corpus = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch_all_data_2015_2025_tok_stanza.csv\")\n","\n","# 输出目录与文件\n","out_dir = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification\")\n","out_file = out_dir / \"ch_néologisme_data.csv\"\n","\n","# 确保输出目录存在\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========= 读取参考词表 =========\n","ref_terms = []\n","with path_refs.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = \" \".join(line.strip().split())\n","        if t:\n","            ref_terms.append(t)\n","\n","seen = set()\n","ref_terms = [t for t in ref_terms if not (t in seen or seen.add(t))]\n","\n","def ntoks(s: str) -> int:\n","    return len(s.split())\n","\n","max_len = max(ntoks(t) for t in ref_terms) if ref_terms else 1\n","\n","# ========= 读取语料 =========\n","df = pd.read_csv(path_corpus, encoding=\"utf-8\")\n","if \"content\" not in df.columns:\n","    raise ValueError(\"CSV 中没有 'content' 列。\")\n","\n","contents = df[\"content\"].fillna(\"\").astype(str).tolist()\n","\n","# ========= 构建 n-gram 频数字典 =========\n","ngram_counter = Counter()\n","for text in contents:\n","    toks = text.split()\n","    n_tokens = len(toks)\n","    if n_tokens == 0:\n","        continue\n","    up_to = min(max_len, n_tokens)\n","    for n in range(1, up_to + 1):\n","        for i in range(0, n_tokens - n + 1):\n","            ngram = \" \".join(toks[i:i+n])\n","            ngram_counter[ngram] += 1\n","\n","# ========= 统计结果 =========\n","rows = []\n","for term in ref_terms:\n","    freq = ngram_counter.get(term, 0)\n","    rows.append({\n","        \"term\": term,\n","        \"n_tokens\": ntoks(term),\n","        \"frequency\": int(freq),\n","        \"found\": bool(freq > 0),\n","    })\n","\n","result = pd.DataFrame(rows).sort_values(\n","    by=[\"found\", \"frequency\", \"n_tokens\", \"term\"],\n","    ascending=[False, False, True, True]\n",").reset_index(drop=True)\n","\n","# ========= 保存结果 =========\n","result.to_csv(out_file, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"结果已保存到：{out_file}\")\n","print(f\"总参考项数：{len(ref_terms)}\")\n","print(f\"新词数量：{len(result[result['found'] == False])}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dXgU6Hn_qKA","executionInfo":{"status":"ok","timestamp":1755199388832,"user_tz":-120,"elapsed":3848,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"4572ada2-9236-4ed0-9734-af7ff9c9ad9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["结果已保存到：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch_néologisme_data.csv\n","总参考项数：126358\n","新词数量：111120\n"]}]},{"cell_type":"markdown","source":["#méthode 2 avec stanza"],"metadata":{"id":"tgrP7Lcm9GAH"}},{"cell_type":"code","source":["#添加对应的年份\n","\n","\n","# -*- coding: utf-8 -*-\n","from pathlib import Path\n","import pandas as pd\n","from collections import Counter\n","\n","# ========= 配置文件路径 =========\n","path_refs = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt\")\n","path_corpus = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch_all_data_2015_2025_tok_stanza.csv\")\n","\n","# 输出目录与文件\n","out_dir = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification\")\n","out_file = out_dir / \"ch_néologisme_data_years.csv\"\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========= 读取参考词表 =========\n","ref_terms = []\n","with path_refs.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = \" \".join(line.strip().split())\n","        if t:\n","            ref_terms.append(t)\n","\n","# 去重保序\n","seen = set()\n","ref_terms = [t for t in ref_terms if not (t in seen or seen.add(t))]\n","\n","def ntoks(s: str) -> int:\n","    return len(s.split())\n","\n","max_len = max(ntoks(t) for t in ref_terms) if ref_terms else 1\n","\n","# ========= 读取语料 =========\n","df = pd.read_csv(path_corpus, encoding=\"utf-8\")\n","if not {\"content\", \"date\"}.issubset(df.columns):\n","    raise ValueError(\"CSV 中必须有 'content' 和 'date' 列。\")\n","\n","df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n","\n","# 如果日期是 YYYY-MM-DD 格式，提取年份\n","df[\"year\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.year\n","\n","# ========= 构建 n-gram 频数字典 =========\n","ngram_counter = Counter()\n","# 保存首次出现年份\n","first_year_dict = {}\n","\n","for idx, row in df.iterrows():\n","    toks = row[\"content\"].split()\n","    year = row[\"year\"]\n","    n_tokens = len(toks)\n","    if n_tokens == 0:\n","        continue\n","    up_to = min(max_len, n_tokens)\n","    for n in range(1, up_to + 1):\n","        for i in range(0, n_tokens - n + 1):\n","            ngram = \" \".join(toks[i:i+n])\n","            ngram_counter[ngram] += 1\n","            # 记录首次出现年份\n","            if ngram not in first_year_dict and pd.notnull(year):\n","                first_year_dict[ngram] = int(year)\n","\n","# ========= 统计结果 =========\n","rows = []\n","for term in ref_terms:\n","    freq = ngram_counter.get(term, 0)\n","    first_year = first_year_dict.get(term, \"\")\n","    rows.append({\n","        \"term\": term,\n","        \"n_tokens\": ntoks(term),\n","        \"frequency\": int(freq),\n","        \"found\": bool(freq > 0),\n","        \"first_year\": first_year\n","    })\n","\n","result = pd.DataFrame(rows).sort_values(\n","    by=[\"found\", \"frequency\", \"n_tokens\", \"term\"],\n","    ascending=[False, False, True, True]\n",").reset_index(drop=True)\n","\n","# ========= 保存 =========\n","result.to_csv(out_file, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"结果已保存到：{out_file}\")\n","print(f\"总参考项数：{len(ref_terms)}\")\n","print(f\"新词数量：{len(result[result['found'] == False])}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKRr7C1oCutq","executionInfo":{"status":"ok","timestamp":1755199527057,"user_tz":-120,"elapsed":2056,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"f9f0d28a-924c-478d-b9c4-3a98b054bfad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-568053732.py:43: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df[\"year\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.year\n"]},{"output_type":"stream","name":"stdout","text":["结果已保存到：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch_néologisme_data_years.csv\n","总参考项数：126358\n","新词数量：111120\n"]}]},{"cell_type":"markdown","source":["#méthode 3 avec stanza\n","\n","---\n","\n"],"metadata":{"id":"lHCw-_Uz9IzI"}},{"cell_type":"code","source":["#参考词汇不在语料库\n","\n","\n","# -*- coding: utf-8 -*-\n","from pathlib import Path\n","import pandas as pd\n","from collections import Counter\n","\n","# ========== 配置路径 ==========\n","# 参考词表（2015 年之前的词典词汇），每行一个词/词组\n","path_refs = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt\")\n","\n","# 新闻语料（2015–2025），列包含 author, title, date, content, url\n","# 其中 content 已经是分词后的空格序列\n","path_corpus = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch_all_data_2015_2025_tok_stanza.csv\")\n","\n","\n","\n","\n","# 输出目录与文件\n","out_dir = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification\")\n","out_all  = out_dir / \"ch_néologisme_data_all.csv\"\n","out_new  = out_dir / \"ch_néologisme_new_only.csv\"\n","out_old  = out_dir / \"ch_néologisme_in_reference.csv\"\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 读取参考词表 ==========\n","# 假设每一行一个词（与语料的 token 一致；若参考表含多词短语且内部带空格，则本脚本按整行当作一个条目处理）\n","ref_terms = []\n","with path_refs.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = line.strip()\n","        if t:\n","            ref_terms.append(t)\n","\n","# 去重保序\n","seen = set()\n","ref_terms = [t for t in ref_terms if not (t in seen or seen.add(t))]\n","ref_set = set(ref_terms)\n","\n","# ========== 读取语料 ==========\n","df = pd.read_csv(path_corpus, encoding=\"utf-8\")\n","\n","required_cols = {\"content\", \"date\"}\n","if not required_cols.issubset(df.columns):\n","    raise ValueError(f\"CSV 缺少必要列：{required_cols - set(df.columns)}\")\n","\n","# 解析日期 -> 年份，并按日期升序排序，确保“首次出现年份”正确\n","df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n","df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n","df[\"year\"] = df[\"date_parsed\"].dt.year\n","\n","# content 规范化\n","df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n","\n","# ========== 统计语料词频 & 首次出现年份 ==========\n","freq = Counter()\n","first_year = {}\n","\n","for _, row in df.iterrows():\n","    year = row[\"year\"]\n","    if pd.isna(year):\n","        # 没有有效日期的样本不参与首次年份统计，但仍可计数（如需也排除计数，可改为 continue）\n","        tokens = row[\"content\"].split()\n","        freq.update(tokens)\n","        continue\n","\n","    tokens = row[\"content\"].split()\n","    freq.update(tokens)\n","    y = int(year)\n","    for tok in tokens:\n","        # 因为 df 已按日期升序排序，第一次赋值即为最早年份\n","        if tok not in first_year:\n","            first_year[tok] = y\n","\n","# ========== 形成总表 ==========\n","rows = []\n","for tok, f in freq.items():\n","    is_new = tok not in ref_set          # 不在参考词表 => 新词\n","    fy = first_year.get(tok, None)       # 语料中的首次出现年份（可能为 None：该词只出现在无日期样本）\n","    rows.append({\n","        \"term\": tok,\n","        \"frequency\": int(f),\n","        \"first_year\": fy,\n","        \"in_reference\": (not is_new),    # True=旧词；False=新词\n","        \"status\": \"new\" if is_new else \"in_reference\"\n","    })\n","\n","result = pd.DataFrame(rows)\n","\n","# 便于观察：优先显示新词、频次高、年份早\n","result = result.sort_values(\n","    by=[\"status\", \"frequency\", \"first_year\", \"term\"],\n","    ascending=[True, False, True, True]\n",").reset_index(drop=True)\n","\n","# ========== 拆分并保存 ==========\n","new_df = result[result[\"status\"] == \"new\"].copy()\n","old_df = result[result[\"status\"] == \"in_reference\"].copy()\n","\n","# 保存 CSV（带 BOM，便于 Excel 打开）\n","result.to_csv(out_all, index=False, encoding=\"utf-8-sig\")\n","new_df.to_csv(out_new, index=False, encoding=\"utf-8-sig\")\n","old_df.to_csv(out_old, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"总词数（语料词表大小）：{len(result)}\")\n","print(f\"新词数（不在参考表）：{len(new_df)}\")\n","print(f\"旧词数（在参考表）：{len(old_df)}\")\n","print(f\"已保存：\\n- {out_all}\\n- {out_new}\\n- {out_old}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_xrGOlNFADU","executionInfo":{"status":"ok","timestamp":1755589984930,"user_tz":-120,"elapsed":1094,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"70871ad0-17b7-4490-999d-eb98e3ffd7b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-845452333.py:49: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n"]},{"output_type":"stream","name":"stdout","text":["总词数（语料词表大小）：32811\n","新词数（不在参考表）：17573\n","旧词数（在参考表）：15238\n","已保存：\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch_néologisme_data_all.csv\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch_néologisme_new_only.csv\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch_néologisme_in_reference.csv\n"]}]},{"cell_type":"markdown","source":["#méthode avec Thulac"],"metadata":{"id":"C8MVDn4f9Pqg"}},{"cell_type":"code","source":["# 去掉标点符号和数字，百分比\n","\n","from pathlib import Path\n","import pandas as pd\n","from collections import Counter\n","import unicodedata\n","import re\n","\n","# ========== 配置路径 ==========\n","path_refs = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt\")\n","#path_corpus = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch_all_data_2015_2025_tok_stanza.csv\")\n","path_corpus = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch2_all_data_2015_2025_tok_thulac.csv\")\n","\n","out_dir = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification\")\n","out_all  = out_dir / \"ch1_néologisme_data_all.csv\"\n","out_new  = out_dir / \"ch1_néologisme_new_only.csv\"\n","out_old  = out_dir / \"ch1_néologisme_in_reference.csv\"\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 读取参考词表 ==========\n","ref_terms = []\n","with path_refs.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = line.strip()\n","        if t:\n","            ref_terms.append(t)\n","\n","seen = set()\n","ref_terms = [t for t in ref_terms if not (t in seen or seen.add(t))]\n","ref_set = set(ref_terms)\n","\n","# ========== 读取语料 ==========\n","df = pd.read_csv(path_corpus, encoding=\"utf-8\")\n","\n","required_cols = {\"content\", \"date\"}\n","if not required_cols.issubset(df.columns):\n","    raise ValueError(f\"CSV 缺少必要列：{required_cols - set(df.columns)}\")\n","\n","df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n","df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n","df[\"year\"] = df[\"date_parsed\"].dt.year\n","df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n","\n","# ========== 过滤函数 ==========\n","# 标点过滤\n","def is_punctuation(token):\n","    return all(unicodedata.category(ch).startswith(\"P\") or ch.isspace() for ch in token)\n","\n","# 数字过滤（阿拉伯数字、中文数字大写/小写）\n","num_pattern = re.compile(r\"^[0-9一二三四五六七八九十百千万亿〇零壹贰叁肆伍陆柒捌玖拾佰仟]+$\")\n","def is_number(token):\n","    return bool(num_pattern.match(token))\n","\n","# 百分比过滤（包含阿拉伯数字+% 或 中文“百分之”结构）\n","percent_pattern = re.compile(r\"^([0-9]+%|百分之[一二三四五六七八九十百千万亿〇零壹贰叁肆伍陆柒捌玖拾佰仟]+)$\")\n","def is_percentage(token):\n","    return bool(percent_pattern.match(token))\n","\n","# ========== 统计词频 & 首次出现年份 ==========\n","freq = Counter()\n","first_year = {}\n","\n","for _, row in df.iterrows():\n","    year = row[\"year\"]\n","    # 去掉标点 & 数字 & 百分比\n","    tokens = [\n","        tok for tok in row[\"content\"].split()\n","        if tok and not is_punctuation(tok) and not is_number(tok) and not is_percentage(tok)\n","    ]\n","    freq.update(tokens)\n","    if pd.isna(year):\n","        continue\n","    y = int(year)\n","    for tok in tokens:\n","        if tok not in first_year:\n","            first_year[tok] = y\n","\n","# ========== 生成结果表 ==========\n","rows = []\n","for tok, f in freq.items():\n","    is_new = tok not in ref_set\n","    fy = first_year.get(tok, None)\n","    rows.append({\n","        \"term\": tok,\n","        \"frequency\": int(f),\n","        \"first_year\": fy,\n","        \"in_reference\": (not is_new),\n","        \"status\": \"new\" if is_new else \"in_reference\"\n","    })\n","\n","result = pd.DataFrame(rows).sort_values(\n","    by=[\"status\", \"frequency\", \"first_year\", \"term\"],\n","    ascending=[True, False, True, True]\n",").reset_index(drop=True)\n","\n","# ========== 保存结果 ==========\n","new_df = result[result[\"status\"] == \"new\"].copy()\n","old_df = result[result[\"status\"] == \"in_reference\"].copy()\n","\n","result.to_csv(out_all, index=False, encoding=\"utf-8-sig\")\n","new_df.to_csv(out_new, index=False, encoding=\"utf-8-sig\")\n","old_df.to_csv(out_old, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"总词数（去标点+数字+百分比）：{len(result)}\")\n","print(f\"新词数：{len(new_df)}\")\n","print(f\"旧词数：{len(old_df)}\")\n","print(f\"已保存：\\n- {out_all}\\n- {out_new}\\n- {out_old}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BeRrXj9F8xq","executionInfo":{"status":"ok","timestamp":1755589999014,"user_tz":-120,"elapsed":3845,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"c12afa1d-0119-4d7d-f147-424d299c07f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["总词数（去标点+数字+百分比）：52674\n","新词数：29972\n","旧词数：22702\n","已保存：\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch1_néologisme_data_all.csv\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch1_néologisme_new_only.csv\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch1_néologisme_in_reference.csv\n"]}]},{"cell_type":"markdown","source":["#méthode 2 avec Thulac"],"metadata":{"id":"A4KkO03P-XVo"}},{"cell_type":"code","source":["# 去掉标点符号和数字，百分比\n","\n","from pathlib import Path\n","import pandas as pd\n","from collections import Counter\n","import unicodedata\n","import re\n","\n","# ========== 配置路径 ==========\n","path_refs = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt\")\n","path_corpus = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch2_all_data_2015_2025_tok_thulac.csv\")\n","\n","out_dir = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification\")\n","out_all  = out_dir / \"ch2_néologisme_data_all.csv\"\n","out_new  = out_dir / \"ch2_néologisme_new_only.csv\"\n","out_old  = out_dir / \"ch2_néologisme_in_reference.csv\"\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 读取参考词表 ==========\n","ref_terms = []\n","with path_refs.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = line.strip()\n","        if t:\n","            ref_terms.append(t)\n","\n","seen = set()\n","ref_terms = [t for t in ref_terms if not (t in seen or seen.add(t))]\n","ref_set = set(ref_terms)\n","\n","# ========== 读取语料 ==========\n","df = pd.read_csv(path_corpus, encoding=\"utf-8\")\n","\n","required_cols = {\"content\", \"date\"}\n","if not required_cols.issubset(df.columns):\n","    raise ValueError(f\"CSV 缺少必要列：{required_cols - set(df.columns)}\")\n","\n","df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n","df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n","df[\"year\"] = df[\"date_parsed\"].dt.year\n","df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n","\n","# ========== 过滤函数 ==========\n","# 标点过滤\n","def is_punctuation(token):\n","    return all(unicodedata.category(ch).startswith(\"P\") or ch.isspace() for ch in token)\n","\n","# 数字过滤（阿拉伯数字、中文数字大写/小写）\n","num_pattern = re.compile(r\"^[0-9一二三四五六七八九十百千万亿〇零壹贰叁肆伍陆柒捌玖拾佰仟]+$\")\n","def is_number(token):\n","    return bool(num_pattern.match(token))\n","\n","# 百分比过滤（包含阿拉伯数字+% 或 中文“百分之”结构）\n","percent_pattern = re.compile(r\"^([0-9]+%|百分之[一二三四五六七八九十百千万亿〇零壹贰叁肆伍陆柒捌玖拾佰仟]+)$\")\n","def is_percentage(token):\n","    return bool(percent_pattern.match(token))\n","\n","# ========== 统计词频 & 首次出现年份 ==========\n","freq = Counter()\n","first_year = {}\n","\n","for _, row in df.iterrows():\n","    year = row[\"year\"]\n","    # 去掉标点 & 数字 & 百分比\n","    tokens = [\n","        tok for tok in row[\"content\"].split()\n","        if tok and not is_punctuation(tok) and not is_number(tok) and not is_percentage(tok)\n","    ]\n","    freq.update(tokens)\n","    if pd.isna(year):\n","        continue\n","    y = int(year)\n","    for tok in tokens:\n","        if tok not in first_year:\n","            first_year[tok] = y\n","\n","# ========== 生成结果表 ==========\n","rows = []\n","for tok, f in freq.items():\n","    is_new = tok not in ref_set\n","    fy = first_year.get(tok, None)\n","    rows.append({\n","        \"term\": tok,\n","        \"frequency\": int(f),\n","        \"first_year\": fy,\n","        \"in_reference\": (not is_new),\n","        \"status\": \"new\" if is_new else \"in_reference\"\n","    })\n","\n","result = pd.DataFrame(rows).sort_values(\n","    by=[\"status\", \"frequency\", \"first_year\", \"term\"],\n","    ascending=[True, False, True, True]\n",").reset_index(drop=True)\n","\n","# ========== 保存结果 ==========\n","new_df = result[result[\"status\"] == \"new\"].copy()\n","old_df = result[result[\"status\"] == \"in_reference\"].copy()\n","\n","result.to_csv(out_all, index=False, encoding=\"utf-8-sig\")\n","new_df.to_csv(out_new, index=False, encoding=\"utf-8-sig\")\n","old_df.to_csv(out_old, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"总词数（去标点+数字+百分比）：{len(result)}\")\n","print(f\"新词数：{len(new_df)}\")\n","print(f\"旧词数：{len(old_df)}\")\n","print(f\"已保存：\\n- {out_all}\\n- {out_new}\\n- {out_old}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8n5c5seOQpoU","executionInfo":{"status":"ok","timestamp":1755590007261,"user_tz":-120,"elapsed":3934,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"57863f78-a6ce-4c3b-deb6-53e84e9c1a32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["总词数（去标点+数字+百分比）：52674\n","新词数：29972\n","旧词数：22702\n","已保存：\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch2_néologisme_data_all.csv\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch2_néologisme_new_only.csv\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch2_néologisme_in_reference.csv\n"]}]},{"cell_type":"markdown","source":["#méthode 3 avec Thulac"],"metadata":{"id":"oTPixm7aEl0q"}},{"cell_type":"code","source":["# 去掉标点符号和数字，百分比\n","\n","from pathlib import Path\n","import pandas as pd\n","from collections import Counter\n","import unicodedata\n","import re\n","\n","# ========== 配置路径 ==========\n","path_refs = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt\")\n","path_corpus = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch2_all_data_2015_2025_tok_thulac.csv\")\n","\n","out_dir = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification\")\n","out_all  = out_dir / \"ch3_néologisme_data_all.csv\"\n","out_new  = out_dir / \"ch3_néologisme_new_only.csv\"\n","out_old  = out_dir / \"ch3_néologisme_in_reference.csv\"\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 读取参考词表 ==========\n","ref_terms = []\n","with path_refs.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = line.strip()\n","        if t:\n","            ref_terms.append(t)\n","\n","seen = set()\n","ref_terms = [t for t in ref_terms if not (t in seen or seen.add(t))]\n","ref_set = set(ref_terms)\n","\n","# ========== 读取语料 ==========\n","df = pd.read_csv(path_corpus, encoding=\"utf-8\")\n","\n","required_cols = {\"content\", \"date\"}\n","if not required_cols.issubset(df.columns):\n","    raise ValueError(f\"CSV 缺少必要列：{required_cols - set(df.columns)}\")\n","\n","df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n","df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n","df[\"year\"] = df[\"date_parsed\"].dt.year\n","df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n","\n","# ========== 过滤函数 ==========\n","# 标点过滤\n","def is_punctuation(token):\n","    return all(unicodedata.category(ch).startswith(\"P\") or ch.isspace() for ch in token)\n","\n","# 数字过滤（阿拉伯数字、中文数字大写/小写）\n","num_pattern = re.compile(r\"^[0-9一二三四五六七八九十百千万亿〇零壹贰叁肆伍陆柒捌玖拾佰仟]+$\")\n","def is_number(token):\n","    return bool(num_pattern.match(token))\n","\n","# 百分比过滤（包含阿拉伯数字+% 或 中文“百分之”结构）\n","percent_pattern = re.compile(r\"^([0-9]+%|百分之[一二三四五六七八九十百千万亿〇零壹贰叁肆伍陆柒捌玖拾佰仟]+)$\")\n","def is_percentage(token):\n","    return bool(percent_pattern.match(token))\n","\n","# ========== 统计词频 & 首次出现年份 ==========\n","freq = Counter()\n","first_year = {}\n","\n","for _, row in df.iterrows():\n","    year = row[\"year\"]\n","    # 去掉标点 & 数字 & 百分比\n","    tokens = [\n","        tok for tok in row[\"content\"].split()\n","        if tok and not is_punctuation(tok) and not is_number(tok) and not is_percentage(tok)\n","    ]\n","    freq.update(tokens)\n","    if pd.isna(year):\n","        continue\n","    y = int(year)\n","    for tok in tokens:\n","        if tok not in first_year:\n","            first_year[tok] = y\n","\n","# ========== 生成结果表 ==========\n","rows = []\n","for tok, f in freq.items():\n","    is_new = tok not in ref_set\n","    fy = first_year.get(tok, None)\n","    rows.append({\n","        \"term\": tok,\n","        \"frequency\": int(f),\n","        \"first_year\": fy,\n","        \"in_reference\": (not is_new),\n","        \"status\": \"new\" if is_new else \"in_reference\"\n","    })\n","\n","result = pd.DataFrame(rows).sort_values(\n","    by=[\"status\", \"frequency\", \"first_year\", \"term\"],\n","    ascending=[True, False, True, True]\n",").reset_index(drop=True)\n","\n","# ========== 保存结果 ==========\n","new_df = result[result[\"status\"] == \"new\"].copy()\n","old_df = result[result[\"status\"] == \"in_reference\"].copy()\n","\n","result.to_csv(out_all, index=False, encoding=\"utf-8-sig\")\n","new_df.to_csv(out_new, index=False, encoding=\"utf-8-sig\")\n","old_df.to_csv(out_old, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"总词数（去标点+数字+百分比）：{len(result)}\")\n","print(f\"新词数：{len(new_df)}\")\n","print(f\"旧词数：{len(old_df)}\")\n","print(f\"已保存：\\n- {out_all}\\n- {out_new}\\n- {out_old}\")\n"],"metadata":{"id":"LAZHe81qEyDO","colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"status":"error","timestamp":1755589287311,"user_tz":-120,"elapsed":47,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"f5dc2d52-f151-4eb6-8f23-0abdb969a312"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3055467436.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# ========== 读取参考词表 ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mref_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mpath_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt'"]}]},{"cell_type":"markdown","source":["#méthode amélioré\n","\n","1.   Élément de liste\n","2.   Élément de liste\n","\n"],"metadata":{"id":"Gti1CzLgUjuo"}},{"cell_type":"code","source":["import time\n","start = time.time()\n","\n","from tqdm.auto import tqdm\n","def log(msg):\n","    print(f\"[{time.time() - start:8.2f}s] {msg}\")\n","\n","\n","\n","\n","\n","from pathlib import Path\n","import pandas as pd\n","from collections import Counter, defaultdict\n","import numpy as np\n","import unicodedata, re, math\n","\n","# =========================\n","# 配置\n","# =========================\n","IN_REF = Path(\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt\")\n","IN_CORPUS = Path(\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch2_all_data_2015_2025_tok_thulac.csv\")\n","OUT_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification\")\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# 阈值（可按语料调整）\n","MINF_UNI, MINDF_UNI = 5, 3     # unigram 词频/文档频\n","MINF_BI,  MINDF_BI  = 5, 3     # bigram 词频/文档频\n","MINF_TRI, MINDF_TRI = 3, 2     # trigram 词频/文档频\n","PMI2_MIN, PMI3_MIN  = 3.5, 2.5 # PMI 阈值\n","ENT_MIN             = 2.0      # 左右熵阈值（L+R）\n","ABSORB_RATIO        = 0.80     # 去嵌套：被更长单位覆盖比例阈值\n","SENT_SEP = set(\"。！？!?；;：:\") # 断句标记（基于分好词后仍保留的中文标点）\n","\n","STOP = {\"的\",\"了\",\"和\",\"与\",\"及\",\"等\",\"在\",\"把\",\"被\",\"对\",\"于\",\"之\",\"其\"}  # 边界停用词\n","\n","# =========================\n","# 读取参考词表\n","# =========================\n","ref_set = set()\n","with IN_REF.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = line.strip()\n","        if t: ref_set.add(t)\n","\n","# =========================\n","# 读取语料\n","# 需包含：content（空格分词后文本）、date（可选）\n","# =========================\n","df = pd.read_csv(IN_CORPUS, encoding=\"utf-8\")\n","if \"content\" not in df.columns:\n","    raise ValueError(\"CSV 缺少 'content' 列。\")\n","df[\"date_parsed\"] = pd.to_datetime(df.get(\"date\", None), errors=\"coerce\")\n","df[\"year\"] = df[\"date_parsed\"].dt.year\n","df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n","\n","def normalize(s: str) -> str:\n","    s = unicodedata.normalize(\"NFKC\", s)\n","    s = s.replace(\"\\u3000\", \" \").strip()\n","    return s\n","\n","df[\"content\"] = df[\"content\"].map(normalize)\n","\n","# 噪声过滤（与之前一致）\n","punct_or_space = lambda ch: unicodedata.category(ch).startswith(\"P\") or ch.isspace()\n","def is_punct_token(tok):  # 全是标点/空白\n","    return all(punct_or_space(ch) for ch in tok)\n","\n","num_pat = re.compile(r\"^[0-9０-９一二三四五六七八九十百千万亿〇零壹贰叁肆伍陆柒捌玖拾佰仟]+$\")\n","def is_number(tok): return bool(num_pat.match(tok))\n","\n","percent_pat = re.compile(r\"^([0-9０-９]+%|百分之[一二三四五六七八九十百千万亿〇零壹贰叁肆伍陆柒捌玖拾佰仟]+)$\")\n","def is_percent(tok): return bool(percent_pat.match(tok))\n","\n","url_pat = re.compile(r\"https?://|www\\.\", re.I)\n","mention_pat = re.compile(r\"^[@#]\")\n","def bad_token(tok):\n","    if not tok: return True\n","    if is_punct_token(tok) or is_number(tok) or is_percent(tok): return True\n","    if url_pat.search(tok) or mention_pat.search(tok): return True\n","    return False\n","\n","# =========================\n","# 工具：按句生成 n-gram（不跨句）\n","# =========================\n","def split_by_sentence(tokens):\n","    sents, cur = [], []\n","    for t in tokens:\n","        cur.append(t)\n","        if t in SENT_SEP:\n","            sents.append(cur); cur = []\n","    if cur: sents.append(cur)\n","    return sents\n","\n","def bad_edge_ngram(ng):\n","    parts = ng.split()\n","    return (parts[0] in STOP) or (parts[-1] in STOP)\n","\n","# =========================\n","# 统计（分别计数 1/2/3-gram）\n","# =========================\n","uni_freq, bi_freq, tri_freq = Counter(), Counter(), Counter()\n","uni_df,   bi_df,   tri_df   = Counter(), Counter(), Counter()\n","first_year = {}\n","year_counts_uni = defaultdict(lambda: Counter())  # 仅用于首现/爆发度（以 unigram 为主，也可扩展）\n","\n","for _, row in df.iterrows():\n","    year = row[\"year\"]\n","    tokens = [t for t in row[\"content\"].split() if not bad_token(t)]\n","    sents = split_by_sentence(tokens)\n","\n","    seen_uni, seen_bi, seen_tri = set(), set(), set()\n","\n","    for sent in sents:\n","        clean = [w for w in sent if w.strip() and w not in SENT_SEP]\n","\n","        # 1-gram\n","        uni_freq.update(clean)\n","        seen_uni.update(set(clean))\n","\n","        # 2-gram / 3-gram（句内滚动，但过滤边界停用词）\n","        for n in (2,3):\n","            for i in range(len(clean)-n+1):\n","                ng = \" \".join(clean[i:i+n])\n","                if bad_edge_ngram(ng):\n","                    continue\n","                if n == 2:\n","                    bi_freq[ng] += 1; seen_bi.add(ng)\n","                else:\n","                    tri_freq[ng] += 1; seen_tri.add(ng)\n","\n","    # 文档频\n","    uni_df.update(seen_uni)\n","    bi_df.update(seen_bi)\n","    tri_df.update(seen_tri)\n","\n","    # 首现年份（以 unigram 为例，n-gram 也可扩展）\n","    if pd.notna(year):\n","        y = int(year)\n","        for w in seen_uni:\n","            if w not in first_year:\n","                first_year[w] = y\n","            year_counts_uni[y][w] += 1\n","\n","# =========================\n","# PMI（仅 n>=2）\n","# =========================\n","N_uni = sum(uni_freq.values())\n","def p_uni(w): return (uni_freq[w] + 1) / (N_uni + len(uni_freq))\n","\n","def p_bi(ng):\n","    N_bi = sum(bi_freq.values())\n","    return (bi_freq[ng] + 1) / (N_bi + len(bi_freq) + 1e-9)\n","\n","def p_tri(ng):\n","    N_tri = sum(tri_freq.values())\n","    return (tri_freq[ng] + 1) / (N_tri + len(tri_freq) + 1e-9)\n","\n","def PMI2(ng):\n","    a,b = ng.split()\n","    return math.log( p_bi(ng) / (p_uni(a)*p_uni(b)) + 1e-12 )\n","\n","def PMI3(ng):\n","    a,b,c = ng.split()\n","    return math.log( p_tri(ng) / (p_uni(a)*p_uni(b)*p_uni(c)) + 1e-12 )\n","\n","# =========================\n","# 左右熵（衡量凝固度）\n","# =========================\n","left_ctx, right_ctx = defaultdict(Counter), defaultdict(Counter)\n","for _, row in df.iterrows():\n","    toks = [t for t in row[\"content\"].split() if t.strip()]\n","    for i, w in enumerate(toks):\n","        if i>0: left_ctx[w][toks[i-1]] += 1\n","        if i<len(toks)-1: right_ctx[w][toks[i+1]] += 1\n","\n","def entropy(counter):\n","    total = sum(counter.values()) or 1\n","    return -sum((c/total)*math.log((c/total)+1e-12) for c in counter.values())\n","\n","def lr_entropy_ng(ng):\n","    parts = ng.split()\n","    return entropy(left_ctx[parts[0]]) + entropy(right_ctx[parts[-1]])\n","\n","# =========================\n","# 去嵌套（C-value 思路）\n","# bigram 若大多被某个 trigram 覆盖，则剔除\n","# =========================\n","def suppress_nested_bi(bi_freq, tri_freq, ratio=0.8):\n","    keep = {}\n","    index_max = defaultdict(int)  # bigram -> 覆盖它的 trigram 最大频次\n","    for tri, ftri in tri_freq.items():\n","        parts = tri.split()\n","        for i in range(2):\n","            index_max[\" \".join(parts[i:i+2])] = max(index_max[\" \".join(parts[i:i+2])], ftri)\n","    for bi, fbi in bi_freq.items():\n","        if index_max[bi] >= ratio * fbi:\n","            continue\n","        keep[bi] = fbi\n","    return Counter(keep)\n","\n","bi_freq = suppress_nested_bi(bi_freq, tri_freq, ABSORB_RATIO)\n","\n","# （可选）unigram 被高频 bigram“吸收”时降权/剔除，这里先不做，保留 unigram 的可见性\n","\n","# =========================\n","# 选择与打标签（按 n 分表）\n","# =========================\n","def select_unigram():\n","    rows=[]\n","    for w,f in uni_freq.items():\n","        if f<MINF_UNI or uni_df[w]<MINDF_UNI:\n","            continue\n","        in_ref = (w in ref_set)\n","        rows.append({\n","            \"term\": w, \"n\":1, \"freq\":int(f), \"doc_freq\":int(uni_df[w]),\n","            \"first_year\": first_year.get(w), \"in_reference\": in_ref,\n","            \"status\": \"in_reference\" if in_ref else \"new\"\n","        })\n","    return pd.DataFrame(rows).sort_values([\"status\",\"freq\",\"doc_freq\"], ascending=[True,False,False])\n","\n","def select_bigram():\n","    rows=[]\n","    for ng,f in bi_freq.items():\n","        if f<MINF_BI or bi_df[ng]<MINDF_BI:\n","            continue\n","        if bad_edge_ngram(ng):\n","            continue\n","        pmi = PMI2(ng); ent = lr_entropy_ng(ng)\n","        if pmi<PMI2_MIN or ent<ENT_MIN:\n","            continue\n","        in_ref = (ng in ref_set)\n","        rows.append({\n","            \"term\": ng, \"n\":2, \"freq\":int(f), \"doc_freq\":int(bi_df[ng]),\n","            \"PMI\":round(pmi,3), \"LRent\":round(ent,3),\n","            \"in_reference\": in_ref, \"status\": \"in_reference\" if in_ref else \"new\"\n","        })\n","    return pd.DataFrame(rows).sort_values([\"status\",\"PMI\",\"freq\"], ascending=[True,False,False])\n","\n","def select_trigram():\n","    rows=[]\n","    for ng,f in tri_freq.items():\n","        if f<MINF_TRI or tri_df[ng]<MINDF_TRI:\n","            continue\n","        if bad_edge_ngram(ng):\n","            continue\n","        pmi = PMI3(ng); ent = lr_entropy_ng(ng)\n","        if pmi<PMI3_MIN or ent<ENT_MIN:\n","            continue\n","        in_ref = (ng in ref_set)\n","        rows.append({\n","            \"term\": ng, \"n\":3, \"freq\":int(f), \"doc_freq\":int(tri_df[ng]),\n","            \"PMI\":round(pmi,3), \"LRent\":round(ent,3),\n","            \"in_reference\": in_ref, \"status\": \"in_reference\" if in_ref else \"new\"\n","        })\n","    return pd.DataFrame(rows).sort_values([\"status\",\"PMI\",\"freq\"], ascending=[True,False,False])\n","\n","res_uni = select_unigram()\n","res_bi  = select_bigram()\n","res_tri = select_trigram()\n","\n","# =========================\n","# “成语/四字格”优先：保留连续四字（无空格），删除被其覆盖的子 bigram\n","# =========================\n","def is_4char_ng(ng):\n","    chars = \"\".join(ng.split())\n","    return len(chars) == 4\n","\n","good_tri_4 = set(t for t in res_tri[\"term\"] if is_4char_ng(t))\n","if len(good_tri_4) > 0:\n","    def covered_by_4(bi):\n","        b = \"\".join(bi.split())\n","        for tri in good_tri_4:\n","            if b in \"\".join(tri.split()):\n","                return True\n","        return False\n","    res_bi = res_bi[~res_bi[\"term\"].apply(covered_by_4)]\n","\n","# =========================\n","# 导出分表 + 合并总表（按 n 优先级：tri > bi > uni）\n","# =========================\n","res_uni.to_csv(OUT_DIR/\"chi_neo_uni.csv\", index=False, encoding=\"utf-8-sig\")\n","res_bi.to_csv( OUT_DIR/\"chi_neo_bi.csv\",  index=False, encoding=\"utf-8-sig\")\n","res_tri.to_csv( OUT_DIR/\"chi_neo_tri.csv\", index=False, encoding=\"utf-8-sig\")\n","\n","# 合并并去重（如果同一个短项被更长项包含，则保留更长的）\n","chosen = []\n","taken = set()\n","\n","def norm_chars(s): return \"\".join(s.split())\n","\n","for df_part in [res_tri, res_bi, res_uni]:  # 优先保留更长\n","    for _, r in df_part.iterrows():\n","        t = r[\"term\"]; t_norm = norm_chars(t)\n","        # 若被已有更长项完全覆盖，则跳过\n","        if any(t_norm in norm_chars(x[\"term\"]) for x in chosen if len(norm_chars(x[\"term\"])) >= len(t_norm)):\n","            continue\n","        chosen.append(r)\n","\n","res_final = pd.DataFrame(chosen).reset_index(drop=True)\n","res_final.to_csv(OUT_DIR/\"neo_final.csv\", index=False, encoding=\"utf-8-sig\")\n","\n","print(\"已保存：\")\n","print(\" -\", OUT_DIR/\"chi_neo_uni.csv\")\n","print(\" -\", OUT_DIR/\"chi_neo_bi.csv\")\n","print(\" -\", OUT_DIR/\"chi_neo_tri.csv\")\n","print(\" -\", OUT_DIR/\"chi_neo_final.csv\")\n","print(f\"候选总数：uni={len(res_uni)}, bi={len(res_bi)}, tri={len(res_tri)}, final={len(res_final)}\")\n","\n","\n","end = time.time()\n","print(f\"总耗时：{end - start:.2f} 秒\")\n"],"metadata":{"id":"NIGJ0YrRciqe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#En vientnamien"],"metadata":{"id":"kKwyqY0WHiJR"}},{"cell_type":"code","source":["# 去掉标点符号 + 含数字的词 + 百分比（越南语版）\n","\n","from pathlib import Path\n","import pandas as pd\n","from collections import Counter\n","import unicodedata\n","import re\n","\n","# ========== 配置路径（越南语） ==========\n","# 2015 年之前的参考词表（越南语）\n","path_refs = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all2004_2015_vi copie.txt\")\n","\n","# 2015–2025 越南语新闻语料（content 已分词：空格分隔）\n","path_corpus = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/vi_all_data_2015_2025_tokenized.csv\")\n","\n","# 输出目录与文件（越南语前缀）\n","out_dir = Path(r\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification\")\n","out_all  = out_dir / \"vi_néologisme_data_all.csv\"\n","out_new  = out_dir / \"vi_néologisme_new_only.csv\"\n","out_old  = out_dir / \"vi_néologisme_in_reference.csv\"\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 读取参考词表 ==========\n","ref_terms = []\n","with path_refs.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = line.strip()\n","        if t:\n","            ref_terms.append(t)\n","\n","# 去重保序\n","seen = set()\n","ref_terms = [t for t in ref_terms if not (t in seen or seen.add(t))]\n","ref_set = set(ref_terms)\n","\n","# ========== 读取语料 ==========\n","df = pd.read_csv(path_corpus, encoding=\"utf-8\")\n","\n","required_cols = {\"content\", \"date\"}\n","if not required_cols.issubset(df.columns):\n","    raise ValueError(f\"CSV 缺少必要列：{required_cols - set(df.columns)}\")\n","\n","# 解析日期 -> 年份；按日期升序，保证“首次出现年份”正确\n","df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n","df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n","df[\"year\"] = df[\"date_parsed\"].dt.year\n","df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n","\n","# ========== 过滤函数 ==========\n","# 仅标点（Unicode 类别 P）或空白的 token\n","def is_punctuation(token: str) -> bool:\n","    return all(unicodedata.category(ch).startswith(\"P\") or ch.isspace() for ch in token)\n","\n","# 是否包含任意“数字字符”（含 0-9、全角数字等）\n","def has_any_digit(token: str) -> bool:\n","    return any(ch.isdigit() for ch in token)\n","\n","# 百分比（阿拉伯数字 + 可选小数（逗号或点）+ %），如 50%、50,5%、50.5%\n","percent_pattern = re.compile(r\"^\\d+(?:[.,]\\d+)?%$\")\n","def is_percentage(token: str) -> bool:\n","    return bool(percent_pattern.match(token))\n","\n","# ========== 统计词频 & 首次出现年份 ==========\n","freq = Counter()\n","first_year = {}\n","\n","for _, row in df.iterrows():\n","    year = row[\"year\"]\n","    # 过滤：标点 / 含数字 / 百分比\n","    tokens = [\n","        tok for tok in row[\"content\"].split()\n","        if tok and not is_punctuation(tok) and not has_any_digit(tok) and not is_percentage(tok)\n","    ]\n","    freq.update(tokens)\n","    if pd.isna(year):\n","        continue\n","    y = int(year)\n","    for tok in tokens:\n","        if tok not in first_year:\n","            first_year[tok] = y  # 已排序，首次赋值即最早年份\n","\n","# ========== 生成结果表 ==========\n","rows = []\n","for tok, f in freq.items():\n","    in_ref = tok in ref_set\n","    rows.append({\n","        \"term\": tok,\n","        \"frequency\": int(f),\n","        \"first_year\": first_year.get(tok, None),  # 出现在语料中的首年\n","        \"in_reference\": in_ref,                   # True=旧词；False=新词\n","        \"status\": \"in_reference\" if in_ref else \"new\"\n","    })\n","\n","result = pd.DataFrame(rows).sort_values(\n","    by=[\"status\", \"frequency\", \"first_year\", \"term\"],\n","    ascending=[True, False, True, True]\n",").reset_index(drop=True)\n","\n","# ========== 拆分并保存 ==========\n","new_df = result[result[\"status\"] == \"new\"].copy()\n","old_df = result[result[\"status\"] == \"in_reference\"].copy()\n","\n","# 带 BOM，便于 Excel\n","result.to_csv(out_all, index=False, encoding=\"utf-8-sig\")\n","new_df.to_csv(out_new, index=False, encoding=\"utf-8-sig\")\n","old_df.to_csv(out_old, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"总词数（去标点/含数字/百分比）：{len(result)}\")\n","print(f\"新词数：{len(new_df)}\")\n","print(f\"旧词数：{len(old_df)}\")\n","print(f\"已保存：\\n- {out_all}\\n- {out_new}\\n- {out_old}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vaj27GVWHkGe","executionInfo":{"status":"ok","timestamp":1755201033076,"user_tz":-120,"elapsed":4051,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"3746efdf-23bd-4514-9fe3-541d5b7927b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2742277300.py:44: UserWarning: Parsing dates in %d/%m/%Y %I:%M %p format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n","  df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n"]},{"output_type":"stream","name":"stdout","text":["总词数（去标点/含数字/百分比）：28083\n","新词数：24416\n","旧词数：3667\n","已保存：\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/vi_néologisme_data_all.csv\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/vi_néologisme_new_only.csv\n","- /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/vi_néologisme_in_reference.csv\n"]}]},{"cell_type":"markdown","source":["#Version améliorée"],"metadata":{"id":"Uae-dLThd_cL"}}]}