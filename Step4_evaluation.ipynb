{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNlCMWXKd+kTaCxuMm5G59A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#步骤 1：将所有 CSV 文件合并成一个文件\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NhxMPjSkZ5BV","executionInfo":{"status":"ok","timestamp":1755810530985,"user_tz":-120,"elapsed":20658,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"d8edf9fa-478d-40ab-c270-cbcf4fd95157"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#Code (Colab/py) — Modules prêts à coller\n","\n","Chemins adaptés à tes fichiers；部分功能（繁→简）用 opencc，可选。\n","\n","A. Préparation & normalisation"],"metadata":{"id":"NBLCpeT2LA24"}},{"cell_type":"code","source":["#A. Préparation & normalisation\n","\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import re, unicodedata\n","from urllib.parse import urlparse\n","\n","# ====== Chemins ======\n","path_refs   = Path(\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/liste de référence/all_ch.txt\")\n","path_corpus = Path(\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch2_all_data_2015_2025_tok_thulac.csv\")\n","path_neo = Path(\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch2_néologisme_new_only.csv\")\n","out_dir     = Path(\"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Evaluation\")\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ====== OpenCC (繁->简，可选) ======\n","try:\n","    from opencc import OpenCC\n","    cc = OpenCC('t2s')\n","except Exception:\n","    cc = None\n","\n","ZERO_WIDTH = re.compile(r\"[\\u200B-\\u200D\\uFEFF]\")\n","NUM_CHI = \"零一二三四五六七八九〇壹贰叁肆伍陆柒捌玖拾佰仟万亿\"\n","PUNCT_CAT = lambda ch: unicodedata.category(ch).startswith(\"P\")\n","\n","def normalize_text(x: str) -> str:\n","    if not isinstance(x, str):\n","        x = str(x) if pd.notna(x) else \"\"\n","    x = ZERO_WIDTH.sub(\"\", x)\n","    x = unicodedata.normalize(\"NFKC\", x)  # 全->半, 形态兼容\n","    if cc:\n","        x = cc.convert(x)  # 繁->简\n","    x = re.sub(r\"\\s+\", \" \", x).strip()\n","    return x\n","\n","def is_punct_token(tok: str) -> bool:\n","    return all(PUNCT_CAT(ch) or ch.isspace() for ch in tok)\n","\n","num_pattern = re.compile(rf\"^[0-9{NUM_CHI}]+$\")\n","def is_number_token(tok: str) -> bool:\n","    return bool(num_pattern.match(tok))\n","\n","percent_pattern = re.compile(r\"^([0-9]+%|百分之[零一二三四五六七八九十百千万亿〇]+)$\")\n","def is_percentage_token(tok: str) -> bool:\n","    return bool(percent_pattern.match(tok))\n","\n","def valid_token(tok: str) -> bool:\n","    return tok and (not is_punct_token(tok)) and (not is_number_token(tok)) and (not is_percentage_token(tok))\n","\n","# ====== Lecture référence ======\n","ref_terms = []\n","with path_refs.open(\"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        t = normalize_text(line.strip())\n","        if t:\n","            ref_terms.append(t)\n","ref_set = set(pd.unique(pd.Series(ref_terms)))\n","\n","# ====== Lecture corpus ======\n","df = pd.read_csv(path_corpus, encoding=\"utf-8\")\n","if not {\"content\",\"date\"}.issubset(df.columns):\n","    raise ValueError(\"Le CSV doit contenir les colonnes 'content' et 'date'.\")\n","\n","# Normalisations\n","df[\"content\"] = df[\"content\"].fillna(\"\").astype(str).map(normalize_text)\n","df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n","df[\"year\"] = df[\"date_parsed\"].dt.year\n","\n","# doc_id & source\n","if \"url\" in df.columns:\n","    def get_domain(u):\n","        try:\n","            return urlparse(u).netloc.lower()\n","        except Exception:\n","            return \"unknown\"\n","    df[\"source\"] = df[\"url\"].astype(str).map(get_domain)\n","else:\n","    df[\"source\"] = \"unknown\"\n","\n","if \"doc_id\" not in df.columns:\n","    df[\"doc_id\"] = np.arange(len(df))\n"],"metadata":{"id":"LPA5jeQrZ5XS","executionInfo":{"status":"ok","timestamp":1755810715610,"user_tz":-120,"elapsed":2029,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#B. Méthode 1 — fréquence、首现年份、文档/来源数（+ 阈值）\n","\n","from collections import Counter, defaultdict\n","\n","freq1 = Counter()\n","first_year1 = {}\n","docset1 = defaultdict(set)\n","sourceset1 = defaultdict(set)\n","\n","for _, row in df.iterrows():\n","    y = row[\"year\"]\n","    did = row[\"doc_id\"]\n","    src = row[\"source\"]\n","    tokens = [t for t in row[\"content\"].split() if valid_token(t)]\n","    freq1.update(tokens)\n","    if pd.notna(y):\n","        y = int(y)\n","        for t in tokens:\n","            if t not in first_year1:\n","                first_year1[t] = y\n","    for t in set(tokens):\n","        docset1[t].add(did)\n","        sourceset1[t].add(src)\n","\n","rows1 = []\n","for t, f in freq1.items():\n","    rows1.append({\n","        \"term\": t,\n","        \"frequency\": int(f),\n","        \"first_year\": first_year1.get(t),\n","        \"doc_count\": len(docset1[t]),\n","        \"source_count\": len(sourceset1[t]),\n","        \"in_reference\": t in ref_set,\n","        \"status\": \"new\" if t not in ref_set else \"in_reference\",\n","        \"method1\": 1\n","    })\n","m1 = pd.DataFrame(rows1)\n","\n","# Seuils recommandés (à ajuster)\n","MIN_FREQ   = 3\n","MIN_DOC    = 2\n","MIN_SOURCE = 2\n","\n","m1_candidates = m1.query(\"status == 'new' and frequency >= @MIN_FREQ and doc_count >= @MIN_DOC and source_count >= @MIN_SOURCE\").copy()\n","m1_candidates.head(3)\n"],"metadata":{"id":"bLh8DpURLHpi","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1755810726780,"user_tz":-120,"elapsed":4596,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"654dd5c6-894d-43d7-ba0b-988062282fa7"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [term, frequency, first_year, doc_count, source_count, in_reference, status, method1]\n","Index: []"],"text/html":["\n","  <div id=\"df-fb86a09c-34fd-4e68-b9c1-103d432fa083\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>term</th>\n","      <th>frequency</th>\n","      <th>first_year</th>\n","      <th>doc_count</th>\n","      <th>source_count</th>\n","      <th>in_reference</th>\n","      <th>status</th>\n","      <th>method1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb86a09c-34fd-4e68-b9c1-103d432fa083')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fb86a09c-34fd-4e68-b9c1-103d432fa083 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fb86a09c-34fd-4e68-b9c1-103d432fa083');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"m1_candidates","repr_error":"Out of range float values are not JSON compliant: nan"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#C. Méthode 2 — n-grammes 1–5（含 PMI 简版）\n","\n","from math import log2\n","from itertools import islice\n","\n","MAX_N = 5\n","\n","# 先得到 unigram 统计（供 PMI）\n","uni = Counter()\n","docset_uni = defaultdict(set)\n","\n","for _, row in df.iterrows():\n","    did = row[\"doc_id\"]\n","    toks = [t for t in row[\"content\"].split() if valid_token(t)]\n","    uni.update(toks)\n","    for t in set(toks):\n","        docset_uni[t].add(did)\n","\n","TOTAL_TOK = sum(uni.values())\n","P_uni = {t: uni[t] / TOTAL_TOK for t in uni}\n","\n","# 句子边界（避免跨句拼接，简化：用句号/问号/叹号/分号等粗分）\n","SPLIT_RE = re.compile(r\"[。！？；;?!]\")\n","\n","def tokens_by_sentence(text):\n","    # 假设 content 已经是空格分词；这里先粗切句，再分词\n","    # 简化：直接按标点切割原 text，再对每段 split()\n","    parts = SPLIT_RE.split(text)\n","    for p in parts:\n","        toks = [t for t in p.split() if valid_token(t)]\n","        if toks:\n","            yield toks\n","\n","def sliding_ngrams(seq, n):\n","    it = iter(seq)\n","    win = list(islice(it, n))\n","    if len(win) == n:\n","        yield tuple(win)\n","    for x in it:\n","        win = win[1:] + [x]\n","        yield tuple(win)\n","\n","# 统计 n-gram\n","ng_counts = Counter()\n","ng_docset = defaultdict(set)\n","\n","for _, row in df.iterrows():\n","    did = row[\"doc_id\"]\n","    for toks in tokens_by_sentence(row[\"content\"]):\n","        L = len(toks)\n","        for n in range(1, MAX_N+1):\n","            if L < n:\n","                continue\n","            for ng in sliding_ngrams(toks, n):\n","                ng_counts[ng] += 1\n","                ng_docset[ng].add(did)\n","\n","# PMI（对 n>=2 用相邻 PMI 的平均；n==1 时设为NaN）\n","def pmi_adjacent(ng):\n","    if len(ng) == 1:\n","        return np.nan\n","    # P(ng) 近似用频次 / 所有窗口数；简化以 TOTAL_TOK 近似分母（保守）\n","    p_ng = ng_counts[ng] / TOTAL_TOK\n","    # 相邻对的几何近似：平均 PMI\n","    pairs = list(zip(ng, ng[1:]))\n","    pmis = []\n","    for a,b in pairs:\n","        p_a, p_b = P_uni.get(a, 1e-12), P_uni.get(b, 1e-12)\n","        # 用 bigram 概率未知时，用联合近似：频率( (a,b) )/TOTAL_TOK；在我们的统计里，(a,b) 也是一个 n-gram\n","        p_ab = ng_counts.get((a,b), 0) / TOTAL_TOK\n","        if p_ab <= 0:\n","            pmis.append(-np.inf)\n","        else:\n","            pmis.append(log2(p_ab / (p_a * p_b)))\n","    return float(np.mean(pmis)) if pmis else np.nan\n","\n","rows2 = []\n","for ng, f in ng_counts.items():\n","    n = len(ng)\n","    # 拼接成中文串（无空格），也保留空格版以便调试\n","    joined = \"\".join(ng)\n","    spaced = \" \".join(ng)\n","    rows2.append({\n","        \"ngram_joined\": joined,\n","        \"ngram_spaced\": spaced,\n","        \"n\": n,\n","        \"frequency\": int(f),\n","        \"doc_count\": len(ng_docset[ng]),\n","        \"pmi_adj\": pmi_adjacent(ng)\n","    })\n","m2_all = pd.DataFrame(rows2)\n","\n","# 只保留不在参考表中的候选（用无空格形式对比参考）\n","m2_candidates = m2_all[\n","    (~m2_all[\"ngram_joined\"].isin(ref_set)) &\n","    (\n","        (m2_all[\"n\"] == 1) |  # 单字/单词也许有用（可按需禁用）\n","        ((m2_all[\"n\"] >= 2) &\n","         (m2_all[\"frequency\"] >= MIN_FREQ) &\n","         (m2_all[\"doc_count\"] >= MIN_DOC) &\n","         (m2_all[\"pmi_adj\"] >= 2.0))  # PMI阈值可调：1.5~3.0\n","    )\n","].copy()\n","\n","# 为 n-gram 估计 first_year（首次出现在某文档的年份）\n","first_year2 = {}\n","for _, row in df.iterrows():\n","    y = row[\"year\"]\n","    if pd.isna(y):\n","        continue\n","    y = int(y)\n","    for toks in tokens_by_sentence(row[\"content\"]):\n","        for n in range(1, MAX_N+1):\n","            for ng in sliding_ngrams(toks, n):\n","                key = \"\".join(ng)\n","                if key not in first_year2:\n","                    first_year2[key] = y\n","\n","m2_candidates[\"first_year\"] = m2_candidates[\"ngram_joined\"].map(first_year2.get)\n","m2_candidates[\"method2\"] = 1\n"],"metadata":{"id":"I67DRD1ILKnO","executionInfo":{"status":"ok","timestamp":1755810796131,"user_tz":-120,"elapsed":57844,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#D. Méthode 3 — audit de la référence（谁不在语料里）\n","\n","\n","# 参考表每个词在 2015–2025 的频次/文档数\n","freq_ref_in_corpus = Counter()\n","docset_ref_in_corpus = defaultdict(set)\n","\n","for _, row in df.iterrows():\n","    did = row[\"doc_id\"]\n","    toks = set([t for t in row[\"content\"].split() if valid_token(t)])\n","    inter = toks.intersection(ref_set)\n","    for t in inter:\n","        freq_ref_in_corpus[t] += 1  # 注意：这里是“包含该词的句段数”的简化；也可按出现次数统计\n","        docset_ref_in_corpus[t].add(did)\n","\n","rows3 = []\n","for t in ref_set:\n","    rows3.append({\n","        \"term\": t,\n","        \"doc_count\": len(docset_ref_in_corpus[t]),\n","        \"frequency_proxy\": int(freq_ref_in_corpus[t])\n","    })\n","m3_audit = pd.DataFrame(rows3).sort_values([\"doc_count\",\"frequency_proxy\"], ascending=True).reset_index(drop=True)\n","m3_audit[\"method3_present\"] = (m3_audit[\"doc_count\"] > 0).astype(int)\n"],"metadata":{"id":"esHsCvl-LOSM","executionInfo":{"status":"ok","timestamp":1755810995259,"user_tz":-120,"elapsed":4599,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#E. Fusion & échantillonnage pour annotation（生成 label_me.csv）\n","\n","# 统一字段并合并\n","m1_small = m1_candidates.rename(columns={\"term\":\"unit\"})[[\"unit\",\"frequency\",\"doc_count\",\"source_count\",\"first_year\"]].copy()\n","m1_small[\"method1\"] = 1\n","\n","m2_small = m2_candidates.rename(columns={\"ngram_joined\":\"unit\"})[[\"unit\",\"frequency\",\"doc_count\",\"first_year\",\"pmi_adj\",\"n\"]].copy()\n","m2_small[\"source_count\"] = np.nan\n","m2_small[\"method2\"] = 1\n","\n","# 合并\n","pool = pd.concat([m1_small, m2_small], ignore_index=True)\n","pool = (pool.groupby(\"unit\", as_index=False)\n","            .agg({\n","                \"frequency\":\"max\",\n","                \"doc_count\":\"max\",\n","                \"source_count\":\"max\",\n","                \"first_year\":\"min\",\n","                \"pmi_adj\":\"max\",\n","                \"n\":\"max\",\n","                \"method1\":\"max\",\n","                \"method2\":\"max\"\n","            })\n","        )\n","\n","# 加上一列示例上下文（从首个命中的文档中取左右各10个token）\n","def get_first_context(term, window=10):\n","    for _, row in df.iterrows():\n","        toks = row[\"content\"].split()\n","        for i, tk in enumerate(toks):\n","            if tk == term:\n","                L = max(0, i-window); R = min(len(toks), i+window+1)\n","                return \"… \" + \"\".join(toks[L:R]) + \" …\"\n","        # 兼容 n-gram（已去空格合并的）\n","        joined = \"\".join([t for t in toks if valid_token(t)])\n","        pos = joined.find(term)\n","        if pos != -1:\n","            start = max(0, pos-30); end = min(len(joined), pos+len(term)+30)\n","            return \"… \" + joined[start:end] + \" …\"\n","    return \"\"\n","\n","pool[\"example\"] = pool[\"unit\"].map(get_first_context)\n","\n","# 分层抽样：M1_only / M2_only / both / low_freq\n","def stratified_sample(df_in, query, k=50, random_state=42):\n","    sub = df_in.query(query)\n","    if len(sub) <= k:\n","        return sub\n","    return sub.sample(k, random_state=random_state)\n","\n","s1 = stratified_sample(pool, \"method1==1 and method2!=1\", k=50)\n","s2 = stratified_sample(pool, \"method2==1 and method1!=1\", k=50)\n","s3 = stratified_sample(pool, \"method1==1 and method2==1\", k=50)\n","s4 = stratified_sample(pool, \"frequency<=2\", k=50)  # 边界低频样本\n","\n","label_df = pd.concat([s1,s2,s3,s4], ignore_index=True).drop_duplicates(subset=[\"unit\"])\n","label_df[\"label\"] = \"\"  # 你来手工填：1=真新词 / 0=非\n","label_path = out_dir / \"label_me.csv\"\n","label_df.to_csv(label_path, index=False, encoding=\"utf-8-sig\")\n","label_path\n"],"metadata":{"id":"BykQpkZtLRfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#F. 计算指标（精度 / pooling 召回 / κ）\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n","\n","# 读回你人工打标后的文件\n","labelled = pd.read_csv(out_dir / \"label_me.csv\", encoding=\"utf-8\")\n","labelled = labelled.dropna(subset=[\"label\"])\n","labelled[\"label\"] = labelled[\"label\"].astype(int)\n","\n","# 单方法预测：把方法命中当作“预测为新词”\n","pred_m1 = (labelled[\"method1\"]==1).astype(int)\n","pred_m2 = (labelled[\"method2\"]==1).astype(int)\n","y_true  = labelled[\"label\"]\n","\n","def report_prec(name, y_pred, y_true):\n","    p = precision_score(y_true, y_pred, zero_division=0)\n","    r = recall_score(y_true, y_pred, zero_division=0)  # 注意：这是在样本上的召回，不是全局召回\n","    f = f1_score(y_true, y_pred, zero_division=0)\n","    print(f\"{name:10s}  Precision={p:.3f}  Recall(sample)={r:.3f}  F1={f:.3f}\")\n","\n","report_prec(\"M1\", pred_m1, y_true)\n","report_prec(\"M2\", pred_m2, y_true)\n","report_prec(\"M1∪M2\", ((pred_m1+pred_m2)>0).astype(int), y_true)\n","report_prec(\"M1∩M2\", ((pred_m1+pred_m2)==2).astype(int), y_true)\n","\n","# Pooling 估计相对召回：把 M1∪M2 的“真”为近似全集\n","pool_true = ((pred_m1+pred_m2)>0) & (y_true==1)\n","denom = pool_true.sum() if pool_true.sum()>0 else 1\n","recall_m1_pool = ((pred_m1==1) & (y_true==1)).sum() / denom\n","recall_m2_pool = ((pred_m2==1) & (y_true==1)).sum() / denom\n","print(f\"Recall_pool  M1={recall_m1_pool:.3f}  M2={recall_m2_pool:.3f}\")\n","\n","# 若有两份标注（A/B）可算 Cohen's κ\n","# labA = pd.read_csv(out_dir / \"label_me_A.csv\")[\"label\"].astype(int)\n","# labB = pd.read_csv(out_dir / \"label_me_B.csv\")[\"label\"].astype(int)\n","# print(\"Cohen κ =\", cohen_kappa_score(labA, labB))\n"],"metadata":{"id":"iRHa57Y0LUVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#G.（可选）阈值扫描（灵敏度）\n","\n","def eval_with_thresholds(m1df, m2df, min_freq, min_doc, min_src, pmi_thres):\n","    a = m1df.query(\"frequency>=@min_freq and doc_count>=@min_doc and source_count>=@min_src\").copy()\n","    a[\"unit\"] = a[\"unit\"].astype(str); a[\"method1\"]=1\n","    b = m2df.query(\"(n>=2) and frequency>=@min_freq and doc_count>=@min_doc and pmi_adj>=@pmi_thres\").copy()\n","    b[\"unit\"] = b[\"unit\"].astype(str); b[\"method2\"]=1\n","    merged = pd.merge(labelled[[\"unit\",\"label\"]],\n","                      pd.concat([a[[\"unit\",\"method1\"]], b[[\"unit\",\"method2\"]]], ignore_index=True)\n","                      .groupby(\"unit\",as_index=False).max(),\n","                      on=\"unit\", how=\"left\").fillna(0)\n","    y_true = merged[\"label\"].astype(int)\n","    pred_m1 = (merged.get(\"method1\",0)==1).astype(int)\n","    pred_m2 = (merged.get(\"method2\",0)==1).astype(int)\n","    return {\n","        \"min_freq\":min_freq,\"min_doc\":min_doc,\"min_src\":min_src,\"pmi\":pmi_thres,\n","        \"prec_m1\": precision_score(y_true, pred_m1, zero_division=0),\n","        \"prec_m2\": precision_score(y_true, pred_m2, zero_division=0),\n","        \"prec_union\": precision_score(y_true, ((pred_m1+pred_m2)>0).astype(int), zero_division=0)\n","    }\n","\n","grid = []\n","for mf in [1,2,3,5]:\n","    for md in [1,2,3]:\n","        for ms in [1,2]:\n","            for pmi in [1.5,2.0,2.5,3.0]:\n","                grid.append(eval_with_thresholds(m1_small, m2_small, mf, md, ms, pmi))\n","pd.DataFrame(grid).sort_values(\"prec_union\", ascending=False).head(10)\n"],"metadata":{"id":"LCC1SvbuLW4x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comment interpréter & décider ?\n","\n","Précision cible（按任务需求）：\n","\n","如果要提供**高质量“新词清单”**供词典录入，倾向提高 Precision（提高阈值：min_doc/min_source/PMI）。\n","\n","做探索/召回时，放宽阈值，接受更多候选，后续人工筛。\n","\n","看分层表现：\n","\n","M1∩M2 往往精度最高；M2_only 中可能含大量“新复合词/专名”，要靠 PMI+doc_count 控噪。\n","\n","低频（freq≤2） 区域里误报多，建议单独复核。\n","\n","对齐社会时间线：抽查高频“真新词”的 first_year 是否与社会事件年份相符（增强可信度）。"],"metadata":{"id":"RkqQjkjBLgAj"}},{"cell_type":"code","source":[],"metadata":{"id":"fiIn_67_LgaZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2） 人工抽样 échantillonnage manuel"],"metadata":{"id":"D3usaTgllFdb"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","\n","# ========= 参数区 =========\n","SEED = 20250821\n","N_TARGET = 400  # 目标样本数\n","PATH_CAND = \"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/identification/ch2_néologisme_new_only.csv\"\n","# 可选：若有出现记录（为计算频次、首现年、抽上下文）\n","PATH_OCC = \"/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch2_all_data_2015_2025_tok_thulac.csv\"\n","\n","np.random.seed(SEED)\n","\n","# ========= 读取 =========\n","cand = pd.read_csv(PATH_CAND)\n","# 统一列名\n","cand = cand.rename(columns={ '词': 'term', 'token': 'term' })\n","assert 'term' in cand.columns, \"候选文件需要包含一列 term\"\n","\n","# 若有出现记录，用它计算 freq/first_year/context\n","occ = None\n","try:\n","    occ = pd.read_csv(PATH_OCC)\n","    # 根据你的实际列名改这里：\n","    occ = occ.rename(columns={'token':'term', 'year':'year', 'docid':'doc_id', 'doc_id':'doc_id', 'context':'context'})\n","    assert set(['term','year']).issubset(occ.columns)\n","except Exception as e:\n","    print(\"未加载出现记录文件，仅基于候选 term 抽样；建议提供出现记录以改进分层与上下文。\", e)\n","\n","# ========= 统计特征 =========\n","if occ is not None:\n","    # 频次\n","    freq = occ.groupby('term').size().rename('freq_total')\n","    # 首现年\n","    first_year = occ.groupby('term')['year'].min().rename('first_year')\n","    # 年度频次宽表\n","    by_year = occ.pivot_table(index='term', columns='year', values='doc_id', aggfunc='count', fill_value=0)\n","    by_year.columns = [f'freq_{int(c)}' for c in by_year.columns]\n","    # 合并\n","    stat = pd.concat([freq, first_year, by_year], axis=1).reset_index()\n","else:\n","    # 无出现记录时，简单占位\n","    stat = cand[['term']].drop_duplicates().copy()\n","    stat['freq_total'] = 1\n","    stat['first_year'] = 2015\n","\n","base = cand[['term']].drop_duplicates().merge(stat, on='term', how='left')\n","\n","# ========= 形态/启发式特征（仅用于分层覆盖）=========\n","def is_abbrev_like(t):\n","    # 两字中文或包含英文字母/数字的，视为缩略/混写候选（可按需完善）\n","    if pd.isna(t): return False\n","    t = str(t)\n","    has_latin = any('a' <= ch.lower() <= 'z' for ch in t)\n","    has_digit = any(ch.isdigit() for ch in t)\n","    return (len(t) == 2) or has_latin or has_digit\n","\n","def has_name_suffix(t):\n","    suffixes = ['市','省','县','区','镇','乡','校','大学','学院','医院','公司','集团','科技','控股','新区','局','厅','办','委','银行','证券','保险','药业','中心','研究院']\n","    return any(str(t).endswith(suf) for suf in suffixes)\n","\n","base['is_abbrev'] = base['term'].apply(is_abbrev_like)\n","base['has_name_suffix'] = base['term'].apply(has_name_suffix)\n","base['latin_or_digit'] = base['term'].apply(lambda x: any(c.isdigit() or ('a'<=c.lower()<='z') for c in str(x)))\n","\n","# ========= 频次层 =========\n","q95 = base['freq_total'].quantile(0.95) if base['freq_total'].notna().any() else 1\n","low_mask = base['freq_total'] <= 3\n","high_mask = base['freq_total'] >= q95\n","mid_mask = ~(low_mask | high_mask)\n","\n","base['freq_layer'] = np.select(\n","    [high_mask, mid_mask, low_mask],\n","    ['high','mid','low'],\n","    default='mid'\n",")\n","\n","# ========= 年份层（首现年）=========\n","def year_bucket(y):\n","    try:\n","        y = int(y)\n","    except:\n","        y = 2015\n","    if 2015 <= y <= 2017: return '2015-2017'\n","    if 2018 <= y <= 2019: return '2018-2019'\n","    if y == 2020:         return '2020'\n","    if 2021 <= y <= 2023: return '2021-2023'\n","    if 2024 <= y <= 2025: return '2024-2025'\n","    return 'unknown'\n","\n","base['year_layer'] = base['first_year'].apply(year_bucket)\n","\n","# ========= 分层配额（示例：先按频次层，再按年份层）=========\n","N_total = min(N_TARGET, len(base))\n","alloc = []\n","\n","for f_layer, df_f in base.groupby('freq_layer'):\n","    # 频次层按占比配额\n","    n_f = int(round(N_total * len(df_f) / len(base)))\n","    if n_f == 0: n_f = min(1, len(df_f))\n","\n","    # 该层内按年份层再均衡分配\n","    year_groups = list(df_f.groupby('year_layer'))\n","    # 平均分配；可按各年层大小加权\n","    weights = np.array([len(g) for _, g in year_groups], dtype=float)\n","    weights = weights / weights.sum()\n","    for (y_layer, g), w in zip(year_groups, weights):\n","        n_y = int(round(n_f * w))\n","        if n_y == 0 and len(g) > 0:\n","            n_y = 1\n","        alloc.append((f_layer, y_layer, n_y))\n","\n","# ========= 在每个（频次×年份）子层内抽样，并尽量覆盖形态特征 =========\n","samples = []\n","for (f_layer, y_layer, n_y) in alloc:\n","    sub = base[(base['freq_layer']==f_layer) & (base['year_layer']==y_layer)]\n","    if len(sub) == 0 or n_y == 0:\n","        continue\n","\n","    # 优先保障 is_abbrev / has_name_suffix / latin_or_digit 覆盖（各抽1条，如果有）\n","    picks = []\n","    for col in ['is_abbrev','has_name_suffix','latin_or_digit']:\n","        cand_sub = sub[sub[col] == True]\n","        if len(cand_sub) > 0 and len(picks) < n_y:\n","            picks.append(cand_sub.sample(n=1, random_state=SEED))\n","\n","    picked = pd.concat(picks) if picks else pd.DataFrame(columns=sub.columns)\n","    # 剩余名额随机补齐\n","    remain = n_y - len(picked)\n","    if remain > 0:\n","        pool = sub[~sub['term'].isin(picked['term'])] if len(picked)>0 else sub\n","        if len(pool) > 0:\n","            picked2 = pool.sample(n=min(remain, len(pool)), random_state=SEED)\n","            picked = pd.concat([picked, picked2])\n","    samples.append(picked)\n","\n","sample_df = pd.concat(samples).drop_duplicates(subset=['term'])\n","\n","# 若总数偏离目标，微调（全局补齐/裁剪）\n","if len(sample_df) < N_total:\n","    pool = base[~base['term'].isin(sample_df['term'])]\n","    add = pool.sample(n=min(N_total-len(sample_df), len(pool)), random_state=SEED)\n","    sample_df = pd.concat([sample_df, add]).drop_duplicates(subset=['term'])\n","elif len(sample_df) > N_total:\n","    sample_df = sample_df.sample(n=N_total, random_state=SEED)\n","\n","# ========= 填充上下文例句（最多3条）=========\n","def collect_context(term, occ_df, k=3):\n","    if occ_df is None or 'context' not in occ_df.columns:\n","        return \"\"\n","    ex = occ_df[occ_df['term']==term].dropna(subset=['context'])\n","    if len(ex)==0:\n","        return \"\"\n","    # 不同文档优先\n","    ex = ex.drop_duplicates(subset=['doc_id','context'])\n","    return \"||\".join(ex['context'].astype(str).head(k).tolist())\n","\n","if occ is not None:\n","    ctx_map = {t: collect_context(t, occ, 3) for t in sample_df['term']}\n","    sample_df['context_examples'] = sample_df['term'].map(ctx_map)\n","    # doc_id 样本\n","    doc_map = occ.groupby('term')['doc_id'].apply(lambda s: \",\".join(map(str, s.dropna().astype(str).head(5)))).to_dict()\n","    sample_df['doc_ids_samples'] = sample_df['term'].map(doc_map)\n","else:\n","    sample_df['context_examples'] = \"\"\n","    sample_df['doc_ids_samples'] = \"\"\n","\n","# ========= 生成盲审标注表（A / B）=========\n","common_cols = ['term','first_year','freq_total'] + \\\n","              [c for c in sample_df.columns if c.startswith('freq_')] + \\\n","              ['doc_ids_samples','context_examples','freq_layer','year_layer','is_abbrev','has_name_suffix','latin_or_digit']\n","\n","# 若没有年度频次列，也不影响\n","common_cols = [c for c in common_cols if c in sample_df.columns]\n","\n","annot_cols = common_cols + [\n","    'detector_source', 'candidate_rank',\n","    'label_primary', 'label_secondary', 'rationale_short', 'annotator_id', 'annotation_date'\n","]\n","\n","for col in ['detector_source','candidate_rank','label_primary','label_secondary','rationale_short','annotator_id','annotation_date']:\n","    if col not in sample_df.columns:\n","        sample_df[col] = \"\"\n","\n","annot_A = sample_df[annot_cols].copy()\n","annot_B = sample_df[annot_cols].copy()\n","\n","outdir = Path(\"/content\")\n","annot_A.to_csv(outdir/\"gold_sample_annot_A.csv\", index=False)\n","annot_B.to_csv(outdir/\"gold_sample_annot_B.csv\", index=False)\n","\n","print(\"完成！已输出：\")\n","print(outdir/\"gold_sample_annot_A.csv\")\n","print(outdir/\"gold_sample_annot_B.csv\")\n"],"metadata":{"id":"sz1yoQ0klHn-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3）黄金标准Gold Standard  \n","#annotation du jeu de référence (gold standard)\n","#constitution d’un gold standard par annotation manuelle。"],"metadata":{"id":"QcZ3oWp6lpjo"}},{"cell_type":"code","source":["# === Step 2. 统计候选 ===\n","import pandas as pd\n","from pathlib import Path\n","\n","PATH_CAND = \"/content/drive/.../identification/ch2_néologisme_new_only.csv\"\n","PATH_OCC  = \"/content/drive/.../Token/ch2_all_data_2015_2025_tok_thulac.csv\"  # 包含 term/year/doc_id/context\n","OUT_DIR   = \"/content/drive/.../gold_v1\"  # 你想存放金标文件的文件夹\n","\n","Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n","\n","cand = pd.read_csv(PATH_CAND)\n","cand = cand.rename(columns={'token':'term','词':'term'}).drop_duplicates(subset=['term'])\n","assert 'term' in cand.columns\n","\n","occ = pd.read_csv(PATH_OCC)\n","occ = occ.rename(columns={'token':'term','docid':'doc_id'})\n","need_cols = {'term','year','doc_id','context'}\n","assert need_cols.issubset(occ.columns), f\"缺列：{need_cols - set(occ.columns)}\"\n","\n","# 频次与首现年\n","freq = occ.groupby('term').size().rename('freq_total')\n","first_year = occ.groupby('term')['year'].min().rename('first_year')\n","\n","# 年度频次\n","by_year = occ.pivot_table(index='term', columns='year', values='doc_id', aggfunc='count', fill_value=0)\n","by_year.columns = [f'freq_{int(c)}' for c in by_year.columns]\n","\n","# 合并\n","stat = pd.concat([freq, first_year, by_year], axis=1).reset_index()\n","base = cand[['term']].merge(stat, on='term', how='left').fillna(0)\n","\n","# 上下文与 doc 样本\n","def contexts_for(t, k=3):\n","    sub = occ[occ['term']==t].dropna(subset=['context'])\n","    sub = sub.drop_duplicates(subset=['doc_id','context'])\n","    return \"||\".join(sub['context'].astype(str).head(k).tolist())\n","\n","def docids_for(t, k=5):\n","    sub = occ[occ['term']==t]\n","    return \",\".join(map(str, sub['doc_id'].dropna().astype(str).head(k).tolist()))\n","\n","base['context_examples'] = base['term'].apply(lambda t: contexts_for(t, 3))\n","base['doc_ids_samples']  = base['term'].apply(lambda t: docids_for(t, 5))\n","\n","base.to_csv(f\"{OUT_DIR}/candidates_with_stats.csv\", index=False)\n","print(\"OK:\", f\"{OUT_DIR}/candidates_with_stats.csv\")\n"],"metadata":{"id":"xp3XqTDilsl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Step 3. 分层抽样 ===\n","import numpy as np\n","\n","SEED = 20250821\n","N_TARGET = 400\n","\n","df = pd.read_csv(f\"{OUT_DIR}/candidates_with_stats.csv\")\n","\n","# 频次层\n","q95 = df['freq_total'].quantile(0.95)\n","low  = df['freq_total'] <= 3\n","high = df['freq_total'] >= q95\n","mid  = ~(low | high)\n","df['freq_layer'] = np.select([high, mid, low], ['high','mid','low'], default='mid')\n","\n","# 年份层\n","def y_bucket(y):\n","    y = int(y) if pd.notna(y) else 2015\n","    if 2015 <= y <= 2017: return '2015-2017'\n","    if 2018 <= y <= 2019: return '2018-2019'\n","    if y == 2020:         return '2020'\n","    if 2021 <= y <= 2023: return '2021-2023'\n","    if 2024 <= y <= 2025: return '2024-2025'\n","    return 'unknown'\n","\n","df['year_layer'] = df['first_year'].apply(y_bucket)\n","\n","# 形态覆盖用（仅用于抽样多样性，不是最终标签）\n","def abbrev_like(t):\n","    t = str(t)\n","    has_lat = any('a' <= c.lower() <= 'z' for c in t)\n","    has_dig = any(c.isdigit() for c in t)\n","    return (len(t)==2) or has_lat or has_dig\n","\n","def has_name_suffix(t):\n","    suf = ['市','省','县','区','镇','乡','校','大学','学院','医院','公司','集团','科技','控股','新区','局','厅','办','委','银行','证券','保险','药业','中心','研究院']\n","    return any(str(t).endswith(s) for s in suf)\n","\n","df['is_abbrev'] = df['term'].apply(abbrev_like)\n","df['has_name_suffix'] = df['term'].apply(has_name_suffix)\n","df['latin_or_digit'] = df['term'].apply(lambda t: any(c.isdigit() or ('a'<=c.lower()<='z') for c in str(t)))\n","\n","# 分层配额\n","np.random.seed(SEED)\n","N_total = min(N_TARGET, len(df))\n","alloc = []\n","for f, g1 in df.groupby('freq_layer'):\n","    n_f = int(round(N_total * len(g1)/len(df)))\n","    if n_f == 0: n_f = min(1, len(g1))\n","    for y, g2 in g1.groupby('year_layer'):\n","        w = len(g2) / len(g1)\n","        n_y = int(round(n_f * w)) or (1 if len(g2)>0 else 0)\n","        alloc.append((f,y,n_y))\n","\n","# 逐层抽样并尽量覆盖形态\n","picked = []\n","for f,y,n in alloc:\n","    sub = df[(df['freq_layer']==f)&(df['year_layer']==y)]\n","    if n==0 or len(sub)==0:\n","        continue\n","    sel = []\n","    for col in ['is_abbrev','has_name_suffix','latin_or_digit']:\n","        cand = sub[sub[col]==True]\n","        if len(cand)>0 and len(sel)<n:\n","            sel.append(cand.sample(1, random_state=SEED))\n","    chosen = pd.concat(sel) if sel else pd.DataFrame(columns=sub.columns)\n","    remain = n - len(chosen)\n","    if remain>0:\n","        pool = sub[~sub['term'].isin(chosen['term'])] if len(chosen)>0 else sub\n","        if len(pool)>0:\n","            chosen = pd.concat([chosen, pool.sample(min(remain, len(pool)), random_state=SEED)])\n","    picked.append(chosen)\n","\n","sample_df = pd.concat(picked).drop_duplicates(subset=['term'])\n","\n","# 全局补齐/裁剪\n","if len(sample_df) < N_total:\n","    pool = df[~df['term'].isin(sample_df['term'])]\n","    add = pool.sample(min(N_total-len(sample_df), len(pool)), random_state=SEED)\n","    sample_df = pd.concat([sample_df, add]).drop_duplicates(subset=['term'])\n","elif len(sample_df) > N_total:\n","    sample_df = sample_df.sample(N_total, random_state=SEED)\n","\n","sample_df.to_csv(f\"{OUT_DIR}/gold_sample_pool.csv\", index=False)\n","print(\"OK:\", f\"{OUT_DIR}/gold_sample_pool.csv\", len(sample_df))\n"],"metadata":{"id":"a84a0nS6lx2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Step 4. 生成 A/B 标注表 ===\n","common_cols = ['term','first_year','freq_total'] + \\\n","              [c for c in sample_df.columns if c.startswith('freq_')] + \\\n","              ['doc_ids_samples','context_examples','freq_layer','year_layer','is_abbrev','has_name_suffix','latin_or_digit']\n","\n","# 去不存在的列\n","common_cols = [c for c in common_cols if c in sample_df.columns]\n","\n","annot_cols = common_cols + [\n","    'detector_source','candidate_rank',\n","    'label_primary','label_secondary','rationale_short','annotator_id','annotation_date'\n","]\n","\n","for col in ['detector_source','candidate_rank','label_primary','label_secondary','rationale_short','annotator_id','annotation_date']:\n","    if col not in sample_df.columns:\n","        sample_df[col] = \"\"\n","\n","annot_A = sample_df[annot_cols].copy()\n","annot_B = sample_df[annot_cols].copy()\n","\n","annot_A.to_csv(f\"{OUT_DIR}/gold_sample_annot_A.csv\", index=False)\n","annot_B.to_csv(f\"{OUT_DIR}/gold_sample_annot_B.csv\", index=False)\n","print(\"OK: 生成 A/B 标注文件\")\n"],"metadata":{"id":"AHG7MGp8l13v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Step 5. 一致性与冲突清单 ===\n","import pandas as pd\n","from sklearn.metrics import cohen_kappa_score\n","\n","A_path = f\"{OUT_DIR}/gold_sample_annot_A_filled.csv\"  # A 回传后的文件\n","B_path = f\"{OUT_DIR}/gold_sample_annot_B_filled.csv\"  # B 回传后的文件\n","\n","A = pd.read_csv(A_path)\n","B = pd.read_csv(B_path)\n","\n","# 只保留 term 和标注列\n","A_ = A[['term','label_primary','label_secondary']].rename(columns={'label_primary':'label_A','label_secondary':'sec_A'})\n","B_ = B[['term','label_primary','label_secondary']].rename(columns={'label_primary':'label_B','label_secondary':'sec_B'})\n","\n","merged = A_.merge(B_, on='term', how='inner')\n","\n","# 计算 kappa（主标签）\n","kappa = cohen_kappa_score(merged['label_A'], merged['label_B'])\n","print(\"Cohen's kappa (primary):\", round(kappa, 3))\n","\n","# 可选：二值化（NEO vs 非NEO），看“是否新词”的一致性\n","binA = merged['label_A'].apply(lambda x: 'NEO' if x=='NEO' else 'NON')\n","binB = merged['label_B'].apply(lambda x: 'NEO' if x=='NEO' else 'NON')\n","kappa_bin = cohen_kappa_score(binA, binB)\n","print(\"Cohen's kappa (NEO vs NON):\", round(kappa_bin, 3))\n","\n","# 冲突清单 → 供仲裁\n","conflicts = merged[merged['label_A'] != merged['label_B']].copy()\n","conflicts = conflicts.merge(A[['term','rationale_short']], on='term', how='left').rename(columns={'rationale_short':'rationale_A'})\n","conflicts = conflicts.merge(B[['term','rationale_short']], on='term', how='left').rename(columns={'rationale_short':'rationale_B'})\n","conflicts.to_csv(f\"{OUT_DIR}/conflicts_for_adjudication.csv\", index=False)\n","print(\"OK: 冲突清单\", f\"{OUT_DIR}/conflicts_for_adjudication.csv\")\n"],"metadata":{"id":"ddep2ospl5Gh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Step 6. 生成最终金标 ===\n","conf = pd.read_csv(f\"{OUT_DIR}/conflicts_for_adjudication_filled.csv\")  # 仲裁者填好 adjudicated_label 后的文件\n","\n","# 把无冲突的项（A==B）先收集\n","agree = merged[merged['label_A'] == merged['label_B']].copy()\n","agree['adjudicated_label'] = agree['label_A']  # 直接采用一致标签\n","\n","# 合并仲裁结果\n","gold = pd.concat([\n","    agree[['term','adjudicated_label']],\n","    conf[['term','adjudicated_label']]\n","], axis=0, ignore_index=True)\n","\n","# 连接统计信息与上下文，形成完整金标表\n","gold_full = gold.merge(df, on='term', how='left')\n","gold_full.to_csv(f\"{OUT_DIR}/gold_standard_v1.csv\", index=False)\n","print(\"OK: 金标 v1\", f\"{OUT_DIR}/gold_standard_v1.csv\", len(gold_full))\n"],"metadata":{"id":"AEJZ0Efyl8xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Step 7. 评测 ===\n","from sklearn.metrics import classification_report\n","\n","# 假设你的系统输出：term + pred_label（NEO/NOT_NEO/...）\n","SYS = pd.read_csv(\"/content/drive/.../my_system_preds.csv\")  # 你自己的系统输出\n","GOLD = pd.read_csv(f\"{OUT_DIR}/gold_standard_v1.csv\").rename(columns={'adjudicated_label':'gold'})\n","\n","eval_df = GOLD.merge(SYS[['term','pred_label']], on='term', how='left').dropna(subset=['pred_label'])\n","\n","print(classification_report(eval_df['gold'], eval_df['pred_label'], digits=3))\n","\n","# 可选：二值化只看“是否新词”\n","g_bin = eval_df['gold'].apply(lambda x: 'NEO' if x=='NEO' else 'NON')\n","p_bin = eval_df['pred_label'].apply(lambda x: 'NEO' if x=='NEO' else 'NON')\n","print(\"=== NEO vs NON ===\")\n","print(classification_report(g_bin, p_bin, digits=3))\n"],"metadata":{"id":"Sll2WU02mAkV"},"execution_count":null,"outputs":[]}]}