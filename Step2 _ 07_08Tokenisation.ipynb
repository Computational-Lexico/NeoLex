{"cells":[{"cell_type":"markdown","metadata":{"id":"XaCeHrJsNfdL"},"source":["# **le corpus de référence en chinois**"]},{"cell_type":"markdown","metadata":{"id":"yntUf71jInqi"},"source":["1)现代汉语常用词表 2014.txt"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":54,"status":"error","timestamp":1755594401545,"user":{"displayName":"Alexander Delaporte","userId":"06309705023555247130"},"user_tz":-120},"id":"Br45ICV4IsJ0","outputId":"5166ed38-9420-4936-c119-bcb80f90c1e8"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/现代汉语常用词表 2014.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-351708093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/现代汉语常用词表 2014_new.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m      \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/现代汉语常用词表 2014.txt'"]}],"source":["# 文件路径\n","input_file = \"/content/现代汉语常用词表 2014.txt\"\n","output_file = \"/content/现代汉语常用词表 2014_new.txt\"\n","\n","with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n","     open(output_file, \"w\", encoding=\"utf-8\") as fout:\n","    for line in fin:\n","        # 按制表符分隔，只取第一个元素\n","        parts = line.strip().split(\"\\t\")\n","        if parts:  # 防止空行\n","            fout.write(parts[0] + \"\\n\")\n","\n","print(\"提取完成！输出文件为：\", output_file)\n"]},{"cell_type":"markdown","metadata":{"id":"YoIN1nF9V9EV"},"source":["2) 2016,现代汉语第七版.txt"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":63,"status":"error","timestamp":1755594464794,"user":{"displayName":"Alexander Delaporte","userId":"06309705023555247130"},"user_tz":-120},"id":"GTi_Cgl3V8oI","outputId":"27106f71-8d16-4405-9706-c97037b811fd"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/2016,现代汉语第七版.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2900496875.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/2016,现代汉语第七版_new.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m      \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/2016,现代汉语第七版.txt'"]}],"source":["import re\n","\n","# 输入输出路径\n","input_file = \"/content/2016,现代汉语第七版.txt\"\n","output_file = \"/content/2016,现代汉语第七版_new.txt\"\n","\n","with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n","     open(output_file, \"w\", encoding=\"utf-8\") as fout:\n","    for line in fin:\n","        # 用正则提取【】中的内容\n","        match = re.search(r\"【([^】]+)】\", line)\n","        if match:\n","            word = match.group(1)\n","            fout.write(word + \"\\n\")\n","\n","print(\"提取完成！输出文件为：\", output_file)\n"]},{"cell_type":"markdown","metadata":{"id":"hfZPJbiSXDbi"},"source":["3) 2009现代汉语词典"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1754749506786,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"},"user_tz":-120},"id":"Yew_9TasXJKb","outputId":"a82db66e-d8da-43b5-a9dc-f9ffe78da512"},"outputs":[{"name":"stdout","output_type":"stream","text":["提取完成！输出文件为： /content/2009上册_new.txt\n"]}],"source":["import re\n","\n","# 文件路径\n","input_file = \"/content/2009上册.txt\"\n","output_file = \"/content/2009上册_new.txt\"\n","\n","with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n","     open(output_file, \"w\", encoding=\"utf-8\") as fout:\n","    for line in fin:\n","        parts = line.strip().split(\"\\t\")\n","        if len(parts) >= 3:\n","            # 第三列是汉字词条，去掉非汉字字符（保留中文）\n","            hanzi = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", parts[2])\n","            if hanzi:\n","                fout.write(hanzi + \"\\n\")\n","\n","print(\"提取完成！输出文件为：\", output_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153,"status":"ok","timestamp":1754749522333,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"},"user_tz":-120},"id":"HT9fCRRtXOr0","outputId":"11942d6c-902d-4d40-bee6-82625606fbb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["提取完成！输出文件为： /content/2009下册_new.txt\n"]}],"source":["import re\n","\n","# 文件路径\n","input_file = \"/content/2009下册.txt\"\n","output_file = \"/content/2009下册_new.txt\"\n","\n","with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n","     open(output_file, \"w\", encoding=\"utf-8\") as fout:\n","    for line in fin:\n","        parts = line.strip().split(\"\\t\")\n","        if len(parts) >= 3:\n","            # 第三列是汉字词条，去掉非汉字字符（保留中文）\n","            hanzi = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", parts[2])\n","            if hanzi:\n","                fout.write(hanzi + \"\\n\")\n","\n","print(\"提取完成！输出文件为：\", output_file)\n"]},{"cell_type":"markdown","metadata":{"id":"Il5K5HUutILy"},"source":["维基百科词典\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RR1_hDSWtJrI","colab":{"base_uri":"https://localhost:8080/","height":829},"executionInfo":{"status":"error","timestamp":1755594570386,"user_tz":-120,"elapsed":2044,"user":{"displayName":"Alexander Delaporte","userId":"06309705023555247130"}},"outputId":"a949ddf8-c685-426e-e4b2-9c7e74811056"},"outputs":[{"output_type":"stream","name":"stdout","text":["开始抓取中文维基词典标题...\n"]},{"output_type":"error","ename":"JSONDecodeError","evalue":"Expecting value: line 1 column 1 (char 0)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[1;32m    513\u001b[0m             and not use_decimal and not allow_nan and not kw):\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4283490248.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"allpages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;31m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"]}],"source":["import requests\n","import json\n","from datetime import datetime\n","\n","API_URL = \"https://zh.wiktionary.org/w/api.php\"\n","CUTOFF_DATE = datetime(2015, 1, 1)\n","OUTPUT_FILE = \"wiktionary_zh_pre2015.jsonl\"\n","\n","titles = []\n","apcontinue = None\n","\n","print(\"开始抓取中文维基词典标题...\")\n","\n","# 1. 获取所有主命名空间的标题\n","while True:\n","    params = {\n","        \"action\": \"query\",\n","        \"list\": \"allpages\",\n","        \"apnamespace\": 0,   # 主命名空间\n","        \"aplimit\": \"500\",\n","        \"format\": \"json\"\n","    }\n","    if apcontinue:\n","        params[\"apcontinue\"] = apcontinue\n","\n","    r = requests.get(API_URL, params=params)\n","    data = r.json()\n","\n","    for page in data[\"query\"][\"allpages\"]:\n","        titles.append(page[\"title\"])\n","\n","    if \"continue\" in data:\n","        apcontinue = data[\"continue\"][\"apcontinue\"]\n","    else:\n","        break\n","\n","print(f\"总共抓到 {len(titles)} 个词条标题，开始过滤 2015 年以前的...\")\n","\n","# 2. 过滤出 2015 年以前创建的词条\n","pre2015_titles = []\n","for i, title in enumerate(titles, 1):\n","    params = {\n","        \"action\": \"query\",\n","        \"prop\": \"revisions\",\n","        \"titles\": title,\n","        \"rvlimit\": 1,\n","        \"rvdir\": \"newer\",  # 最早修订\n","        \"rvprop\": \"timestamp\",\n","        \"format\": \"json\"\n","    }\n","    r = requests.get(API_URL, params=params)\n","    pages = r.json()[\"query\"][\"pages\"]\n","\n","    for _, page_data in pages.items():\n","        if \"revisions\" in page_data:\n","            ts = page_data[\"revisions\"][0][\"timestamp\"]\n","            created_time = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n","            if created_time < CUTOFF_DATE:\n","                pre2015_titles.append(title)\n","\n","    if i % 500 == 0:\n","        print(f\"已检查 {i}/{len(titles)}...\")\n","\n","# 3. 写入 JSONL 文件\n","with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n","    for t in sorted(pre2015_titles):\n","        f.write(json.dumps({\"text\": t, \"source\": \"wiktionary\"}, ensure_ascii=False) + \"\\n\")\n","\n","print(f\"完成！共保存 {len(pre2015_titles)} 个 2015 年以前的中文 Wiktionary 词条到 {OUTPUT_FILE}\")\n"]},{"cell_type":"markdown","source":["#Fusionner le corpus"],"metadata":{"id":"qzJx2qkM9HBp"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"TlUmYGCUmeBI","executionInfo":{"status":"error","timestamp":1755594753988,"user_tz":-120,"elapsed":3625,"user":{"displayName":"Alexander Delaporte","userId":"06309705023555247130"}},"outputId":"b03490a9-6216-47f1-adb4-696ed3895848","colab":{"base_uri":"https://localhost:8080/","height":311}},"execution_count":4,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"code","source":["#步骤 1：将所有 CSV 文件合并成一个文件\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"ZGo3_Zov9IHj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755437704261,"user_tz":-120,"elapsed":22305,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"12a7e6f7-ad36-45ee-eb01-1d4390a180d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# 递归合并（带分隔符/编码自适应）+ 详尽报告\n","import pandas as pd\n","import os, time, re\n","from tqdm import tqdm\n","\n","start = time.time()\n","\n","FOLDER = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Avant-Token/ch_corpus'\n","OUT    = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_all_data_2015_2025.csv'\n","REPORT = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_merge_report.csv'\n","\n","REQUIRED_COLS = ['author','title','date','content','url']  # 你的目标列\n","\n","def read_csv_robust(path):\n","    \"\"\"返回 (df, used_sep, used_enc, err)\"\"\"\n","    seps_try = [None, ',', ';', '\\t', '|']   # None=自动嗅探（engine='python'）\n","    encs_try = ['utf-8', 'utf-8-sig', 'latin1']\n","    last_err = None\n","    for enc in encs_try:\n","        for sep in seps_try:\n","            try:\n","                df = pd.read_csv(\n","                    path,\n","                    sep=sep,\n","                    engine='python',\n","                    encoding=enc,\n","                    on_bad_lines='skip'\n","                )\n","                return df, (sep if sep is not None else 'auto'), enc, None\n","            except Exception as e:\n","                last_err = str(e)\n","                continue\n","    return pd.DataFrame(), 'fail', 'fail', last_err\n","\n","if not os.path.exists(FOLDER):\n","    raise FileNotFoundError(f\"路径不存在: {FOLDER}\")\n","\n","# 收集 CSV\n","csv_files = []\n","for root, _, files in os.walk(FOLDER):\n","    for f in files:\n","        if f.lower().endswith('.csv'):\n","            csv_files.append(os.path.join(root, f))\n","print(f\"共找到 {len(csv_files)} 个 CSV 文件\\n\")\n","\n","# 读取 + 报告\n","records = []\n","dfs = []\n","\n","for fp in tqdm(csv_files, desc=\"读取 CSV\", unit=\"文件\"):\n","    df, used_sep, used_enc, err = read_csv_robust(fp)\n","\n","    # 标注来源、路径年份提示\n","    source = 'unknown'\n","    for s in ['Kenh','Genk','Cafebize']:\n","        if re.search(rf'/{s}/', fp, flags=re.I):\n","            source = s.lower()\n","            break\n","    y_hint = None\n","    y = re.search(r'(20[1-2]\\d)', fp)\n","    if y: y_hint = y.group(1)\n","\n","    # 统一列名（去空格小写），避免“Date”“URL”之类大小写不一致\n","    old_cols = df.columns.tolist()\n","    df.columns = [c.strip() for c in df.columns]\n","    lower_map = {c: c.lower() for c in df.columns}\n","    df.rename(columns=lower_map, inplace=True)\n","\n","    row_count = len(df)\n","    has_cols = {c: (c in df.columns) for c in REQUIRED_COLS}\n","\n","    records.append({\n","        'file': fp,\n","        'rows': row_count,\n","        'used_sep': used_sep,\n","        'used_encoding': used_enc,\n","        'has_author': has_cols['author'],\n","        'has_title': has_cols['title'],\n","        'has_date': has_cols['date'],\n","        'has_content': has_cols['content'],\n","        'has_url': has_cols['url'],\n","        'source': source,\n","        'year_hint': y_hint,\n","        'error': '' if err is None else err[:120]\n","    })\n","\n","    # 只要不是“完全空表”就先收进来；后续再统一处理日期\n","    if row_count > 0:\n","        # 缺的列补空，确保结构统一\n","        for c in REQUIRED_COLS:\n","            if c not in df.columns:\n","                df[c] = None\n","        df['source'] = source\n","        df['year_hint'] = y_hint\n","        dfs.append(df[REQUIRED_COLS + ['source','year_hint']])\n","\n","# 输出报告\n","report_df = pd.DataFrame.from_records(records)\n","report_df.to_csv(REPORT, index=False)\n","print(f\" 读取报告已保存：{REPORT}\")\n","print(\"—— Top 5 读取最少行的文件 ——\")\n","print(report_df.sort_values('rows').head(5)[['file','rows','used_sep','used_encoding','error']])\n","\n","if not dfs:\n","    raise ValueError(\"所有文件都读空了，请检查报告 merge_report.csv\")\n","\n","# 合并\n","all_data = pd.concat(dfs, ignore_index=True)\n","print(f\"合并后总行数：{len(all_data):,}\")\n","\n","# 日期解析（尽量保留；只对成功解析的行做年份过滤，失败的行也保留）\n","def try_parse_date(s):\n","    if pd.isna(s): return pd.NaT\n","    s = str(s).strip()\n","    for fmt in (\"%d/%m/%Y %I:%M %p\",\"%d/%m/%Y %H:%M\",\"%d/%m/%Y\",\n","                \"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d %H:%M\",\"%Y-%m-%d\",\n","                \"%Y/%m/%d %H:%M:%S\",\"%Y/%m/%d\"):\n","        try: return pd.to_datetime(s, format=fmt)\n","        except: pass\n","    return pd.to_datetime(s, dayfirst=True, errors='coerce')\n","\n","all_data['date_parsed'] = all_data['date'].apply(try_parse_date)\n","ok = all_data['date_parsed'].notna().sum()\n","bad = len(all_data) - ok\n","print(f\"日期解析成功：{ok:,} 行；失败：{bad:,} 行\")\n","\n","# 仅对成功解析的行做 2015–2025 过滤；解析失败的行保留（方便后续人工修正）\n","mask_ok = all_data['date_parsed'].between('2015-01-01','2025-12-31', inclusive='both')\n","mask_bad = all_data['date_parsed'].isna()\n","final = all_data[mask_ok | mask_bad].copy()\n","print(f\"筛选后（保留无效日期）：{len(final):,} 行\")\n","\n","final.to_csv(OUT, index=False)\n","print(f\"已保存：{OUT}    用时 {time.time()-start:.2f}s\")\n"],"metadata":{"id":"L7GDOFpR9O-X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755099483368,"user_tz":-120,"elapsed":597275,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"28faedb4-be57-4023-b536-b98b9b1e22ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["共找到 1262 个 CSV 文件\n","\n"]},{"output_type":"stream","name":"stderr","text":["读取 CSV: 100%|██████████| 1262/1262 [09:51<00:00,  2.13文件/s]\n"]},{"output_type":"stream","name":"stdout","text":[" 读取报告已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_merge_report.csv\n","—— Top 5 读取最少行的文件 ——\n","                                                   file  rows used_sep  \\\n","1240  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","1241  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","1242  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","1243  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","1244  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","\n","     used_encoding error  \n","1240         utf-8        \n","1241         utf-8        \n","1242         utf-8        \n","1243         utf-8        \n","1244         utf-8        \n","合并后总行数：1,262\n","日期解析成功：899 行；失败：363 行\n","筛选后（保留无效日期）：1,262 行\n","已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_all_data_2015_2025.csv    用时 597.02s\n"]}]},{"cell_type":"code","source":["#改正过后的代码\n","\n","\n","# 递归合并（带分隔符/编码自适应）+ 强化日期解析 + 基础去重 + 详尽报告\n","import pandas as pd\n","import numpy as np\n","import os, time, re\n","from tqdm import tqdm\n","\n","start = time.time()\n","\n","# === 路径设置（与原脚本一致） ===\n","FOLDER = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Avant-Token/ch_corpus'\n","OUT    = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_all_data_2015_2025.csv'\n","REPORT = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_merge_report.csv'\n","REPORT_STATS = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_merge_report_stats.csv'\n","\n","REQUIRED_COLS = ['author','title','date','content','url']  # 你的目标列\n","\n","# === 读取函数：更多编码与分隔符 + 自动嗅探 ===\n","def read_csv_robust(path):\n","    \"\"\"返回 (df, used_sep, used_enc, err)\"\"\"\n","    encs_try = ['utf-8', 'utf-8-sig', 'gb18030', 'gbk', 'big5', 'cp1252', 'latin1']\n","    seps_try = [None, ',', '\\t', ';', '|']   # None=limited sniffing\n","    last_err = None\n","    for enc in encs_try:\n","        for sep in seps_try:\n","            try:\n","                df = pd.read_csv(\n","                    path,\n","                    sep=sep,\n","                    engine='python',\n","                    encoding=enc,\n","                    on_bad_lines='skip'\n","                )\n","                if df is None:\n","                    continue\n","                return df, (sep if sep is not None else 'auto'), enc, None\n","            except Exception as e:\n","                last_err = str(e)\n","                continue\n","    return pd.DataFrame(), 'fail', 'fail', last_err\n","\n","# === 日期解析增强：中文/时间戳/ISO/RFC 3339/常见格式 ===\n","_ts10 = re.compile(r'^\\d{10}$')   # 秒\n","_ts13 = re.compile(r'^\\d{13}$')   # 毫秒\n","_just_digits = re.compile(r'^\\d+$')\n","\n","def normalize_cn_date(s: str) -> str:\n","    \"\"\"把中文日期部件替换成统一符号；去掉中文‘日’/‘号’/星期等；统一标点\"\"\"\n","    s = s.strip()\n","    # 去中文星期\n","    s = re.sub(r'星期[一二三四五六日天]|周[一二三四五六日天]', '', s)\n","    # 年月日替换\n","    s = s.replace('年','-').replace('月','-').replace('日','').replace('号','')\n","    # 全角与杂字符\n","    s = s.replace('\\u3000',' ').replace('：',':').replace('．','.')\n","    s = re.sub(r'\\s+', ' ', s)\n","    return s\n","\n","def try_parse_date(val):\n","    if pd.isna(val):\n","        return pd.NaT\n","    s = str(val).strip()\n","    if not s:\n","        return pd.NaT\n","\n","    # 纯数字时间戳\n","    if _just_digits.match(s):\n","        try:\n","            if _ts13.match(s):  # 毫秒\n","                return pd.to_datetime(int(s), unit='ms', utc=False)\n","            if _ts10.match(s):  # 秒\n","                return pd.to_datetime(int(s), unit='s', utc=False)\n","        except:\n","            pass\n","\n","    # 规范化中文日期\n","    s_norm = normalize_cn_date(s)\n","\n","    # 先按常见格式试\n","    fmts = [\n","        \"%Y-%m-%d %H:%M:%S\",\n","        \"%Y-%m-%d %H:%M\",\n","        \"%Y-%m-%d\",\n","        \"%Y/%m/%d %H:%M:%S\",\n","        \"%Y/%m/%d %H:%M\",\n","        \"%Y/%m/%d\",\n","        \"%Y.%m.%d %H:%M:%S\",\n","        \"%Y.%m.%d\",\n","        \"%d/%m/%Y %H:%M:%S\",\n","        \"%d/%m/%Y %H:%M\",\n","        \"%d/%m/%Y\",\n","        \"%m/%d/%Y %H:%M:%S\",\n","        \"%m/%d/%Y %H:%M\",\n","        \"%m/%d/%Y\",\n","    ]\n","    for fmt in fmts:\n","        try:\n","            return pd.to_datetime(s_norm, format=fmt)\n","        except:\n","            pass\n","\n","    # 最后宽松解析（含 ISO8601/RFC 3339、带时区等；dayfirst=True）\n","    try:\n","        return pd.to_datetime(s_norm, errors='coerce', utc=False, infer_datetime_format=False, dayfirst=True)\n","    except:\n","        return pd.NaT\n","\n","# === 基础工具 ===\n","def detect_source_from_path(fp: str) -> str:\n","    # 你之前的规则保留，同时从路径抽取一个可读的上级目录作为回退\n","    for s in ['Kenh','Genk','Cafebize']:\n","        if re.search(rf'/{s}/', fp, flags=re.I):\n","            return s.lower()\n","    # 回退：取倒数第2或第1级目录名做 source\n","    parts = [p for p in fp.split(os.sep) if p]\n","    if len(parts) >= 2:\n","        return parts[-2].lower()\n","    return 'unknown'\n","\n","def normalize_url(u):\n","    if pd.isna(u):\n","        return u\n","    u = str(u).strip()\n","    if not u:\n","        return u\n","    # 简单归一化：小写 + 去掉末尾斜杠\n","    u = u.lower()\n","    u = re.sub(r'//+', '/', u)  # 多斜杠\n","    u = re.sub(r'^https:/','https://',u)\n","    u = re.sub(r'^http:/','http://',u)\n","    u = u.rstrip('/')\n","    return u\n","\n","# === 主流程 ===\n","if not os.path.exists(FOLDER):\n","    raise FileNotFoundError(f\"路径不存在: {FOLDER}\")\n","\n","# 收集 CSV\n","csv_files = []\n","for root, _, files in os.walk(FOLDER):\n","    for f in files:\n","        if f.lower().endswith('.csv'):\n","            csv_files.append(os.path.join(root, f))\n","print(f\"共找到 {len(csv_files)} 个 CSV 文件\\n\")\n","\n","# 读取 + 报告\n","records = []\n","dfs = []\n","\n","for fp in tqdm(csv_files, desc=\"读取 CSV\", unit=\"文件\"):\n","    df, used_sep, used_enc, err = read_csv_robust(fp)\n","\n","    # 标注来源、路径年份提示\n","    source = detect_source_from_path(fp)\n","    y_hint = None\n","    y = re.search(r'(20[1-2]\\d)', fp)\n","    if y:\n","        y_hint = y.group(1)\n","\n","    # 统一列名（去空格+原样；再映射到小写）\n","    old_cols = df.columns.tolist()\n","    df.columns = [c.strip() for c in df.columns]\n","    lower_map = {c: c.lower() for c in df.columns}\n","    df.rename(columns=lower_map, inplace=True)\n","\n","    row_count = len(df)\n","    has_cols = {c: (c in df.columns) for c in REQUIRED_COLS}\n","\n","    records.append({\n","        'file': fp,\n","        'rows': row_count,\n","        'used_sep': used_sep,\n","        'used_encoding': used_enc,\n","        'has_author': has_cols['author'],\n","        'has_title': has_cols['title'],\n","        'has_date': has_cols['date'],\n","        'has_content': has_cols['content'],\n","        'has_url': has_cols['url'],\n","        'source': source,\n","        'year_hint': y_hint,\n","        'error': '' if err is None else err[:200]\n","    })\n","\n","    # 只要不是“完全空表”就收；缺列补 None，确保结构统一\n","    if row_count > 0:\n","        for c in REQUIRED_COLS:\n","            if c not in df.columns:\n","                df[c] = None\n","        df['source'] = source\n","        df['year_hint'] = y_hint\n","        # 保留基础列\n","        dfs.append(df[REQUIRED_COLS + ['source','year_hint']])\n","\n","# 输出读取报告（逐文件）\n","report_df = pd.DataFrame.from_records(records)\n","report_df.to_csv(REPORT, index=False, encoding='utf-8')\n","print(f\" 读取报告已保存：{REPORT}\")\n","\n","print(\"—— Top 5 读取最少行的文件 ——\")\n","if not report_df.empty:\n","    print(report_df.sort_values('rows').head(5)[['file','rows','used_sep','used_encoding','error']])\n","\n","if not dfs:\n","    raise ValueError(\"所有文件都读空了，请检查读取报告（merge_report.csv）\")\n","\n","# 合并\n","all_data = pd.concat(dfs, ignore_index=True)\n","print(f\"合并后总行数：{len(all_data):,}\")\n","\n","# 轻度清洗：去首尾空白\n","for c in REQUIRED_COLS:\n","    all_data[c] = all_data[c].astype(str).str.strip().replace({'': np.nan})\n","\n","# 解析日期\n","all_data['date_parsed'] = all_data['date'].apply(try_parse_date)\n","ok = all_data['date_parsed'].notna().sum()\n","bad = len(all_data) - ok\n","print(f\"日期解析成功：{ok:,} 行；失败：{bad:,} 行\")\n","\n","# 可选：对解析失败但有 year_hint 的行做“软补年”，便于年度统计（可追溯）\n","mask_bad = all_data['date_parsed'].isna()\n","has_yhint = all_data['year_hint'].notna()\n","fill_mask = mask_bad & has_yhint\n","all_data['date_filled'] = all_data['date_parsed']\n","all_data.loc[fill_mask, 'date_filled'] = pd.to_datetime(\n","    all_data.loc[fill_mask, 'year_hint'].astype(str) + '-01-01', errors='coerce'\n",")\n","\n","# 仅对成功解析的行做 2015–2025 过滤；解析失败的行保留（与你原策略一致）\n","mask_ok = all_data['date_parsed'].between('2015-01-01','2025-12-31', inclusive='both')\n","final = all_data[mask_ok | mask_bad].copy()\n","print(f\"筛选后（保留无效日期）：{len(final):,} 行\")\n","\n","# ===== 基础去重 =====\n","# 1) URL 归一化后去重（优先）\n","final['url_norm'] = final['url'].apply(normalize_url)\n","before = len(final)\n","final = final.drop_duplicates(subset=['url_norm'], keep='first')\n","after = len(final)\n","print(f\"按 URL 去重：移除 {before - after:,} 行，剩余 {after:,} 行\")\n","\n","# 2) 兜底：对有 title 且有 date_filled 的再按 (title, date_filled) 去重\n","mask_title = final['title'].notna() & (final['title'].str.len() > 0)\n","mask_dtok = final['date_filled'].notna()\n","before = len(final)\n","final = final.sort_values(by=['date_filled','title']).drop_duplicates(subset=['title','date_filled'], keep='first')\n","after2 = len(final)\n","print(f\"按 (title, date_filled) 兜底去重：再移除 {before - after2:,} 行，剩余 {after2:,} 行\")\n","\n","# 导出主结果\n","# 列顺序：核心列 + 解析列 + 归一化/辅助列\n","cols_out = REQUIRED_COLS + ['source','year_hint','date_parsed','date_filled','url_norm']\n","# 若某列不存在（极端情况），跳过\n","cols_out = [c for c in cols_out if c in final.columns]\n","final.to_csv(OUT, index=False, encoding='utf-8')\n","print(f\"已保存：{OUT}    用时 {time.time()-start:.2f}s\")\n","\n","# ===== 统计报告（更可读的质量视图）=====\n","stats = {}\n","\n","# 1) 总览\n","stats['total_files'] = [len(csv_files)]\n","stats['total_rows_after_concat'] = [len(all_data)]\n","stats['date_parse_ok'] = [ok]\n","stats['date_parse_bad'] = [bad]\n","stats['rows_after_filter_keep_bad'] = [len(all_data[mask_ok | mask_bad])]\n","stats['rows_after_dedup'] = [len(final)]\n","\n","# 2) 编码/分隔符分布（基于逐文件报告）\n","enc_dist = (report_df['used_encoding']\n","            .value_counts(dropna=False)\n","            .rename_axis('encoding').reset_index(name='files'))\n","sep_dist = (report_df['used_sep']\n","            .value_counts(dropna=False)\n","            .rename_axis('sep').reset_index(name='files'))\n","\n","# 3) 来源的日期命中率（基于逐行）\n","src_ok = final.assign(_ok=final['date_parsed'].notna()).groupby('source')['_ok'].agg(['sum','count'])\n","src_ok['ok_rate'] = (src_ok['sum'] / src_ok['count']).round(4)\n","src_ok = src_ok.reset_index().rename(columns={'sum':'ok_rows','count':'total_rows'})\n","\n","# 4) 年度分布（优先用 date_parsed，其次用 date_filled）\n","year_series = pd.Series(pd.NaT, index=final.index)\n","year_series.loc[final['date_parsed'].notna()] = final.loc[final['date_parsed'].notna(), 'date_parsed']\n","year_series.loc[final['date_parsed'].isna() & final['date_filled'].notna()] = final.loc[final['date_parsed'].isna() & final['date_filled'].notna(), 'date_filled']\n","year = pd.to_datetime(year_series, errors='coerce').dt.year\n","year_dist = year.value_counts(dropna=False).sort_index().rename_axis('year').reset_index(name='rows')\n","\n","# 5) 缺列概览（按文件）\n","missing_cols = []\n","for c in REQUIRED_COLS:\n","    missing_cols.append(\n","        report_df[~report_df[f'has_{c}']].shape[0]\n","    )\n","missing_df = pd.DataFrame({\n","    'col': REQUIRED_COLS,\n","    'files_missing': missing_cols\n","})\n","\n","# 6) 写出 stats 到 CSV（多表合并为一个文件，添加 tag 区分）\n","def tag_df(df, tag):\n","    out = df.copy()\n","    out.insert(0, '_section', tag)\n","    return out\n","\n","overview_df = pd.DataFrame(stats)\n","overview_df = tag_df(overview_df, 'overview')\n","\n","enc_dist = tag_df(enc_dist, 'encoding_dist')\n","sep_dist = tag_df(sep_dist, 'sep_dist')\n","src_ok = tag_df(src_ok, 'source_date_ok')\n","year_dist = tag_df(year_dist, 'year_dist')\n","missing_df = tag_df(missing_df, 'missing_required_cols')\n","\n","stats_out = pd.concat([overview_df, enc_dist, sep_dist, src_ok, year_dist, missing_df], ignore_index=True)\n","stats_out.to_csv(REPORT_STATS, index=False, encoding='utf-8')\n","print(f\"统计报告已保存：{REPORT_STATS}\")\n","\n","# 屏幕上再打印些关键信息\n","print(\"\\n—— 编码分布（前10）——\")\n","print(enc_dist.head(10))\n","print(\"\\n—— 分隔符分布（前10）——\")\n","print(sep_dist.head(10))\n","print(\"\\n—— 各来源日期命中率（前10）——\")\n","print(src_ok.sort_values('ok_rate', ascending=False).head(10))\n","print(\"\\n—— 年度分布 ——\")\n","print(year_dist)\n","print(\"\\n—— 必需列缺失（按文件计数）——\")\n","print(missing_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxEmOX5eJAya","executionInfo":{"status":"ok","timestamp":1755101937750,"user_tz":-120,"elapsed":7505,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"aa91c022-def5-4e27-8ff9-935b87c096ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["共找到 1286 个 CSV 文件\n","\n"]},{"output_type":"stream","name":"stderr","text":["读取 CSV: 100%|██████████| 1286/1286 [00:06<00:00, 191.53文件/s]\n"]},{"output_type":"stream","name":"stdout","text":[" 读取报告已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_merge_report.csv\n","—— Top 5 读取最少行的文件 ——\n","                                                   file  rows used_sep  \\\n","1272  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","1273  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","1274  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","1275  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","1276  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","\n","     used_encoding error  \n","1272         utf-8        \n","1273         utf-8        \n","1274         utf-8        \n","1275         utf-8        \n","1276         utf-8        \n","合并后总行数：1,286\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1557293574.py:106: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n","  return pd.to_datetime(s_norm, errors='coerce', utc=False, infer_datetime_format=False, dayfirst=True)\n"]},{"output_type":"stream","name":"stdout","text":["日期解析成功：945 行；失败：341 行\n","筛选后（保留无效日期）：1,286 行\n","按 URL 去重：移除 775 行，剩余 511 行\n","按 (title, date_filled) 兜底去重：再移除 4 行，剩余 507 行\n","已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_all_data_2015_2025.csv    用时 7.42s\n","统计报告已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_merge_report_stats.csv\n","\n","—— 编码分布（前10）——\n","        _section encoding  files\n","0  encoding_dist    utf-8   1286\n","\n","—— 分隔符分布（前10）——\n","   _section   sep  files\n","0  sep_dist  auto   1286\n","\n","—— 各来源日期命中率（前10）——\n","          _section source  ok_rows  total_rows  ok_rate\n","9   source_date_ok   2024       35          35   1.0000\n","10  source_date_ok   2025       25          25   1.0000\n","4   source_date_ok   2019       41          42   0.9762\n","5   source_date_ok   2020       33          39   0.8462\n","8   source_date_ok   2023       27          36   0.7500\n","7   source_date_ok   2022       24          43   0.5581\n","6   source_date_ok   2021       21          39   0.5385\n","3   source_date_ok   2018       24          45   0.5333\n","0   source_date_ok   2015       42          82   0.5122\n","2   source_date_ok   2017       22          64   0.3438\n","\n","—— 年度分布 ——\n","     _section  year  rows\n","0   year_dist  2015    83\n","1   year_dist  2016    54\n","2   year_dist  2017    63\n","3   year_dist  2018    42\n","4   year_dist  2019    41\n","5   year_dist  2020    46\n","6   year_dist  2021    37\n","7   year_dist  2022    44\n","8   year_dist  2023    39\n","9   year_dist  2024    32\n","10  year_dist  2025    26\n","\n","—— 必需列缺失（按文件计数）——\n","                _section      col  files_missing\n","0  missing_required_cols   author           1286\n","1  missing_required_cols    title              0\n","2  missing_required_cols     date              0\n","3  missing_required_cols  content              0\n","4  missing_required_cols      url            750\n"]}]},{"cell_type":"code","source":["#第二次更改版本\n","\n","\n","import pandas as pd\n","import os, time, re\n","from tqdm import tqdm\n","\n","start = time.time()\n","\n","FOLDER = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Avant-Token/ch_corpus'\n","OUT    = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch1_all_data_2015_2025.csv'\n","REPORT = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch1_merge_report.csv'\n","\n","REQUIRED_COLS = ['author','title','date','content','url']\n","\n","def read_csv_robust(path):\n","    seps_try = [None, ',', ';', '\\t', '|']\n","    encs_try = ['utf-8', 'utf-8-sig', 'latin1']\n","    for enc in encs_try:\n","        for sep in seps_try:\n","            try:\n","                df = pd.read_csv(path, sep=sep, engine='python', encoding=enc, on_bad_lines='skip')\n","                return df, (sep if sep else 'auto'), enc, None\n","            except Exception as e:\n","                last_err = str(e)\n","    return pd.DataFrame(), 'fail', 'fail', last_err\n","\n","if not os.path.exists(FOLDER):\n","    raise FileNotFoundError(f\"路径不存在: {FOLDER}\")\n","\n","csv_files = [os.path.join(root, f) for root, _, files in os.walk(FOLDER) for f in files if f.lower().endswith('.csv')]\n","print(f\"共找到 {len(csv_files)} 个 CSV 文件\\n\")\n","\n","records, dfs = [], []\n","\n","for fp in tqdm(csv_files, desc=\"读取 CSV\", unit=\"文件\"):\n","    df, used_sep, used_enc, err = read_csv_robust(fp)\n","\n","    # 去掉不可见字符（防止 BOM / 全角空格）\n","    df.columns = [c.strip().replace('\\ufeff','') for c in df.columns]\n","    lower_map = {c: c.lower() for c in df.columns}\n","    df.rename(columns=lower_map, inplace=True)\n","\n","    row_count = len(df)\n","    has_cols = {c: (c in df.columns) for c in REQUIRED_COLS}\n","\n","    records.append({\n","        'file': fp, 'rows': row_count,\n","        'used_sep': used_sep, 'used_encoding': used_enc,\n","        **{f'has_{c}': has_cols[c] for c in REQUIRED_COLS},\n","        'error': '' if err is None else err[:120]\n","    })\n","\n","    if row_count > 0:\n","        for c in REQUIRED_COLS:\n","            if c not in df.columns:\n","                df[c] = None\n","        dfs.append(df[REQUIRED_COLS])\n","\n","report_df = pd.DataFrame.from_records(records)\n","report_df.to_csv(REPORT, index=False)\n","print(f\"读取报告已保存：{REPORT}\")\n","\n","if not dfs:\n","    raise ValueError(\"所有文件都读空了，请检查 merge_report.csv\")\n","\n","all_data = pd.concat(dfs, ignore_index=True)\n","\n","# 日期解析\n","def try_parse_date(s):\n","    if pd.isna(s): return pd.NaT\n","    s = str(s).strip()\n","    for fmt in (\"%d/%m/%Y %I:%M %p\",\"%d/%m/%Y %H:%M\",\"%d/%m/%Y\",\n","                \"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d %H:%M\",\"%Y-%m-%d\",\n","                \"%Y/%m/%d %H:%M:%S\",\"%Y/%m/%d\"):\n","        try:\n","            return pd.to_datetime(s, format=fmt)\n","        except:\n","            pass\n","    return pd.to_datetime(s, dayfirst=True, errors='coerce')\n","\n","all_data['date'] = pd.to_datetime(all_data['date'], errors='coerce')\n","\n","mask_ok = all_data['date'].between('2015-01-01','2025-12-31', inclusive='both')\n","mask_bad = all_data['date'].isna()\n","final = all_data[mask_ok | mask_bad].copy()\n","\n","final.to_csv(OUT, index=False)\n","print(f\"已保存：{OUT}    用时 {time.time()-start:.2f}s\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zI3XWOoWKQub","executionInfo":{"status":"ok","timestamp":1755101917924,"user_tz":-120,"elapsed":18458,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"14fa3830-ebce-46ef-f8d9-5829c4e8fd75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["共找到 1286 个 CSV 文件\n","\n"]},{"output_type":"stream","name":"stderr","text":["读取 CSV: 100%|██████████| 1286/1286 [00:17<00:00, 72.02文件/s] \n","/tmp/ipython-input-1832548863.py:82: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  all_data['date'] = pd.to_datetime(all_data['date'], errors='coerce')\n"]},{"output_type":"stream","name":"stdout","text":["读取报告已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch1_merge_report.csv\n","已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch1_all_data_2015_2025.csv    用时 18.42s\n"]}]},{"cell_type":"markdown","metadata":{"id":"wHoqnCKxMPeZ"},"source":["#Tokenisation en chinois\n","\n"]},{"cell_type":"code","source":["#Thulac\n","\n","!pip -q install thulac tqdm pandas"],"metadata":{"id":"hvon9m1P9TNp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755437963807,"user_tz":-120,"elapsed":7937,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"a2d8947a-58c1-4520-b459-912baec2c324"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import thulac\n","from tqdm import tqdm\n","\n","# 1) 初始化（seg_only=True 只分词不标注词性，快）\n","lac = thulac.thulac(seg_only=True)  # 第一次调用会加载模型\n","\n","# 2) 路径与读取\n","IN_CSV  = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch1_all_data_2015_2025.csv'\n","OUT_CSV = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch1_all_data_2015_2025_tok_thulac.csv'\n","\n","df = pd.read_csv(IN_CSV, encoding='utf-8')\n","if 'content' not in df.columns:\n","    raise ValueError(\"CSV 里没有 'content' 列，请确认。\")\n","\n","# 3) 分词函数\n","def zh_tokenize_thulac(text: str) -> str:\n","    if pd.isna(text):\n","        return \"\"\n","    s = str(text).strip()\n","    if not s:\n","        return \"\"\n","    # lac.cut(s, text=True) 直接返回分好词的字符串，默认用空格分开\n","    return lac.cut(s, text=True)\n","\n","# 4) 批处理\n","tqdm.pandas(desc=\"THULAC 分词\")\n","df['content_tok'] = df['content'].progress_apply(zh_tokenize_thulac)\n","\n","# 5) 导出\n","df.to_csv(OUT_CSV, index=False, encoding='utf-8')\n","print(f\"已保存：{OUT_CSV}\\n预览：\")\n","print(df[['author','title','date','content_tok']].head(3))"],"metadata":{"id":"N0DW1WT1yYIv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755103817713,"user_tz":-120,"elapsed":42539,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"2e75f9d7-474f-43d3-887d-a6762f47ed9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded succeed\n"]},{"output_type":"stream","name":"stderr","text":["THULAC 分词: 100%|██████████| 1282/1282 [00:41<00:00, 31.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch1_all_data_2015_2025_tok_thulac.csv\n","预览：\n","     author title date                                        content_tok\n","0  (王连香、高雷)   NaN  NaN  人民网 北京 9月 28日 电 （ 记者 王 连 香 ） 在 今日 举行 的 交通 运输部 ...\n","1  (王连香、高雷)   NaN  NaN  人民网 北京 9月 28日 电 （ 记者 王 连 香 ） 据 中国 国 家 铁路 集团 有限...\n","2  (袁勃、胡永秋)   NaN  NaN  新华社 北京 9月 28日 电 （ 记者 冯歆然 ） 外交部 发言人 汪文斌 28日 表示 ...\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import thulac\n","from tqdm import tqdm\n","\n","# 1) 初始化（seg_only=True 只分词不标注词性，快）\n","lac = thulac.thulac(seg_only=True)  # 第一次调用会加载模型\n","\n","# 2) 路径与读取\n","IN_CSV  = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch1_all_data_2015_2025.csv'\n","OUT_CSV = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch2_all_data_2015_2025_tok_thulac.csv'\n","\n","df = pd.read_csv(IN_CSV, encoding='utf-8')\n","if 'content' not in df.columns:\n","    raise ValueError(\"CSV 里没有 'content' 列，请确认。\")\n","\n","# 3) 分词函数\n","def zh_tokenize_thulac(text: str) -> str:\n","    if pd.isna(text):\n","        return \"\"\n","    s = str(text).strip()\n","    if not s:\n","        return \"\"\n","    return lac.cut(s, text=True)  # 直接输出空格分词\n","\n","# 4) 批处理\n","tqdm.pandas(desc=\"THULAC 分词\")\n","df['content'] = df['content'].progress_apply(zh_tokenize_thulac)\n","\n","# 5) 只保留需要的列（去掉原始未分词文本）\n","cols_to_keep = [c for c in df.columns if c != 'content_tok']  # 没有content_tok了\n","df = df[['author','title','date','content']]  # 保留分词后content\n","\n","# 6) 导出\n","df.to_csv(OUT_CSV, index=False, encoding='utf-8')\n","print(f\"已保存：{OUT_CSV}\\n预览：\")\n","print(df.head(3))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xyG2LcuhQSbn","executionInfo":{"status":"ok","timestamp":1755438010327,"user_tz":-120,"elapsed":43702,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"5aa3bcc1-083c-42ea-f56f-1b653124a485"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded succeed\n"]},{"output_type":"stream","name":"stderr","text":["THULAC 分词: 100%|██████████| 1282/1282 [00:37<00:00, 33.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch2_all_data_2015_2025_tok_thulac.csv\n","预览：\n","     author title date                                            content\n","0  (王连香、高雷)   NaN  NaN  人民网 北京 9月 28日 电 （ 记者 王 连 香 ） 在 今日 举行 的 交通 运输部 ...\n","1  (王连香、高雷)   NaN  NaN  人民网 北京 9月 28日 电 （ 记者 王 连 香 ） 据 中国 国 家 铁路 集团 有限...\n","2  (袁勃、胡永秋)   NaN  NaN  新华社 北京 9月 28日 电 （ 记者 冯歆然 ） 外交部 发言人 汪文斌 28日 表示 ...\n"]}]},{"cell_type":"code","source":["#Stanza\n","\n","!pip -q install stanza tqdm pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQ2JwOTdP5JH","executionInfo":{"status":"ok","timestamp":1755102404057,"user_tz":-120,"elapsed":108472,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"feb66565-0016-4d0a-8be7-d73240dfd7a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m959.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# 安装与初始化\n","!pip -q install stanza tqdm pandas\n","\n","import pandas as pd\n","import stanza\n","from tqdm import tqdm\n","\n","# 1) 下载中文模型（只需首次）\n","stanza.download('zh')  # 如果已下载过会跳过\n","\n","# 2) 初始化中文流水线（只开 tokenize，速度更快）\n","nlp = stanza.Pipeline(lang='zh', processors='tokenize', tokenize_pretokenized=False, use_gpu=False)\n","\n","# 3) 路径与读取\n","IN_CSV  = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/ch_all_data_2015_2025.csv'\n","OUT_CSV = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch_all_data_2015_2025_tok_stanza.csv'\n","\n","df = pd.read_csv(IN_CSV, encoding='utf-8')\n","if 'content' not in df.columns:\n","    raise ValueError(\"CSV 里没有 'content' 列，请确认。\")\n","\n","# 4) 分词函数（对 NaN 做保护）\n","def zh_tokenize_stanza(text: str) -> str:\n","    if pd.isna(text):\n","        return \"\"\n","    s = str(text).strip()\n","    if not s:\n","        return \"\"\n","    doc = nlp(s)\n","    tokens = []\n","    for sent in doc.sentences:\n","        for w in sent.tokens:\n","            tokens.append(w.text)\n","    return \" \".join(tokens)\n","\n","# 5) 直接替换原来的 content 列\n","tqdm.pandas(desc=\"Stanza 分词\")\n","df['content'] = df['content'].progress_apply(zh_tokenize_stanza)\n","\n","# 6) 导出\n","df.to_csv(OUT_CSV, index=False, encoding='utf-8')\n","print(f\"已保存：{OUT_CSV}\\n预览：\")\n","print(df[['author','title','date','content']].head(3))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":726,"referenced_widgets":["1bfdb807778a48418af735eab57a6470","d911b3ddd04940e2bbf0e4115da3449b","bbc09144c8ae4ad6afa82079aeabc7a9","0f801adc09914dccb46dbbd4f4dbd87c","249b765bc6b64a3980565186e8d64d87","0e75f1ced01748e88439c181c33b73aa","af3e4428f97b486691ffca27dd2f6aab","fb5a26868834410882fb31c514b48ccb","25e3355c11984e7582cebad94e8ad9b2","f8d9ed78047c463384370db96f7a5f19","3aeffd53fd974415814534378e3cb29a","60dd4584f883426c88c7fe268969a7ff","cb8c508ddec34e52980ac522543312be","44bb8d75bf8d41278ced0688167b3958","f2c3411d1c4a4c74bc9c920999e3c408","8d60a80d5bf84abb8ce3ebfab75f5355","52f8320cb2bd457db131007d60847694","aeed10c12c6047d380fe8dd2e9c4ac49","b5337b23e31547af89aee214a76cd872","45879d8563044833aef73859b36c52fd","0c8357d361d24efc822570b9536f1e3d","08e3de24c42b42c0b57d36ba72df40b8"]},"id":"bYTFb9mBQBZG","executionInfo":{"status":"ok","timestamp":1755104176104,"user_tz":-120,"elapsed":57608,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"fb67bfe1-fdeb-4404-9345-6649ca12f80c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bfdb807778a48418af735eab57a6470"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n","INFO:stanza:\"zh\" is an alias for \"zh-hans\"\n","INFO:stanza:Downloading default packages for language: zh-hans (Simplified_Chinese) ...\n","INFO:stanza:File exists: /root/stanza_resources/zh-hans/default.zip\n","INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n","INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60dd4584f883426c88c7fe268969a7ff"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n","INFO:stanza:\"zh\" is an alias for \"zh-hans\"\n","INFO:stanza:Loading these models for language: zh-hans (Simplified_Chinese):\n","=======================\n","| Processor | Package |\n","-----------------------\n","| tokenize  | gsdsimp |\n","=======================\n","\n","INFO:stanza:Using device: cpu\n","INFO:stanza:Loading: tokenize\n","INFO:stanza:Done loading processors!\n","Stanza 分词: 100%|██████████| 507/507 [00:40<00:00, 12.49it/s]"]},{"output_type":"stream","name":"stdout","text":["已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/ch_all_data_2015_2025_tok_stanza.csv\n","预览：\n","   author                     title              date  \\\n","0     NaN    2015人民微博影响力榜单发布 五大榜单出炉  2015年12月31日16:18   \n","1     NaN    2015人民微博影響力榜單發布 五大榜單出爐  2015年12月31日16:18   \n","2     NaN  2015优秀网络视听作品推选  人民网获两项大奖  2015年12月14日16:12   \n","\n","                                             content  \n","0  人民 网 12 月 31 日 电 又是 岁 末年 初 ， 回想 走过 的 这 一 年 ， 人...  \n","1  人民 網 12 月 31 日 電 又是 歲 末年 初 ， 回想 走過 的 這 一 年 ， 人...  \n","2  近日 ， 在 “ 2015 优秀 网络 视听 作品 推选 活动 ” 中 ， 人民 网 品牌 ...  \n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIS7eELCOSKN"},"outputs":[],"source":["import re\n","\n","input_file = \"input.txt\"\n","output_file = \"output.txt\"\n","\n","with open(input_file, \"r\", encoding=\"utf-8\") as f_in, open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n","    for line in f_in:\n","        # 只匹配以 Bản mẫu 开头的 text\n","        m = re.search(r'\"text\":\\s*\"Bản mẫu:([^\"]+)\"', line)\n","        if m:\n","            content = m.group(1)\n","            # 如果有 \":-\" 或 \":*\"，取前半部分\n","            if \":-\" in content:\n","                content = content.split(\":-\", 1)[0]\n","            if \":*\" in content:\n","                content = content.split(\":*\", 1)[0]\n","            f_out.write(content.strip() + \"\\n\")\n","\n","print(\"处理完成，结果保存在\", output_file)\n"]},{"cell_type":"markdown","metadata":{"id":"BT9UP4nPq481"},"source":["#2. Référence wiktionary en vietnamien"]},{"cell_type":"markdown","metadata":{"id":"I-I_oXD10iL0"},"source":["words in 2018  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMx_hCrz6d2r"},"outputs":[],"source":["import json\n","import re\n","\n","input_file = \"/content/words2018_2.txt\"       # 你的源文件\n","output_file = \"/content/words2018_2_clean.txt\"  # 输出结果\n","\n","def is_vietnamese_word(s: str) -> bool:\n","    s = s.strip()\n","    # 排除空字符串\n","    if not s:\n","        return False\n","    # 排除只有一个字符的条目\n","    if len(s) == 1:\n","        return False\n","    # 排除只含一个带声调字母的条目\n","    if len(s.replace(\" \", \"\")) == 1:\n","        return False\n","    # 允许字母、空格、连字符和撇号\n","    if not re.match(r\"^[\\wÀ-ỹà-ỹ\\s'\\-]+$\", s):\n","        return False\n","    return True\n","\n","with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n","     open(output_file, \"w\", encoding=\"utf-8\") as fout:\n","    for line in fin:\n","        try:\n","            obj = json.loads(line)\n","        except json.JSONDecodeError:\n","            continue\n","        text = obj.get(\"text\", \"\").strip()\n","        if is_vietnamese_word(text):\n","            fout.write(text + \"\\n\")\n","\n","print(\"处理完成！结果已保存到：\", output_file)\n"]},{"cell_type":"markdown","metadata":{"id":"uqExuOdS0jAu"},"source":["wiktionary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"DG1v4f4Aq4Zr","outputId":"a21bde15-9e05-4c3b-e8df-12b308d16e42"},"outputs":[{"name":"stdout","output_type":"stream","text":["processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=0, kept(pre-2015)=0\n","processed=500, kept(pre-2015)=0\n","processed=1000, kept(pre-2015)=0\n","processed=1500, kept(pre-2015)=0\n","processed=1500, kept(pre-2015)=0\n","processed=2000, kept(pre-2015)=0\n","processed=2500, kept(pre-2015)=0\n","processed=3000, kept(pre-2015)=0\n","processed=3000, kept(pre-2015)=0\n","processed=3000, kept(pre-2015)=0\n","processed=3500, kept(pre-2015)=0\n","processed=4000, kept(pre-2015)=0\n","processed=4500, kept(pre-2015)=0\n","processed=4500, kept(pre-2015)=0\n","processed=5000, kept(pre-2015)=0\n","processed=5500, kept(pre-2015)=0\n","processed=6000, kept(pre-2015)=0\n","processed=6000, kept(pre-2015)=0\n","processed=6500, kept(pre-2015)=0\n","processed=7000, kept(pre-2015)=0\n","processed=7500, kept(pre-2015)=0\n","processed=7500, kept(pre-2015)=0\n","processed=8000, kept(pre-2015)=0\n","processed=8500, kept(pre-2015)=0\n","processed=9000, kept(pre-2015)=0\n","processed=9000, kept(pre-2015)=0\n","processed=9500, kept(pre-2015)=0\n","processed=10000, kept(pre-2015)=0\n","processed=10500, kept(pre-2015)=0\n","processed=10500, kept(pre-2015)=0\n","processed=11000, kept(pre-2015)=0\n","processed=11500, kept(pre-2015)=0\n","processed=12000, kept(pre-2015)=0\n","processed=12000, kept(pre-2015)=0\n","processed=12500, kept(pre-2015)=0\n","processed=13000, kept(pre-2015)=0\n","processed=13500, kept(pre-2015)=0\n","processed=13500, kept(pre-2015)=0\n","processed=13500, kept(pre-2015)=0\n","processed=14000, kept(pre-2015)=0\n","processed=14500, kept(pre-2015)=0\n","processed=15000, kept(pre-2015)=0\n","processed=15000, kept(pre-2015)=0\n","processed=15500, kept(pre-2015)=0\n","processed=16000, kept(pre-2015)=0\n","processed=16500, kept(pre-2015)=0\n","processed=16500, kept(pre-2015)=0\n","processed=17000, kept(pre-2015)=0\n","processed=17500, kept(pre-2015)=0\n","processed=18000, kept(pre-2015)=0\n","processed=18000, kept(pre-2015)=0\n","processed=18500, kept(pre-2015)=0\n","processed=19000, kept(pre-2015)=0\n","processed=19500, kept(pre-2015)=0\n","processed=19500, kept(pre-2015)=0\n","processed=20000, kept(pre-2015)=0\n","processed=20500, kept(pre-2015)=0\n","processed=21000, kept(pre-2015)=0\n","processed=21000, kept(pre-2015)=0\n","processed=21500, kept(pre-2015)=0\n","processed=22000, kept(pre-2015)=0\n","processed=22500, kept(pre-2015)=0\n","processed=22500, kept(pre-2015)=0\n","processed=23000, kept(pre-2015)=0\n","processed=23500, kept(pre-2015)=0\n","processed=24000, kept(pre-2015)=0\n","processed=24000, kept(pre-2015)=0\n","processed=24500, kept(pre-2015)=0\n","processed=25000, kept(pre-2015)=0\n","processed=25500, kept(pre-2015)=0\n","processed=25500, kept(pre-2015)=0\n","processed=26000, kept(pre-2015)=0\n","processed=26500, kept(pre-2015)=0\n","processed=27000, kept(pre-2015)=0\n","processed=27000, kept(pre-2015)=0\n","processed=27500, kept(pre-2015)=0\n","processed=28000, kept(pre-2015)=0\n","processed=28500, kept(pre-2015)=0\n","processed=28500, kept(pre-2015)=0\n","processed=29000, kept(pre-2015)=0\n","processed=29500, kept(pre-2015)=0\n","processed=30000, kept(pre-2015)=0\n","processed=30000, kept(pre-2015)=0\n","processed=30500, kept(pre-2015)=0\n","processed=31000, kept(pre-2015)=0\n","processed=31500, kept(pre-2015)=0\n","processed=31500, kept(pre-2015)=0\n","processed=32000, kept(pre-2015)=0\n","processed=32500, kept(pre-2015)=0\n","processed=33000, kept(pre-2015)=0\n","processed=33000, kept(pre-2015)=0\n","processed=33500, kept(pre-2015)=0\n","processed=34000, kept(pre-2015)=0\n","processed=34500, kept(pre-2015)=0\n","processed=34500, kept(pre-2015)=0\n","processed=35000, kept(pre-2015)=0\n","processed=35500, kept(pre-2015)=0\n","processed=36000, kept(pre-2015)=0\n","processed=36000, kept(pre-2015)=0\n","processed=36500, kept(pre-2015)=0\n","processed=37000, kept(pre-2015)=0\n","processed=37500, kept(pre-2015)=0\n","processed=37500, kept(pre-2015)=0\n","processed=38000, kept(pre-2015)=0\n","processed=38500, kept(pre-2015)=0\n","processed=39000, kept(pre-2015)=0\n","processed=39000, kept(pre-2015)=0\n","processed=39500, kept(pre-2015)=0\n","processed=40000, kept(pre-2015)=0\n","processed=40500, kept(pre-2015)=0\n","processed=40500, kept(pre-2015)=0\n","processed=41000, kept(pre-2015)=0\n","processed=41500, kept(pre-2015)=0\n","processed=42000, kept(pre-2015)=0\n","processed=42000, kept(pre-2015)=0\n","processed=42500, kept(pre-2015)=0\n","processed=43000, kept(pre-2015)=0\n","processed=43500, kept(pre-2015)=0\n","processed=43500, kept(pre-2015)=0\n","processed=44000, kept(pre-2015)=0\n","processed=44500, kept(pre-2015)=0\n","processed=45000, kept(pre-2015)=0\n","processed=45000, kept(pre-2015)=0\n","processed=45500, kept(pre-2015)=0\n","processed=46000, kept(pre-2015)=0\n","processed=46500, kept(pre-2015)=0\n","processed=46500, kept(pre-2015)=0\n","processed=46500, kept(pre-2015)=0\n","processed=47000, kept(pre-2015)=0\n","processed=47500, kept(pre-2015)=0\n","processed=48000, kept(pre-2015)=0\n","processed=48000, kept(pre-2015)=0\n","processed=48500, kept(pre-2015)=0\n","processed=49000, kept(pre-2015)=0\n","processed=49500, kept(pre-2015)=0\n","processed=49500, kept(pre-2015)=0\n","processed=50000, kept(pre-2015)=0\n","processed=50500, kept(pre-2015)=0\n","processed=51000, kept(pre-2015)=0\n","processed=51000, kept(pre-2015)=0\n","processed=51000, kept(pre-2015)=0\n","processed=51500, kept(pre-2015)=0\n","processed=52000, kept(pre-2015)=0\n","processed=52500, kept(pre-2015)=0\n","processed=52500, kept(pre-2015)=0\n","processed=53000, kept(pre-2015)=0\n","processed=53500, kept(pre-2015)=0\n","processed=54000, kept(pre-2015)=0\n","processed=54000, kept(pre-2015)=0\n","processed=54500, kept(pre-2015)=0\n","processed=55000, kept(pre-2015)=0\n","processed=55500, kept(pre-2015)=0\n","processed=55500, kept(pre-2015)=0\n","processed=56000, kept(pre-2015)=0\n","processed=56500, kept(pre-2015)=0\n","processed=57000, kept(pre-2015)=0\n","processed=57000, kept(pre-2015)=0\n","processed=57000, kept(pre-2015)=0\n","processed=57500, kept(pre-2015)=0\n","processed=58000, kept(pre-2015)=0\n","processed=58500, kept(pre-2015)=0\n","processed=58500, kept(pre-2015)=0\n","processed=59000, kept(pre-2015)=0\n","processed=59500, kept(pre-2015)=0\n","processed=60000, kept(pre-2015)=0\n","processed=60000, kept(pre-2015)=0\n","processed=60500, kept(pre-2015)=0\n","processed=61000, kept(pre-2015)=0\n","processed=61500, kept(pre-2015)=0\n","processed=61500, kept(pre-2015)=0\n","processed=62000, kept(pre-2015)=0\n","processed=62500, kept(pre-2015)=0\n","processed=63000, kept(pre-2015)=0\n","processed=63000, kept(pre-2015)=0\n","processed=63500, kept(pre-2015)=0\n","processed=64000, kept(pre-2015)=0\n","processed=64500, kept(pre-2015)=0\n","processed=64500, kept(pre-2015)=0\n","processed=65000, kept(pre-2015)=0\n","processed=65500, kept(pre-2015)=0\n","processed=66000, kept(pre-2015)=0\n","processed=66000, kept(pre-2015)=0\n","processed=66500, kept(pre-2015)=0\n","processed=67000, kept(pre-2015)=0\n","processed=67500, kept(pre-2015)=0\n","processed=67500, kept(pre-2015)=0\n","processed=68000, kept(pre-2015)=0\n","processed=68500, kept(pre-2015)=0\n","processed=69000, kept(pre-2015)=0\n","processed=69000, kept(pre-2015)=0\n","processed=69500, kept(pre-2015)=0\n","processed=70000, kept(pre-2015)=0\n","processed=70500, kept(pre-2015)=0\n","processed=70500, kept(pre-2015)=0\n","processed=71000, kept(pre-2015)=0\n","processed=71500, kept(pre-2015)=0\n","processed=72000, kept(pre-2015)=0\n","processed=72000, kept(pre-2015)=0\n","processed=72500, kept(pre-2015)=0\n","processed=73000, kept(pre-2015)=0\n","processed=73500, kept(pre-2015)=0\n","processed=73500, kept(pre-2015)=0\n","processed=74000, kept(pre-2015)=0\n","processed=74500, kept(pre-2015)=0\n","processed=75000, kept(pre-2015)=0\n","processed=75000, kept(pre-2015)=0\n","processed=75500, kept(pre-2015)=0\n","processed=76000, kept(pre-2015)=0\n","processed=76500, kept(pre-2015)=0\n"]}],"source":["# Colab-ready: 抓取“2015 年以前”的越南 Wiktionary 词条标题为词表（JSONL）\n","import requests, time, json, math\n","from datetime import datetime, timezone\n","\n","API = \"https://vi.wiktionary.org/w/api.php\"\n","CUTOFF = datetime(2014, 12, 31, 23, 59, 59, tzinfo=timezone.utc)  # 截止到 2014-12-31\n","OUT = \"wiktionary_vi_pre2015.jsonl\"\n","\n","session = requests.Session()\n","session.headers.update({\n","    \"User-Agent\": \"VN-Wiktionary-Pre2015-Extractor/1.0 (research; contact: youremail@example.com)\"\n","})\n","\n","def wiktionary_all_titles():\n","    \"\"\"遍历越南 Wiktionary 主命名空间（0）的所有非重定向页面，yield 标题\"\"\"\n","    apcontinue = None\n","    while True:\n","        params = {\n","            \"action\": \"query\",\n","            \"format\": \"json\",\n","            \"list\": \"allpages\",\n","            \"apnamespace\": \"0\",          # 主命名空间\n","            \"apfilterredir\": \"nonredirects\",\n","            \"aplimit\": \"500\",            # 最大 500\n","        }\n","        if apcontinue:\n","            params[\"apcontinue\"] = apcontinue\n","\n","        r = session.get(API, params=params, timeout=60)\n","        r.raise_for_status()\n","        data = r.json()\n","        for p in data.get(\"query\", {}).get(\"allpages\", []):\n","            title = p.get(\"title\", \"\").strip()\n","            if title and (\":\" not in title):  # 保险起见，再排除带命名空间前缀的\n","                yield title\n","\n","        if \"continue\" in data and \"apcontinue\" in data[\"continue\"]:\n","            apcontinue = data[\"continue\"][\"apcontinue\"]\n","            # 轻微限速，避免触发流控\n","            time.sleep(0.15)\n","        else:\n","            break\n","\n","def first_revision_timestamp(titles):\n","    \"\"\"\n","    批量查询每个标题的首个修订时间。titles: list[str] (len<=50)\n","    返回 dict[title] = datetime (UTC) 或 None（若查不到）\n","    \"\"\"\n","    params = {\n","        \"action\": \"query\",\n","        \"format\": \"json\",\n","        \"prop\": \"revisions\",\n","        \"rvprop\": \"timestamp\",\n","        \"rvlimit\": \"1\",\n","        \"rvdir\": \"newer\",\n","        \"formatversion\": \"2\",\n","        \"titles\": \"|\".join(titles)\n","    }\n","    r = session.get(API, params=params, timeout=60)\n","    r.raise_for_status()\n","    data = r.json()\n","    out = {}\n","    for page in data.get(\"query\", {}).get(\"pages\", []):\n","        title = page.get(\"title\")\n","        revs = page.get(\"revisions\")\n","        if title and revs:\n","            ts = revs[0][\"timestamp\"]  # e.g. \"2009-01-02T03:04:05Z\"\n","            out[title] = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n","        else:\n","            out[title] = None\n","    return out\n","\n","def batched(iterable, n=50):\n","    buf = []\n","    for x in iterable:\n","        buf.append(x)\n","        if len(buf) >= n:\n","            yield buf\n","            buf = []\n","    if buf:\n","        yield buf\n","\n","kept = 0\n","seen = 0\n","with open(OUT, \"w\", encoding=\"utf-8\") as fout:\n","    for batch in batched(wiktionary_all_titles(), 50):\n","        # 查询该批次的首修时间\n","        for _ in range(3):  # 简单重试\n","            try:\n","                ts_map = first_revision_timestamp(batch)\n","                break\n","            except requests.RequestException:\n","                time.sleep(1.0)\n","        else:\n","            # 连续失败，跳过该批\n","            ts_map = {t: None for t in batch}\n","\n","        for title, ts in ts_map.items():\n","            seen += 1\n","            if ts is not None and ts <= CUTOFF:\n","                obj = {\"text\": title, \"source\": \"wiktionary\"}\n","                fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n","                kept += 1\n","\n","        # 温和限速\n","        time.sleep(0.2)\n","        if seen % 500 == 0:\n","            print(f\"processed={seen}, kept(pre-2015)={kept}\")\n","\n","print(f\"完成！共处理 {seen} 个词条，保留（≤2014-12-31）{kept} 个。输出文件：{OUT}\")\n"]},{"cell_type":"markdown","source":["#Tokenisation en vietnamien"],"metadata":{"id":"8Hzx_gKwyfH2"}},{"cell_type":"markdown","source":[],"metadata":{"id":"G7gY8XVBywI6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Qfk_zRXsBxl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754843064130,"user_tz":-120,"elapsed":33398,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"60df67db-d68f-41a5-af8d-e51db56f71d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#步骤 1：将所有 CSV 文件合并成一个文件\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["#2. 递归合并所有 CSV（2015–2025）计时\n","import pandas as pd\n","import os\n","import time\n","from tqdm import tqdm  # 用于显示进度条\n","\n","start_time = time.time()\n","\n","# 主目录\n","folder_path = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Avant-Token/vi-corpus'\n","\n","if not os.path.exists(folder_path):\n","    raise FileNotFoundError(f\"路径不存在: {folder_path}\")\n","\n","# 遍历所有子文件夹，收集 csv 文件路径\n","csv_files = []\n","for root, dirs, files in os.walk(folder_path):\n","    for file in files:\n","        if file.lower().endswith('.csv'):\n","            csv_files.append(os.path.join(root, file))\n","\n","print(f\"共找到 {len(csv_files)} 个 CSV 文件\\n\")\n","\n","# 合并（带进度条）\n","df_list = []\n","for file_path in tqdm(csv_files, desc=\"正在读取 CSV 文件\", unit=\"文件\"):\n","    try:\n","        df = pd.read_csv(file_path)\n","        df_list.append(df)\n","    except Exception as e:\n","        print(f\" 读取失败 {file_path}: {e}\")\n","\n","if not df_list:\n","    raise ValueError(\"没有找到可合并的 CSV 数据\")\n","\n","# 合并所有 DataFrame\n","all_data = pd.concat(df_list, ignore_index=True)\n","\n","# 如果 date 列可用，过滤 2015–2025\n","if 'date' in all_data.columns:\n","    all_data['date'] = pd.to_datetime(all_data['date'], errors='coerce')\n","    all_data = all_data[\n","        (all_data['date'].dt.year >= 2015) &\n","        (all_data['date'].dt.year <= 2025)\n","    ]\n","\n","# 保存结果\n","merged_path = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/vi_all_data_2015_2025.csv'\n","all_data.to_csv(merged_path, index=False)\n","\n","# 打印总用时\n","elapsed_time = time.time() - start_time\n","print(f\"\\n合并完成，保存到: {merged_path}\")\n","print(f\"总用时: {elapsed_time:.2f} 秒\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3teEgULV3R81","executionInfo":{"status":"ok","timestamp":1754844234228,"user_tz":-120,"elapsed":3598,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"e8023f11-3363-42b6-df69-4049695a6de9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["共找到 759 个 CSV 文件\n","\n"]},{"output_type":"stream","name":"stderr","text":["正在读取 CSV 文件: 100%|██████████| 759/759 [00:02<00:00, 280.49文件/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","合并完成，保存到: /content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/vi_all_data_2015_2025.csv\n","总用时: 3.09 秒\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2970731906.py:41: UserWarning: Parsing dates in %d/%m/%Y %I:%M %p format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n","  all_data['date'] = pd.to_datetime(all_data['date'], errors='coerce')\n"]}]},{"cell_type":"code","source":["# 递归合并（带分隔符/编码自适应）+ 详尽报告\n","import pandas as pd\n","import os, time, re\n","from tqdm import tqdm\n","\n","start = time.time()\n","\n","FOLDER = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Avant-Token/vi-corpus'\n","OUT    = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/vi_all_data_2015_2025.csv'\n","REPORT = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/vi_merge_report.csv'\n","\n","REQUIRED_COLS = ['author','title','date','content','url']  # 你的目标列\n","\n","def read_csv_robust(path):\n","    \"\"\"返回 (df, used_sep, used_enc, err)\"\"\"\n","    seps_try = [None, ',', ';', '\\t', '|']   # None=自动嗅探（engine='python'）\n","    encs_try = ['utf-8', 'utf-8-sig', 'latin1']\n","    last_err = None\n","    for enc in encs_try:\n","        for sep in seps_try:\n","            try:\n","                df = pd.read_csv(\n","                    path,\n","                    sep=sep,\n","                    engine='python',\n","                    encoding=enc,\n","                    on_bad_lines='skip'\n","                )\n","                return df, (sep if sep is not None else 'auto'), enc, None\n","            except Exception as e:\n","                last_err = str(e)\n","                continue\n","    return pd.DataFrame(), 'fail', 'fail', last_err\n","\n","if not os.path.exists(FOLDER):\n","    raise FileNotFoundError(f\"路径不存在: {FOLDER}\")\n","\n","# 收集 CSV\n","csv_files = []\n","for root, _, files in os.walk(FOLDER):\n","    for f in files:\n","        if f.lower().endswith('.csv'):\n","            csv_files.append(os.path.join(root, f))\n","print(f\"共找到 {len(csv_files)} 个 CSV 文件\\n\")\n","\n","# 读取 + 报告\n","records = []\n","dfs = []\n","\n","for fp in tqdm(csv_files, desc=\"读取 CSV\", unit=\"文件\"):\n","    df, used_sep, used_enc, err = read_csv_robust(fp)\n","\n","    # 标注来源、路径年份提示\n","    source = 'unknown'\n","    for s in ['Kenh','Genk','Cafebize']:\n","        if re.search(rf'/{s}/', fp, flags=re.I):\n","            source = s.lower()\n","            break\n","    y_hint = None\n","    y = re.search(r'(20[1-2]\\d)', fp)\n","    if y: y_hint = y.group(1)\n","\n","    # 统一列名（去空格小写），避免“Date”“URL”之类大小写不一致\n","    old_cols = df.columns.tolist()\n","    df.columns = [c.strip() for c in df.columns]\n","    lower_map = {c: c.lower() for c in df.columns}\n","    df.rename(columns=lower_map, inplace=True)\n","\n","    row_count = len(df)\n","    has_cols = {c: (c in df.columns) for c in REQUIRED_COLS}\n","\n","    records.append({\n","        'file': fp,\n","        'rows': row_count,\n","        'used_sep': used_sep,\n","        'used_encoding': used_enc,\n","        'has_author': has_cols['author'],\n","        'has_title': has_cols['title'],\n","        'has_date': has_cols['date'],\n","        'has_content': has_cols['content'],\n","        'has_url': has_cols['url'],\n","        'source': source,\n","        'year_hint': y_hint,\n","        'error': '' if err is None else err[:120]\n","    })\n","\n","    # 只要不是“完全空表”就先收进来；后续再统一处理日期\n","    if row_count > 0:\n","        # 缺的列补空，确保结构统一\n","        for c in REQUIRED_COLS:\n","            if c not in df.columns:\n","                df[c] = None\n","        df['source'] = source\n","        df['year_hint'] = y_hint\n","        dfs.append(df[REQUIRED_COLS + ['source','year_hint']])\n","\n","# 输出报告\n","report_df = pd.DataFrame.from_records(records)\n","report_df.to_csv(REPORT, index=False)\n","print(f\" 读取报告已保存：{REPORT}\")\n","print(\"—— Top 5 读取最少行的文件 ——\")\n","print(report_df.sort_values('rows').head(5)[['file','rows','used_sep','used_encoding','error']])\n","\n","if not dfs:\n","    raise ValueError(\"所有文件都读空了，请检查报告 merge_report.csv\")\n","\n","# 合并\n","all_data = pd.concat(dfs, ignore_index=True)\n","print(f\"合并后总行数：{len(all_data):,}\")\n","\n","# 日期解析（尽量保留；只对成功解析的行做年份过滤，失败的行也保留）\n","def try_parse_date(s):\n","    if pd.isna(s): return pd.NaT\n","    s = str(s).strip()\n","    for fmt in (\"%d/%m/%Y %I:%M %p\",\"%d/%m/%Y %H:%M\",\"%d/%m/%Y\",\n","                \"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d %H:%M\",\"%Y-%m-%d\",\n","                \"%Y/%m/%d %H:%M:%S\",\"%Y/%m/%d\"):\n","        try: return pd.to_datetime(s, format=fmt)\n","        except: pass\n","    return pd.to_datetime(s, dayfirst=True, errors='coerce')\n","\n","all_data['date_parsed'] = all_data['date'].apply(try_parse_date)\n","ok = all_data['date_parsed'].notna().sum()\n","bad = len(all_data) - ok\n","print(f\"日期解析成功：{ok:,} 行；失败：{bad:,} 行\")\n","\n","# 仅对成功解析的行做 2015–2025 过滤；解析失败的行保留（方便后续人工修正）\n","mask_ok = all_data['date_parsed'].between('2015-01-01','2025-12-31', inclusive='both')\n","mask_bad = all_data['date_parsed'].isna()\n","final = all_data[mask_ok | mask_bad].copy()\n","print(f\"筛选后（保留无效日期）：{len(final):,} 行\")\n","\n","final.to_csv(OUT, index=False)\n","print(f\"已保存：{OUT}    用时 {time.time()-start:.2f}s\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y19MG0YS5JKd","executionInfo":{"status":"ok","timestamp":1754844856746,"user_tz":-120,"elapsed":4653,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"ab71920f-78a3-4e96-faa2-9c2c107dd920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["共找到 759 个 CSV 文件\n","\n"]},{"output_type":"stream","name":"stderr","text":["读取 CSV: 100%|██████████| 759/759 [00:03<00:00, 194.61文件/s]\n","/tmp/ipython-input-56446194.py:120: UserWarning: Parsing dates in %Y-%m-%dT%H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  return pd.to_datetime(s, dayfirst=True, errors='coerce')\n"]},{"output_type":"stream","name":"stdout","text":[" 读取报告已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/vi_merge_report.csv\n","—— Top 5 读取最少行的文件 ——\n","                                                  file  rows used_sep  \\\n","751  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","750  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","749  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","748  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","747  /content/drive/MyDrive/Colab Notebooks/STAGE_C...     1     auto   \n","\n","    used_encoding error  \n","751         utf-8        \n","750         utf-8        \n","749         utf-8        \n","748         utf-8        \n","747         utf-8        \n","合并后总行数：759\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-56446194.py:120: UserWarning: Parsing dates in %m:%M %d/%H/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  return pd.to_datetime(s, dayfirst=True, errors='coerce')\n","/tmp/ipython-input-56446194.py:120: UserWarning: Parsing dates in %H:%m %d/%M/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  return pd.to_datetime(s, dayfirst=True, errors='coerce')\n"]},{"output_type":"stream","name":"stdout","text":["日期解析成功：627 行；失败：132 行\n","筛选后（保留无效日期）：758 行\n","已保存：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/vi_all_data_2015_2025.csv    用时 4.67s\n"]}]},{"cell_type":"code","source":["#先按年份合并，再把每年的结果再合并成一个总表，代码结构要稍微改，比如这样：\n","\n","import pandas as pd\n","import os\n","\n","folder_path = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Avant-Token/vi-corpus'\n","\n","if not os.path.exists(folder_path):\n","    raise FileNotFoundError(f\"路径不存在: {folder_path}\")\n","\n","# 保存所有年份的结果\n","yearly_dfs = []\n","\n","for year in range(2015, 2026):  # 2015 到 2025\n","    year_csv_files = []\n","    for root, dirs, files in os.walk(folder_path):\n","        for file in files:\n","            if file.lower().endswith('.csv') and str(year) in file:\n","                year_csv_files.append(os.path.join(root, file))\n","\n","    if year_csv_files:\n","        df_list = []\n","        for file_path in year_csv_files:\n","            try:\n","                df = pd.read_csv(file_path)\n","                df_list.append(df)\n","            except Exception as e:\n","                print(f\"{year} 读取失败 {file_path}: {e}\")\n","\n","        if df_list:\n","            year_df = pd.concat(df_list, ignore_index=True)\n","            yearly_dfs.append(year_df)\n","\n","# 再把所有年份的数据合并成一个总表\n","if yearly_dfs:\n","    all_data = pd.concat(yearly_dfs, ignore_index=True)\n","    merged_path = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/all_data_2015_2025.csv'\n","    all_data.to_csv(merged_path, index=False)\n","    print(f\"按年份合并完成，保存到: {merged_path}\")\n","else:\n","    print(\"没有找到任何数据\")\n"],"metadata":{"id":"iKFp3tL_1aaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Token"],"metadata":{"id":"N7px4oIC1ltu"}},{"cell_type":"code","source":["!pip install pyvi"],"metadata":{"id":"O0y6s0A5zGw8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754844900553,"user_tz":-120,"elapsed":6072,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"75e2f4be-7fdb-4c96-a2da-a751b472a531"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyvi\n","  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.6.1)\n","Collecting sklearn-crfsuite (from pyvi)\n","  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\n","Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n","  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n","Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n","Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n","Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"]}]},{"cell_type":"code","source":["#3. 用 PyVi 分词越南语\n","\n","# 用 PyVi 对 content 列分词（大文件友好、带进度条）\n","import os, time, re\n","import pandas as pd\n","from tqdm import tqdm\n","from pyvi import ViTokenizer\n","\n","INPUT  = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/fusion_avantToken/vi_all_data_2015_2025.csv'\n","OUTPUT = '/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/vi_all_data_2015_2025_tokenized.csv'\n","CHUNK_SIZE = 10000  # 可按内存情况调整，比如 5000/20000\n","\n","start = time.time()\n","\n","if not os.path.exists(INPUT):\n","    raise FileNotFoundError(f'找不到输入文件：{INPUT}')\n","\n","# 可选的小清洗：去除多余空白/URL/HTML 标签（需要就留着，不需要可注释掉）\n","_url_pat  = re.compile(r'https?://\\S+')\n","_html_pat = re.compile(r'<[^>]+>')\n","\n","def clean_text(s: str) -> str:\n","    s = _url_pat.sub(' ', s)\n","    s = _html_pat.sub(' ', s)\n","    s = re.sub(r'\\s+', ' ', s).strip()\n","    return s\n","\n","def tokenize_vi(text):\n","    if pd.isna(text) or str(text).strip() == '':\n","        return ''\n","    # 如不想清洗，改为：txt = str(text)\n","    txt = clean_text(str(text))\n","    return ViTokenizer.tokenize(txt)\n","\n","# 如果输出已存在，先删掉，避免重复追加\n","if os.path.exists(OUTPUT):\n","    os.remove(OUTPUT)\n","\n","# 分块读取 + 处理 + 逐块写出（保留所有原列，并新增 content_tokenized）\n","reader = pd.read_csv(\n","    INPUT,\n","    chunksize=CHUNK_SIZE,\n","    dtype=str,                # 统一按字符串读入，避免类型冲突\n","    encoding_errors='ignore'  # 容忍少量坏字符\n",")\n","\n","first_chunk = True\n","total_rows_in = 0\n","total_rows_out = 0\n","\n","for chunk in tqdm(reader, desc='Tokenizing content', unit='块'):\n","    total_rows_in += len(chunk)\n","\n","    # 确保有 content 列\n","    if 'content' not in chunk.columns:\n","        raise ValueError(\"没有找到 'content' 列，请检查输入 CSV。\")\n","\n","    # 分词\n","    chunk['content_tokenized'] = chunk['content'].apply(tokenize_vi)\n","\n","    # 写出\n","    if first_chunk:\n","        chunk.to_csv(OUTPUT, index=False, mode='w')\n","        first_chunk = False\n","    else:\n","        chunk.to_csv(OUTPUT, index=False, mode='a', header=False)\n","\n","    total_rows_out += len(chunk)\n","\n","elapsed = time.time() - start\n","print(f'分词完成，保存到：{OUTPUT}')\n","print(f'输入行数：{total_rows_in:,}；输出行数：{total_rows_out:,}')\n","print(f'用时：{elapsed:.2f} 秒')\n"],"metadata":{"id":"Q8u-IeJ_1qED","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754845174906,"user_tz":-120,"elapsed":10983,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"6baf0f5c-e0c3-4f24-ddd2-115d265d7fa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Tokenizing content: 1块 [00:10, 10.23s/块]"]},{"output_type":"stream","name":"stdout","text":["分词完成，保存到：/content/drive/MyDrive/Colab Notebooks/STAGE_CRLAO_CNRS/Corpus_data/Token/vi_all_data_2015_2025_tokenized.csv\n","输入行数：758；输出行数：758\n","用时：10.24 秒\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1bfdb807778a48418af735eab57a6470":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d911b3ddd04940e2bbf0e4115da3449b","IPY_MODEL_bbc09144c8ae4ad6afa82079aeabc7a9","IPY_MODEL_0f801adc09914dccb46dbbd4f4dbd87c"],"layout":"IPY_MODEL_249b765bc6b64a3980565186e8d64d87"}},"d911b3ddd04940e2bbf0e4115da3449b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e75f1ced01748e88439c181c33b73aa","placeholder":"​","style":"IPY_MODEL_af3e4428f97b486691ffca27dd2f6aab","value":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "}},"bbc09144c8ae4ad6afa82079aeabc7a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb5a26868834410882fb31c514b48ccb","max":53539,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25e3355c11984e7582cebad94e8ad9b2","value":53539}},"0f801adc09914dccb46dbbd4f4dbd87c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8d9ed78047c463384370db96f7a5f19","placeholder":"​","style":"IPY_MODEL_3aeffd53fd974415814534378e3cb29a","value":" 433k/? [00:00&lt;00:00, 18.2MB/s]"}},"249b765bc6b64a3980565186e8d64d87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e75f1ced01748e88439c181c33b73aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af3e4428f97b486691ffca27dd2f6aab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb5a26868834410882fb31c514b48ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25e3355c11984e7582cebad94e8ad9b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8d9ed78047c463384370db96f7a5f19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aeffd53fd974415814534378e3cb29a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60dd4584f883426c88c7fe268969a7ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cb8c508ddec34e52980ac522543312be","IPY_MODEL_44bb8d75bf8d41278ced0688167b3958","IPY_MODEL_f2c3411d1c4a4c74bc9c920999e3c408"],"layout":"IPY_MODEL_8d60a80d5bf84abb8ce3ebfab75f5355"}},"cb8c508ddec34e52980ac522543312be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52f8320cb2bd457db131007d60847694","placeholder":"​","style":"IPY_MODEL_aeed10c12c6047d380fe8dd2e9c4ac49","value":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "}},"44bb8d75bf8d41278ced0688167b3958":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5337b23e31547af89aee214a76cd872","max":53539,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45879d8563044833aef73859b36c52fd","value":53539}},"f2c3411d1c4a4c74bc9c920999e3c408":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c8357d361d24efc822570b9536f1e3d","placeholder":"​","style":"IPY_MODEL_08e3de24c42b42c0b57d36ba72df40b8","value":" 433k/? [00:00&lt;00:00, 18.3MB/s]"}},"8d60a80d5bf84abb8ce3ebfab75f5355":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52f8320cb2bd457db131007d60847694":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aeed10c12c6047d380fe8dd2e9c4ac49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5337b23e31547af89aee214a76cd872":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45879d8563044833aef73859b36c52fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c8357d361d24efc822570b9536f1e3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08e3de24c42b42c0b57d36ba72df40b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}