{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO996vg6fW0bdE+vnIibsoy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#1. Corpus\n","#Chinese"],"metadata":{"id":"of7mWC0TLyd3"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"krUjyoJbsA4E","executionInfo":{"status":"ok","timestamp":1755018264238,"user_tz":-120,"elapsed":6805,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"2631fff7-c862-4d2d-93ed-146bd4f51516"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"]}],"source":["! pip install requests beautifulsoup4\n"]},{"cell_type":"code","source":["! pip install tqdm\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uXYh8-WixZsx","executionInfo":{"status":"ok","timestamp":1755018272874,"user_tz":-120,"elapsed":8627,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"3a259808-8b99-4059-befb-9fa4051bd5f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from datetime import datetime, timedelta\n","import time\n","import csv\n","from tqdm import tqdm  # 新增导入\n","\n","start_date = datetime(2015, 1, 1)\n","end_date = datetime(2025, 7, 1)\n","\n","headers = {\n","    'User-Agent': 'Mozilla/5.0'\n","}\n","\n","articles_data = []\n","\n","#  开始计时\n","start_time = time.time()\n","\n","#  构建日期列表供 tqdm 使用\n","total_days = (end_date - start_date).days + 1\n","date_list = [start_date + timedelta(days=i) for i in range(total_days)]\n","\n","#  使用 tqdm 包装 date_list 以显示进度条\n","for current_date in tqdm(date_list, desc=\"抓取进度\"):\n","    date_str = current_date.strftime(\"%Y-%m/%d\")\n","    base_url = f\"http://paper.people.com.cn/rmrb/html/{date_str}/nbs.D110000renmrb_01.htm\"\n","\n","    try:\n","        res = requests.get(base_url, headers=headers, timeout=10)\n","        res.encoding = 'utf-8'\n","        if res.status_code == 200:\n","            soup = BeautifulSoup(res.text, 'html.parser')\n","            page_links = soup.select(\"div#pageList a\")\n","            for page in page_links:\n","                page_href = page.get('href')\n","                if not page_href:\n","                    continue\n","                page_url = f\"http://paper.people.com.cn/rmrb/html/{date_str}/{page_href}\"\n","                try:\n","                    page_res = requests.get(page_url, headers=headers, timeout=10)\n","                    page_res.encoding = 'utf-8'\n","                    page_soup = BeautifulSoup(page_res.text, 'html.parser')\n","                    article_links = page_soup.select(\"div.news a\")\n","                    for a in article_links:\n","                        href = a.get('href')\n","                        if not href:\n","                            continue\n","                        article_url = f\"http://paper.people.com.cn/rmrb/html/{date_str}/{href}\"\n","                        try:\n","                            article_res = requests.get(article_url, headers=headers, timeout=10)\n","                            article_res.encoding = 'utf-8'\n","                            article_soup = BeautifulSoup(article_res.text, 'html.parser')\n","                            title = article_soup.select_one(\"h1\").get_text(strip=True) if article_soup.select_one(\"h1\") else \"\"\n","                            subtitle = article_soup.select_one(\"h2\").get_text(strip=True) if article_soup.select_one(\"h2\") else \"\"\n","                            content_div = article_soup.select_one(\"div#ozoom\")\n","                            content = content_div.get_text(separator=\"\\n\").strip() if content_div else \"\"\n","                            articles_data.append([\n","                                current_date.strftime(\"%Y-%m-%d\"),\n","                                title,\n","                                subtitle,\n","                                content,\n","                                article_url\n","                            ])\n","                        except Exception:\n","                            continue\n","                except Exception:\n","                    continue\n","        time.sleep(1)  # 每天请求间隔1秒，避免被封IP\n","    except Exception:\n","        pass\n","\n","# 保存为CSV文件\n","with open(\"people_daily_2015_2025.csv\", \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"日期\", \"标题\", \"副标题\", \"正文\", \"网址\"])\n","    writer.writerows(articles_data)\n","\n","print(\" 抓取完成，共收集文章数：\", len(articles_data))\n","\n","# ⏱ 显示总耗时\n","end_time = time.time()\n","total_time = end_time - start_time\n","minutes = int(total_time // 60)\n","seconds = int(total_time % 60)\n","print(f\"⏱ 总耗时：{minutes} 分 {seconds} 秒\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392},"id":"KfUfNrL_xgY0","outputId":"a321798c-5dec-4466-93f5-af1115040056","executionInfo":{"status":"error","timestamp":1755018740910,"user_tz":-120,"elapsed":6417,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["抓取进度:   9%|▉         | 362/3835 [07:47<1:14:47,  1.29s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3995797430.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["#1) Quotidien du Peuple\n","\n","2022-2025\n","http://paper.people.com.cn/rmrb/pc/layout/202507/23/node_01.html\n","\n","\n","#2) https://www.google.com/advanced_search?q=site:thepaper.cn+2015&client=firefox-b-d&sca_esv=4d1e52ecd0f51357&sxsrf=AE3TifOp3VsAjjic0H6D16Q26gmvMKT8dQ:1753550785198&tbas=0&biw=1232&bih=728&dpr=2\n","\n","\n","# 3) http://www.people.com.cn/\n"],"metadata":{"id":"Ls8QfVsRMblR"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# 目标网页\n","url = \"https://paper.people.com.cn/rmrb/html/2024-07/01/nw.D110000renmrb_20240701_6-20.htm\"\n","headers = {\n","    'User-Agent': 'Mozilla/5.0'\n","}\n","\n","# 发起请求\n","res = requests.get(url, headers=headers)\n","res.encoding = 'utf-8'\n","\n","if res.status_code == 200:\n","    soup = BeautifulSoup(res.text, 'html.parser')\n","\n","    # 提取字段\n","    title = soup.select_one(\"h1\").get_text(strip=True) if soup.select_one(\"h1\") else \"\"\n","    subtitle = soup.select_one(\"h2\").get_text(strip=True) if soup.select_one(\"h2\") else \"\"\n","    title = f\"{title}: {subtitle}\" if subtitle else title\n","\n","    # 作者可能在 <div class=\"edit\">，有时在正文里\n","    author_tag = soup.select_one(\"div.edit\")\n","    author = author_tag.get_text(strip=True).replace(\"编辑：\", \"\") if author_tag else \"\"\n","\n","    # 日期从 URL 获取\n","    date = \"2022-07-01\"\n","\n","    # 正文\n","    content_div = soup.select_one(\"div#ozoom\")\n","    content = content_div.get_text(separator=\"\\n\").strip() if content_div else \"\"\n","\n","    # 来源（固定）\n","    source = \"People's Daily (Renmin Ribao)\"\n","\n","    # 写入 CSV 文件\n","    with open(\"quotidienne87.csv\", \"w\", encoding=\"utf-8-sig\", newline=\"\") as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"author\", \"title\", \"date\", \"content\", \"source\"])\n","        writer.writerow([author, title, date, content, source])\n","\n","    print(\" 成功保存为 'quotidienne6.csv'\")\n","else:\n","    print(f\" 请求失败，状态码：{res.status_code}\")\n"],"metadata":{"id":"bpWV9Emor9Tg","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"error","timestamp":1755101346198,"user_tz":-120,"elapsed":56,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"f84183e6-04e2-4b2c-dadc-7170ec7e1f51"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"unterminated string literal (detected at line 6) (ipython-input-614328957.py, line 6)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-614328957.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    url = \"http://www.people.com.cn/n/2015/0921/c347079-27612558.html\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 6)\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标 URL\n","url = \"http://www.people.com.cn/n1/2015/1214/c393876-27927336.html\"\n","\n","# 请求网页\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","response.encoding = \"utf-8\"  # 确保中文不会乱码\n","\n","# 解析 HTML\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# ===== 提取标题 =====\n","title_tag = soup.find(\"h1\", id=\"p_title\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# ===== 提取作者 =====\n","author_tag = soup.find(\"i\", id=\"p_editor\")\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","# ===== 提取日期 =====\n","date_tag = soup.find(\"span\", id=\"p_publishtime\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# ===== 提取正文内容 =====\n","content_div = soup.find(\"div\", id=\"p_content\")\n","paragraphs = content_div.find_all(\"p\") if content_div else []\n","content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n","\n","# ===== 保存数据到 CSV =====\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"url\": url\n","}]\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"peuple2015_6.csv\", index=False, encoding=\"utf-8-sig\")\n","\n","print(\"已完成抓取，已保存为 peuple2016.csv\")\n"],"metadata":{"id":"B4uVfShOW8ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755101404472,"user_tz":-120,"elapsed":537,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"7c4134c3-557c-4ea7-f32a-60eaf71d4dba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["已完成抓取，已保存为 peuple2016.csv\n"]}]},{"cell_type":"code","source":["#2016\n","\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标 URL\n","url = \"http://www.people.com.cn/n1/2016/0901/c397545-28684604.html\"\n","\n","# 请求网页\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","response.encoding = \"utf-8\"  # 确保中文不会乱码\n","\n","# 解析 HTML\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# ===== 提取标题 =====\n","title_tag = soup.find(\"h1\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# ===== 提取日期 =====\n","# 日期在 <div class=\"fl\"> 里\n","date_tag = soup.find(\"div\", class_=\"fl\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# ===== 提取作者 =====\n","# 作者在 <div class=\"edit clearfix\"> 里\n","author_tag = soup.find(\"div\", class_=\"edit clearfix\")\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","# ===== 提取正文内容 =====\n","content_div = soup.find(\"div\", id=\"rwb_zw\")\n","content = \"\"\n","\n","if content_div:\n","    # 只取所有 <p> 标签的文本（过滤掉图片说明和空标签）\n","    paragraphs = content_div.find_all(\"p\")\n","    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n","\n","# ===== 保存到 CSV =====\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"url\": url\n","}]\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"peuple2016_17.csv\", index=False, encoding=\"utf-8-sig\")\n","\n","print(\" 已完成抓取，已保存为 peuple2016.csv\")\n"],"metadata":{"id":"_STpmODMaqnc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网址\n","url = \"http://www.people.com.cn/n/2015/0921/c347079-27612558.html\"\n","\n","# 发送请求并自动检测编码\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","response.encoding = response.apparent_encoding  # 自动识别编码，防止乱码\n","\n","# 用 BeautifulSoup 解析\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# =========  提取标题 =========\n","title = \"\"\n","title_tag = soup.find(\"h1\", id=\"p_title\")\n","if title_tag:\n","    title = title_tag.get_text(strip=True)\n","\n","# ========= 提取日期 =========\n","date = \"\"\n","date_tag = soup.find(\"span\", id=\"p_publishtime\")\n","if date_tag:\n","    date = date_tag.get_text(strip=True)\n","\n","# ========= 提取作者 =========\n","author = \"\"\n","author_tag = soup.find(\"div\", class_=\"edit\")  # 注意这里有时是 class=\"edit clearfix\"\n","if author_tag:\n","    author = author_tag.get_text(strip=True)\n","\n","# ========= 提取正文内容 =========\n","content = \"\"\n","content_div = soup.find(\"div\", id=\"p_content\")\n","if content_div:\n","    paragraphs = content_div.find_all(\"p\")\n","    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n","\n","# ========= 保存到 CSV =========\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"url\": url\n","}]\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"peuple2015_23.csv\", index=False, encoding=\"utf-8-sig\")  # 用 utf-8-sig 确保 Excel 打开不乱码\n","\n","# 打印结果验证\n","print(f\"抓取完成！\\n标题: {title}\\n日期: {date}\\n作者: {author}\\n内容预览: {content[:60]}...\")\n"],"metadata":{"id":"QtqhkJC-c69u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755101784008,"user_tz":-120,"elapsed":1003,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"535769a5-84f5-452c-a01e-9d1e39d5c9a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["抓取完成！\n","标题: 政务微博办实事排行榜59：@中国保险报 获赞最多\n","日期: 2015年09月21日09:27\n","作者: (责编：张娅喃、李小炜)\n","内容预览: 人民网北京9月21日电（张娅喃 实习生熊捷）9月21日，在人民微博“对话官微”推出的《全国政务微博办实事排行榜》第59期...\n"]}]},{"cell_type":"code","source":["#2018\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网址\n","url = \"http://www.people.com.cn/n1/2018/0303/c32306-29845828.html\"\n","\n","# 发送请求并自动检测编码\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","response.encoding = response.apparent_encoding  # 自动识别编码，防止乱码\n","\n","# 用 BeautifulSoup 解析\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# =========  提取标题 =========\n","title = \"\"\n","title_tag = soup.find(\"h1\", id=\"p_title\")\n","if title_tag:\n","    title = title_tag.get_text(strip=True)\n","\n","# ========= 提取日期 =========\n","date = \"\"\n","date_tag = soup.find(\"i\", id=\"p_publishtime\")   #  这里改成<i>，不是<span>\n","if date_tag:\n","    date = date_tag.get_text(strip=True)\n","\n","# ========= 提取作者 =========\n","author = \"\"\n","author_tag = soup.find(\"div\", id=\"p_editor\")   # 人民网作者标签是id=\"p_editor\"\n","if author_tag:\n","    author = author_tag.get_text(strip=True).replace(\"(责编：\", \"\").replace(\")\", \"\")\n","\n","# ========= 提取正文内容 =========\n","content = \"\"\n","content_div = soup.find(\"div\", id=\"p_content\")\n","if content_div:\n","    paragraphs = content_div.find_all(\"p\")\n","    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n","\n","# ========= 保存到 CSV =========\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"source\": url\n","}]\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"peuple2018_4.csv\", index=False, encoding=\"utf-8-sig\")  # 用 utf-8-sig，Excel 打开不乱码\n","\n","# 打印结果验证\n","print(f\"抓取完成！\\n标题: {title}\\n日期: {date}\\n作者: {author}\\n内容预览: {content[:60]}...\")\n","\n"],"metadata":{"id":"ljTHBgVqQr0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网址\n","url = \"http://www.people.com.cn/n1/2019/0304/c32306-30956836.html\"\n","\n","# 请求头（模拟浏览器，避免被拦截或返回跳转页）\n","headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"}\n","\n","# 发送请求\n","response = requests.get(url, headers=headers)\n","response.encoding = response.apparent_encoding  # 自动识别编码，防止乱码\n","\n","# 🛠️ 调试：打印 URL 和 HTML 前 1000 个字符，检查是否拿到正确页面\n","print(\"实际抓取的 URL:\", response.url)\n","print(\"HTML 前 1000 字符:\\n\", response.text[:1000])\n","\n","# 解析 HTML\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# ========= 提取标题 =========\n","title_tag = soup.find(\"h1\", id=\"p_title\") or soup.find(\"h1\")  # 优先找 id=\"p_title\"，否则找 h1\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","print(\"标题:\", title)\n","\n","# ========= 提取日期 =========\n","date_tag = soup.find(\"div\", class_=\"col-1-1 fl\")\n","date = date_tag.get_text(strip=True).split(\"|\")[0] if date_tag else \"\"\n","print(\"日期:\", date)\n","\n","# ========= 提取作者 =========\n","author_tag = soup.find(\"div\", class_=\"edit cf\")\n","author = author_tag.get_text(strip=True).replace(\"(责编：\", \"\").replace(\")\", \"\") if author_tag else \"\"\n","print(\"作者:\", author)\n","\n","# ========= 提取正文 =========\n","content = \"\"\n","content_div = soup.find(\"div\", class_=\"rm_txt_con cf\")\n","if content_div:\n","    paragraphs = content_div.find_all(\"p\")\n","    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n","print(\"正文前 60 字:\", content[:60])\n","\n","# ========= 保存到 CSV =========\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"source\": url\n","}]\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"peuple2019_18.csv\", index=False, encoding=\"utf-8-sig\")  # 用 utf-8-sig，Excel 打开不乱码\n","\n","print(\"抓取完成！已生成 peuple2019.csv \")\n"],"metadata":{"id":"63dDGjtVX0PH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网址\n","url = \"http://www.people.com.cn/n1/2019/0306/c425620-30960965.html\"\n","\n","# 请求头（模拟浏览器）\n","headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"}\n","\n","# 发送请求\n","response = requests.get(url, headers=headers)\n","response.encoding = response.apparent_encoding  # 自动识别编码，避免乱码\n","\n","# 🛠️ 调试输出（可注释）\n","print(\"实际抓取的 URL:\", response.url)\n","print(\"HTML 前 500 字符:\\n\", response.text[:500])\n","\n","# 解析 HTML\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# ========= 提取标题 =========\n","title_tag = soup.find(\"h1\", id=\"p_title\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","print(\"标题:\", title)\n","\n","# ========= 提取日期 =========\n","date_tag = soup.find(\"i\", id=\"p_publishtime\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","print(\"日期:\", date)\n","\n","# ========= 提取作者 =========\n","author_tag = soup.find(\"div\", id=\"p_editor\")\n","author = author_tag.get_text(strip=True).replace(\"(责编：\", \"\").replace(\")\", \"\") if author_tag else \"\"\n","print(\"作者:\", author)\n","\n","# ========= 提取正文 =========\n","content = \"\"\n","content_div = soup.find(\"div\", id=\"p_content\")\n","if content_div:\n","    paragraphs = content_div.find_all(\"p\")\n","    content = \"\\n\".join(\n","        p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)\n","    )\n","print(\"正文前 60 字:\", content[:60])\n","\n","# ========= 保存到 CSV =========\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"source\": url\n","}]\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"peuple2019_20.csv\", index=False, encoding=\"utf-8-sig\")  # 用 utf-8-sig，Excel 打开不乱码\n","\n","print(\" 抓取完成！已生成 peuple2019.csv \")\n"],"metadata":{"id":"R_nbERdge2Tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# 目标网址\n","url = \"http://world.people.com.cn/n1/2024/0206/c1002-40174467.html\"\n","\n","# 请求网页\n","response = requests.get(url)\n","response.encoding = response.apparent_encoding  # 自动检测真实编码\n","html = response.text\n","\n","# 解析 HTML\n","soup = BeautifulSoup(html, \"html.parser\")\n","\n","# === 抓取标题 ===\n","\n","title = \"摩尔多瓦首发龙年生肖邮票\"\n","\n","# === 抓取作者 (责编) ===\n","# 注意：有时会有多个 edit cf，正文底部的才是作者\n","author_divs = soup.find_all(\"div\", class_=\"edit cf\")\n","author = author_divs[-1].get_text(strip=True).replace(\"责编：\", \"\") if author_divs else \"\"\n","\n","# ===  抓取日期和来源 ===\n","date_div = soup.find(\"div\", class_=\"col-1-1 fl\")\n","date_text = \"\"\n","source = \"\"\n","if date_div:\n","    date_text = date_div.get_text(strip=True).split(\"|\")[0].strip()\n","    source_a = date_div.find(\"a\")\n","    source = source_a.get_text(strip=True) if source_a else \"\"\n","\n","# === 抓取正文内容 ===\n","content_div = soup.find(\"div\", class_=\"rm_txt_con cf\")\n","paragraphs = []\n","if content_div:\n","    for p in content_div.find_all(\"p\"):\n","        txt = p.get_text(strip=True)\n","        if txt:\n","            paragraphs.append(txt)\n","content = \"\\n\".join(paragraphs)\n","\n","# === 保存到 CSV ===\n","with open(\"peuple2024_1.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"author\", \"title\", \"date\", \"content\", \"source\"])\n","    writer.writerow([author, title, date_text, content, source])\n","\n","print(\"抓取完成，已保存到 peuple2023_1.csv\")\n","print(f\"标题：{title}\")\n","print(f\"日期：{date_text}\")\n","print(f\"来源：{source}\")\n","print(f\"作者：{author}\")\n"],"metadata":{"id":"blu4RL-hG26l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/peuple_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"peuple2015_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)\n"],"metadata":{"id":"yWZA7pb5ZO3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/peuple_all_csv.zip\")\n"],"metadata":{"id":"KZOybUt9ZSqN","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1755101797433,"user_tz":-120,"elapsed":33,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"a2678f20-2e1c-41a8-a94b-8342a62197f3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_4c7996e2-7cd3-4aac-aeaa-3488b7ea0627\", \"peuple_all_csv.zip\", 44579)"]},"metadata":{}}]},{"cell_type":"markdown","source":["#penbaixinwei\n"],"metadata":{"id":"W_teGjb2QG4B"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 设置 URL\n","url = \"https://www.thepaper.cn/newsDetail_forward_31158236\"\n","\n","# 请求头\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0\"\n","}\n","\n","# 获取网页内容\n","response = requests.get(url, headers=headers)\n","response.encoding = 'utf-8'\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# 提取标题\n","title_tag = soup.find(\"h1\", class_=\"index_title__B8mhI\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 提取作者（通常在标题上方或下方的某个 <div> 中，包含“澎湃新闻记者”）\n","author = \"\"\n","for div in soup.find_all(\"div\"):\n","    if div.text.strip().startswith(\"澎湃新闻记者\"):\n","        author = div.text.strip().replace(\"澎湃新闻记者\", \"\").strip()\n","        break\n","\n","# 提取日期（第一个包含时间格式的 <span>）\n","date = \"\"\n","for span in soup.find_all(\"span\"):\n","    if span.text.strip().startswith(\"20\") and \":\" in span.text:\n","        date = span.text.strip()\n","        break\n","\n","# 提取正文内容\n","content_div = soup.find(\"div\", class_=\"index_cententWrap__Jv8jK\")\n","paragraphs = content_div.find_all(\"p\") if content_div else []\n","content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n","\n","# 构造数据\n","data = [[author, title, date, content, url]]\n","df = pd.DataFrame(data, columns=[\"author\", \"title\", \"date\", \"content\", \"source\"])\n","\n","# 保存为 CSV\n","df.to_csv(\"pengpai2020_140.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\" 抓取完成，保存 pengpai2015_1.csv\")\n"],"metadata":{"id":"orYgoXbaPUFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# 目标 URL\n","url = \"https://www.thepaper.cn/newsDetail_forward_1351726\"\n","\n","# 发送请求\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n","}\n","response = requests.get(url, headers=headers)\n","response.encoding = \"utf-8\"  # 澎湃新闻用utf-8编码\n","\n","# 解析 HTML\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# 提取 author\n","author_tag = soup.find(\"div\", string=lambda t: t and \"澎湃新闻记者\" in t)\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","# 提取 title\n","title_tag = soup.find(\"h1\", class_=\"index_title__B8mhI\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 提取 date\n","date_tag = soup.find(\"span\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 提取 content\n","content_div = soup.find(\"div\", class_=\"index_cententWrap__Jv8jK\")\n","if content_div:\n","    # 去掉多余标签，只保留纯文本\n","    content = \"\\n\".join(p.get_text(strip=True) for p in content_div.find_all(\"p\"))\n","else:\n","    content = \"\"\n","\n","# 保存到 CSV\n","csv_filename = \"pengpai2015_5.csv\"\n","with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"author\", \"title\", \"date\", \"content\", \"url\"])\n","    writer.writerow([author, title, date, content, url])\n","\n","print(f\"数据已保存到 {csv_filename}\")\n"],"metadata":{"id":"B2u9CS0nzEh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# 目标网页\n","url = \"https://www.thepaper.cn/newsDetail_forward_8250801\"\n","\n","# 请求网页\n","response = requests.get(url)\n","response.encoding = \"utf-8\"\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# 1️标题\n","title_tag = soup.find(\"h1\", class_=\"index_title__B8mhI\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 2️日期（通常是第一个 span，没有“来源”字样）\n","date_tag = soup.find(\"span\", string=lambda t: t and \"来源\" not in t)\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 3️作者（查找包含“来源”的 span，并去掉前缀“来源：”）\n","author_tag = soup.find(\"span\", string=lambda t: t and \"来源\" in t)\n","author = author_tag.get_text(strip=True).replace(\"来源：\", \"\") if author_tag else \"\"\n","\n","# 4️正文（所有 <p> 里的文字）\n","content_div = soup.find(\"div\", class_=\"index_cententWrap__Jv8jK\")\n","content = \"\"\n","if content_div:\n","    paragraphs = content_div.find_all(\"p\")\n","    content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n","\n","# 5️写入 CSV\n","csv_file = \"pengpai2020_19.csv\"\n","with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n","    writer = csv.DictWriter(f, fieldnames=[\"author\", \"title\", \"date\", \"content\", \"url\"])\n","    writer.writeheader()\n","    writer.writerow({\n","        \"author\": author,\n","        \"title\": title,\n","        \"date\": date,\n","        \"content\": content,\n","        \"url\": url\n","    })\n","\n","print(f\"抓取完成，已保存到 {csv_file}\")\n"],"metadata":{"id":"m4z8XoXeZmLd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 要抓取的网页\n","url = \"https://www.thepaper.cn/newsDetail_forward_29761657\"\n","\n","# 请求网页\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","response.encoding = \"utf-8\"\n","\n","# 用 BeautifulSoup 解析\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# ===== 抓取标题 =====\n","title_tag = soup.find(\"h1\", class_=\"index_title__B8mhI\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# ===== 抓取作者和日期 =====\n","# 获取页面中所有 span 标签（顺序敏感）\n","spans = soup.find_all(\"span\")\n","\n","author = \"\"\n","date = \"\"\n","\n","# 遍历 span 标签，找到包含“>”的作为作者，包含“-”的作为日期\n","for s in spans:\n","    text = s.get_text(strip=True)\n","    if \">\" in text and author == \"\":\n","        author = text\n","    elif \"-\" in text and date == \"\":\n","        date = text\n","\n","# ===== 抓取正文内容 =====\n","content_div = soup.find(\"div\", class_=\"index_cententWrap__Jv8jK\")\n","paragraphs = content_div.find_all(\"p\") if content_div else []\n","content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n","\n","# ===== 生成 CSV =====\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"url\": url\n","}]\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"pengpai2024_21.csv\", index=False, encoding=\"utf-8-sig\")\n","\n","print(\"已完成抓取，已保存为 pengpai2024.csv\")\n"],"metadata":{"id":"f4s4UV_00LaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/pengpai_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"pengpai2024_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)\n"],"metadata":{"id":"tfUT4rDy-q1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/pengpai_all_csv.zip\")\n"],"metadata":{"id":"x_RlYVwS-59I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3 彭博财经\n","www.bloomberg.com\n","\n","\n","#https://www.google.com/advanced_search?q=site:thepaper.cn+2015&client=firefox-b-d&sca_esv=4d1e52ecd0f51357&sxsrf=AE3TifOp3VsAjjic0H6D16Q26gmvMKT8dQ:1753550785198&tbas=0&biw=1232&bih=728&dpr=2"],"metadata":{"id":"R-pY9WZb7ld6"}},{"cell_type":"markdown","source":["#4） 网易新闻\n","#news.163.com\n"],"metadata":{"id":"96gVSCDZ8DeV"}},{"cell_type":"markdown","source":["#5）新浪新闻网\n","# news.sina.com.cn\n"],"metadata":{"id":"88Q_z4Yu8PUQ"}},{"cell_type":"markdown","source":["#6) 新华网、东方网\n","\n","#http://www.xinhuanet.com\n","\n","#http://www.eastday.com\n","\n"],"metadata":{"id":"gBvvMyjN-8N9"}},{"cell_type":"code","source":["import re\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网页URL\n","url = \"http://www.xinhuanet.com/politics/2020-07/26/c_1126287151.htm\"\n","\n","# 抓取并解析\n","resp = requests.get(url, timeout=15)\n","resp.encoding = \"utf-8\"\n","soup = BeautifulSoup(resp.text, \"html.parser\")\n","\n","# 标题\n","title_tag = soup.select_one(\"div.h-title\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 日期\n","date_tag = soup.select_one(\"span.h-time\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 作者（责任编辑）\n","author = \"\"\n","pjc = soup.select_one(\"span.p-jc\")\n","if pjc:\n","    raw = pjc.get_text(\" \", strip=True)\n","    m = re.search(r\"责任编辑[:：]\\s*([^\\s\\]]+)\", raw)\n","    author = m.group(1) if m else raw\n","\n","# 正文\n","content = \"\"\n","content_div = soup.find(\"div\", id=\"p-detail\")\n","if content_div:\n","    # 去掉视频/脚本等无关块\n","    for tag in content_div.find_all([\"script\", \"style\", \"iframe\"]):\n","        tag.decompose()\n","    # 取出段落文字（含加粗段）\n","    parts = []\n","    for tag in content_div.find_all([\"p\", \"strong\"]):\n","        txt = tag.get_text(\" \", strip=True)\n","        if txt:\n","            parts.append(txt)\n","    content = \"\\n\".join(parts).strip()\n","\n","# 组装并保存\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"url\": url\n","}]\n","df = pd.DataFrame(data)\n","\n","outname = \"xinhua2020_22.csv\"\n","df.to_csv(outname, index=False, encoding=\"utf-8-sig\")\n","print(f\"文件已保存为 {outname}\")\n"],"metadata":{"id":"3znV_nNMAUDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","def normalize_space(s: str) -> str:\n","    if not s:\n","        return \"\"\n","    # 去掉全角空格/不间断空格，压缩多余空白\n","    s = s.replace(\"\\u3000\", \" \").replace(\"\\xa0\", \" \")\n","    s = re.sub(r\"[ \\t]+\", \" \", s)\n","    # 去掉标点前的多余空格\n","    s = re.sub(r\"\\s+([，。、“”‘’；：！？,.!?;:])\", r\"\\1\", s)\n","    return s.strip()\n","\n","# 你的目标页\n","url = \"http://www.xinhuanet.com/world/20250527/a7997efbf96b4fe0a5e3e5731b136790/c.html\"\n","\n","# 抓取页面\n","resp = requests.get(url, timeout=20, headers={\n","    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n","})\n","# 新华网多为 UTF-8，这里使用 apparent_encoding 兜底\n","resp.encoding = resp.apparent_encoding or \"utf-8\"\n","soup = BeautifulSoup(resp.text, \"html.parser\")\n","\n","# 标题：<span class=\"title\">\n","title = \"\"\n","title_tag = soup.select_one(\"span.title\")\n","if title_tag:\n","    title = normalize_space(title_tag.get_text(strip=True))\n","\n","# 日期：<div class=\"header-time left\"><span class=\"year\"><em> 2021</em></span><span class=\"day\">02/16</span><span class=\"time\"> 14:32:57</span>\n","year = soup.select_one(\"div.header-time .year em\")\n","day = soup.select_one(\"div.header-time .day\")\n","tm  = soup.select_one(\"div.header-time .time\")\n","date = \"\"\n","if year and day and tm:\n","    y = normalize_space(year.get_text())\n","    d = normalize_space(day.get_text()).replace(\"/\", \"-\")  # 02/16 -> 02-16\n","    t = normalize_space(tm.get_text())\n","    date = f\"{y}-{d} {t}\"\n","\n","# 作者（责任编辑）：通常在 <span class=\"editor\">【责任编辑:王秀】</span>\n","author = \"\"\n","# 直接找所有 editor，拼出文本再用正则提取\n","editors = soup.select(\"span.editor\")\n","if editors:\n","    raw = \" \".join(normalize_space(e.get_text(\" \", strip=True)) for e in editors)\n","    m = re.search(r\"责任编辑[:：]?\\s*([^\\s】\\]]+)\", raw)\n","    if m:\n","        author = m.group(1)\n","# 兜底：全页找“责任编辑”\n","if not author:\n","    node = soup.find(string=re.compile(r\"责任编辑\"))\n","    if node:\n","        txt = normalize_space(str(node))\n","        m2 = re.search(r\"责任编辑[:：]?\\s*([^\\s】\\]]+)\", txt)\n","        if m2:\n","            author = m2.group(1)\n","\n","# 正文：<div id=\"detail\"> 下的 <p>；去掉 articleEdit 等杂项\n","content = \"\"\n","detail = soup.find(\"div\", id=\"detail\")\n","if detail:\n","    # 去掉无关块（纠错、iframe 等）\n","    for bad in detail.find_all([\"script\", \"style\", \"iframe\"]):\n","        bad.decompose()\n","    ae = detail.find(\"div\", id=\"articleEdit\")\n","    if ae:\n","        ae.decompose()\n","\n","    parts = []\n","    for p in detail.find_all(\"p\"):\n","        # 跳过只含图片/空行的段落\n","        if p.find([\"img\", \"iframe\"]):\n","            continue\n","        txt = normalize_space(p.get_text(\" \", strip=True))\n","        if not txt:\n","            continue\n","        # 过滤“纠错”等噪声\n","        if \"纠错\" in txt or \"点击进入专题\" in txt:\n","            continue\n","        parts.append(txt)\n","\n","    # 去重并合并\n","    seen = set()\n","    dedup = []\n","    for t in parts:\n","        if t not in seen:\n","            seen.add(t)\n","            dedup.append(t)\n","    content = \"\\n\".join(dedup).strip()\n","\n","# 组装并保存\n","data = [{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"url\": url\n","}]\n","df = pd.DataFrame(data)\n","\n","outname = \"xinhua2025_26.csv\"\n","df.to_csv(outname, index=False, encoding=\"utf-8-sig\")\n","print(f\"文件已保存为 {outname}\")\n","print(df.T)\n"],"metadata":{"id":"tzy76k6w7PrR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755096051367,"user_tz":-120,"elapsed":2721,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"outputId":"0bd5210b-1e65-4be3-ad5c-9b61ca764c73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["文件已保存为 xinhua2025_26.csv\n","                                                         0\n","author                                                 张艳芳\n","title                    全球连线｜2025中德（欧）隐形冠军论坛开幕 共享经济技术合作机遇\n","date                                   2025-05-27 14:13:43\n","content  5月26日，北京中德产业合作发展论坛——2025中德（欧）隐形冠军论坛在北京顺义中德国际会议...\n","url      http://www.xinhuanet.com/world/20250527/a7997e...\n"]}]},{"cell_type":"code","source":["#另一种简单的字\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网页URL\n","url = \"http://www.xinhuanet.com/world/2021-03/24/c_1127250212.htm\"\n","\n","# 发送HTTP请求获取网页内容\n","response = requests.get(url)\n","response.encoding = 'utf-8'  # 确保中文编码正确\n","\n","# 使用BeautifulSoup解析网页内容\n","soup = BeautifulSoup(response.text, 'html.parser')\n","\n","# 提取标题\n","title = soup.find('h1', id='title').get_text(strip=True)\n","\n","# 提取日期\n","date = soup.find('span', id='pubtime').get_text(strip=True)\n","\n","# 提取内容\n","content_div = soup.find('div', id='content')\n","content = \"\\n\".join([p.get_text(strip=True) for p in content_div.find_all('p')])\n","\n","# 提取作者\n","author_div = soup.find('div', class_='share')\n","author = author_div.get_text(strip=True).replace(\"[责任编辑：\", \"\").replace(\"]\", \"\") if author_div else \"\"\n","\n","# URL\n","article_url = url\n","\n","# 将数据存储为字典\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content],\n","    \"url\": [article_url]\n","}\n","\n","# 将字典转换为DataFrame\n","df = pd.DataFrame(data)\n","\n","# 保存为CSV文件\n","df.to_csv('xinhua2020_1.csv', index=False, encoding='utf-8-sig')\n","\n","print(\"文件已保存为 xinhua2015_1.csv\")\n"],"metadata":{"id":"JKDTiv52Htoh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#code 3\n","\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","import pandas as pd\n","\n","# 目标网页URL（首篇）\n","url = \"http://www.xinhuanet.com/world/2021-03/24/c_1127250212.htm\"\n","\n","def fetch(url):\n","    r = requests.get(url, timeout=15)\n","    r.encoding = 'utf-8'  # 新华网老文基本是 utf-8\n","    return BeautifulSoup(r.text, 'html.parser')\n","\n","soup = fetch(url)\n","\n","# 标题\n","title = soup.find('h1', id='title').get_text(strip=True)\n","\n","# 日期\n","date = soup.find('span', class_='time').get_text(strip=True)\n","\n","# 作者（责任编辑）\n","editor = soup.find('span', class_='editor')\n","author = \"\"\n","if editor:\n","    author = re.sub(r\"[\\[\\]：:\\s]*\", \"\", editor.get_text())\n","    author = author.replace(\"责任编辑\", \"\")  # 只保留名字\n","\n","# --- 正文（含分页） ---\n","def collect_content(soup, base_url):\n","    # 本页正文\n","    article = soup.find('div', class_='article')\n","    parts = []\n","    if article:\n","        # 把 p 和 strong 的文字都取出来\n","        for tag in article.find_all(['p', 'strong']):\n","            txt = tag.get_text(\" \", strip=True)\n","            if txt:\n","                parts.append(txt)\n","\n","        # 找到分页的链接（如 _2.htm），并抓取\n","        page_links = set()\n","        for a in article.find_all('a', href=True):\n","            href = a['href']\n","            if re.search(r\"_\\d+\\.htm$\", href):  # 只要分页页\n","                full = urljoin(base_url if base_url.startswith('http') else \"http:\", href)\n","                page_links.add(full)\n","\n","        # 按序抓取其他页\n","        for link in sorted(page_links):\n","            sub = fetch(link)\n","            sub_article = sub.find('div', class_='article')\n","            if sub_article:\n","                for tag in sub_article.find_all(['p', 'strong']):\n","                    txt = tag.get_text(\" \", strip=True)\n","                    if txt:\n","                        parts.append(txt)\n","\n","    # 合并成一段文本\n","    content = \"\\n\".join(parts)\n","    # 轻度清洗：去掉多余空白\n","    content = re.sub(r\"\\s+\\n\", \"\\n\", content).strip()\n","    return content\n","\n","content = collect_content(soup, url)\n","\n","# URL\n","article_url = url\n","\n","# 组装DataFrame\n","df = pd.DataFrame([{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content,\n","    \"url\": article_url\n","}])\n","\n","# 保存CSV\n","outname = 'xinhua2020_1.csv'\n","df.to_csv(outname, index=False, encoding='utf-8-sig')\n","print(f\"文件已保存为 {outname}\")\n","\n"],"metadata":{"id":"VjmqYnqel_-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#code 4 : nous ne parvenons pas\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网页URL\n","url = \"http://www.xinhuanet.com/2021-02/16/c_1127105025.htm\"\n","\n","# 发送HTTP请求获取网页内容\n","response = requests.get(url)\n","response.encoding = 'utf-8'  # 防止中文乱码\n","\n","# 使用BeautifulSoup解析网页内容\n","soup = BeautifulSoup(response.text, 'html.parser')\n","\n","# 提取标题\n","title = soup.find('h1', id='title').get_text(strip=True) if soup.find('h1', id='title') else ''\n","\n","# 提取日期\n","date = soup.find('span', class_='time').get_text(strip=True) if soup.find('span', class_='time') else ''\n","\n","# 提取作者\n","author_tag = soup.find('span', class_='editor')\n","author = author_tag.get_text(strip=True).replace('[责任编辑:', '').replace(']', '') if author_tag else ''\n","\n","# 提取正文内容\n","content_div = soup.find('div', class_='article')\n","content = \"\"\n","if content_div:\n","    paragraphs = content_div.find_all('p')\n","    content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n","\n","# URL\n","article_url = url\n","\n","# 将数据存储为字典\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content],\n","    \"url\": [article_url]\n","}\n","\n","# 将字典转换为DataFrame\n","df = pd.DataFrame(data)\n","\n","# 保存为CSV文件\n","df.to_csv('xinhua2020_9.csv', index=False, encoding='utf-8-sig')\n","\n","print(\"文件已保存为 xinhua_2017.csv\")\n"],"metadata":{"id":"-14nperdnZDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/xinhua_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"xinhua2025_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)\n"],"metadata":{"id":"zVQaeR1ZJhaB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/xinhua_all_csv.zip\")\n"],"metadata":{"id":"H-QXQB2xJjhE","executionInfo":{"status":"ok","timestamp":1755096065668,"user_tz":-120,"elapsed":18,"user":{"displayName":"Lian CHEN","userId":"12677332526305816693"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"a418fb62-f93f-4324-e926-416899acb17e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_72dfbd73-b9a7-42d9-9585-f34303c79483\", \"xinhua_all_csv.zip\", 76782)"]},"metadata":{}}]},{"cell_type":"markdown","source":["#3） 南方周末\n","#https://www.google.com/advanced_search?q=site:thepaper.cn+2015&client=firefox-b-d&sca_esv=4d1e52ecd0f51357&sxsrf=AE3TifOp3VsAjjic0H6D16Q26gmvMKT8dQ:1753550785198&tbas=0&biw=1232&bih=728&dpr=2\n","\n","# https://www.infzm.com/"],"metadata":{"id":"tYm23li-XGjk"}},{"cell_type":"code","source":[],"metadata":{"id":"YSXydYgqNMIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/nanfang_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"nanfang_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)\n"],"metadata":{"id":"TO-IGn3FNKWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/pengpai_all_csv.zip\")\n"],"metadata":{"id":"bivpD6v1NLhJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3) ZHIHU"],"metadata":{"id":"YyyWCVXfE_Tv"}},{"cell_type":"code","source":["! pip install selenium beautifulsoup4 pandas\n"],"metadata":{"id":"OO8zOG6mNMJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#zhihu  话题 2015\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# 设置关键词和搜索地址\n","search_keyword = \"2015\"\n","search_url = f\"https://www.zhihu.com/search?q={search_keyword}&type=content\"\n","\n","# 配置浏览器\n","options = Options()\n","options.add_argument(\"--headless\")\n","options.add_argument(\"--no-sandbox\")\n","options.add_argument(\"--disable-dev-shm-usage\")\n","driver = webdriver.Chrome(options=options)\n","driver.get(search_url)\n","time.sleep(5)\n","\n","# 模拟滚动加载更多话题卡片\n","for _ in range(10):\n","    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","    time.sleep(2)\n","\n","# 解析页面\n","soup = BeautifulSoup(driver.page_source, \"html.parser\")\n","topic_cards = soup.find_all(\"a\", class_=\"TopicLink\")\n","\n","data = []\n","\n","for topic in topic_cards:\n","    try:\n","        topic_name = topic.get_text(strip=True)\n","        href = topic.get(\"href\")\n","        full_url = \"https://www.zhihu.com\" + href if href.startswith(\"/\") else href\n","\n","        # 向上寻找卡片父元素，提取简介和浏览数\n","        parent_div = topic.find_parent(\"div\", class_=\"ContentItem-head\")\n","        description = \"\"\n","        views = \"\"\n","        discussions = \"\"\n","\n","        if parent_div:\n","            meta_div = parent_div.find_next(\"div\", class_=\"ContentItem-meta\")\n","            if meta_div:\n","                desc_div = meta_div.find(\"div\", class_=\"ztext\")\n","                status_links = meta_div.find_all(\"a\", class_=\"Search-statusLink\")\n","                if desc_div:\n","                    description = desc_div.get_text(strip=True)\n","                if status_links and len(status_links) >= 2:\n","                    views = status_links[0].get_text(strip=True)\n","                    discussions = status_links[1].get_text(strip=True)\n","\n","        data.append([topic_name, description, views, discussions, full_url])\n","        print(f\"抓取话题：{topic_name}\")\n","\n","    except Exception as e:\n","        print(\"忽略话题：\", e)\n","        continue\n","\n","driver.quit()\n","\n","# 保存为 CSV\n","df = pd.DataFrame(data, columns=[\"topic\", \"description\", \"views\", \"discussions\", \"url\"])\n","df.to_csv(\"zhihu_topics_2015.csv\", index=False, encoding=\"utf-8-sig\")\n","print(f\"共保存 {len(data)} 个话题到 zhihu_topics_2015.csv\")\n","\n"],"metadata":{"id":"kNxXW-NdLeyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#每个页面的话题\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# 设置关键词和搜索地址\n","search_keyword = \"2015\"\n","search_url = f\"https://www.zhihu.com/search?q={search_keyword}&type=content\"\n","\n","# 配置 Selenium 无头浏览器\n","options = Options()\n","options.add_argument(\"--headless\")\n","options.add_argument(\"--no-sandbox\")\n","options.add_argument(\"--disable-dev-shm-usage\")\n","options.add_argument(\"start-maximized\")\n","options.add_argument(\"user-agent=Mozilla/5.0\")\n","\n","driver = webdriver.Chrome(options=options)\n","driver.get(search_url)\n","time.sleep(5)\n","\n","# 滚动加载更多结果\n","for _ in range(15):\n","    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","    time.sleep(2)\n","\n","# 解析页面\n","soup = BeautifulSoup(driver.page_source, \"html.parser\")\n","topic_cards = soup.find_all(\"a\", class_=\"TopicLink\")\n","\n","data = []\n","\n","for topic in topic_cards:\n","    try:\n","        # 标题\n","        topic_name = topic.get_text(strip=True)\n","\n","        # 链接\n","        href = topic.get(\"href\")\n","        full_url = \"https://www.zhihu.com\" + href if href.startswith(\"/\") else href\n","\n","        # 找简介和数据\n","        parent_div = topic.find_parent(\"div\", class_=\"ContentItem-head\")\n","        description = \"\"\n","        views = \"\"\n","        discussions = \"\"\n","\n","        if parent_div:\n","            meta_div = parent_div.find_next(\"div\", class_=\"ContentItem-meta\")\n","            if meta_div:\n","                desc_div = meta_div.find(\"div\", class_=\"ztext\")\n","                status_links = meta_div.find_all(\"a\", class_=\"Search-statusLink\")\n","                if desc_div:\n","                    description = desc_div.get_text(strip=True)\n","                if status_links and len(status_links) >= 2:\n","                    views = status_links[0].get_text(strip=True)\n","                    discussions = status_links[1].get_text(strip=True)\n","\n","        data.append([topic_name, description, views, discussions, full_url])\n","        print(f\"✔ 抓取话题：{topic_name}\")\n","\n","    except Exception as e:\n","        print(\" 忽略话题：\", e)\n","        continue\n","\n","driver.quit()\n","\n","# 保存为 CSV\n","df = pd.DataFrame(data, columns=[\"topic\", \"description\", \"views\", \"discussions\", \"url\"])\n","df.to_csv(\"zhihu_topics_20151.csv\", index=False, encoding=\"utf-8-sig\")\n","print(f\"抓取完成，共保存 {len(data)} 个话题到 zhihu_topics_2015.csv\")\n"],"metadata":{"id":"7mztScv0OQSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#zhihu 综合 2015\n","\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","from datetime import datetime\n","\n","# 设置关键词和搜索类型为综合内容\n","search_keyword = \"2015\"\n","search_url = f\"https://www.zhihu.com/search?q={search_keyword}&type=content\"\n","\n","# 配置 Selenium 浏览器\n","options = Options()\n","options.add_argument(\"--headless\")\n","options.add_argument(\"--no-sandbox\")\n","options.add_argument(\"--disable-dev-shm-usage\")\n","driver = webdriver.Chrome(options=options)\n","driver.get(search_url)\n","time.sleep(5)\n","\n","# 模拟下滑加载更多内容\n","for _ in range(15):\n","    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","    time.sleep(2)\n","\n","# 解析页面\n","soup = BeautifulSoup(driver.page_source, \"html.parser\")\n","cards = soup.find_all(\"div\", class_=\"SearchResult-Card\")\n","\n","data = []\n","\n","for card in cards:\n","    try:\n","        # 链接\n","        link_tag = card.find(\"a\", href=True)\n","        if not link_tag:\n","            continue\n","        href = link_tag[\"href\"]\n","        url = \"https://www.zhihu.com\" + href if href.startswith(\"/question\") else href\n","\n","        # 标题\n","        title = link_tag.get_text(strip=True)\n","\n","        # 摘要\n","        summary_tag = card.find(\"div\", class_=\"RichContent-inner\")\n","        summary = summary_tag.get_text(strip=True) if summary_tag else \"\"\n","\n","        # 作者\n","        author_tag = card.find(\"div\", class_=\"AuthorInfo-head\")\n","        author = author_tag.get_text(strip=True) if author_tag else \"匿名\"\n","\n","        # 时间（尝试提取）\n","        time_tag = card.find(\"time\")\n","        date_str = time_tag[\"datetime\"][:10] if time_tag and \"datetime\" in time_tag.attrs else \"\"\n","        try:\n","            date = datetime.strptime(date_str, \"%Y-%m-%d\") if date_str else None\n","        except:\n","            date = None\n","\n","        # 若未提取到时间，但关键词中明确包含“2015”，也可保留\n","        if date is None and \"2015\" in summary:\n","            date = datetime(2015, 1, 1)\n","\n","        if date and datetime(2015, 1, 1) <= date <= datetime(2025, 12, 31):\n","            data.append([title, author, date.strftime(\"%Y-%m-%d\"), summary, url])\n","            print(\"✔ 抓取：\", title)\n","\n","    except Exception as e:\n","        print(\"忽略内容：\", e)\n","        continue\n","\n","driver.quit()\n","\n","# 保存为 CSV 文件\n","df = pd.DataFrame(data, columns=[\"title\", \"author\", \"date\", \"summary\", \"link\"])\n","df.to_csv(\"zhihu_mixed_2015_2025.csv\", index=False, encoding=\"utf-8-sig\")\n","print(f\"完成，共保存 {len(data)} 条内容到 zhihu_mixed_2015_2025.csv\")\n"],"metadata":{"id":"0wIP3xVVMccB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"exMTRTmuXCAR"}},{"cell_type":"markdown","source":["#彭博新闻"],"metadata":{"id":"QZNuvSTbTJzV"}},{"cell_type":"markdown","source":["#4) Weibo"],"metadata":{"id":"DYC3D_T0OjGw"}},{"cell_type":"code","source":[],"metadata":{"id":"AP4V3S_vPJQH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#5)Douban"],"metadata":{"id":"KiyjFr_BPJxA"}},{"cell_type":"code","source":[],"metadata":{"id":"ouWCMBvTPLTg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#6)blibli - vidéo"],"metadata":{"id":"YfrTzI6OPMQI"}},{"cell_type":"code","source":[],"metadata":{"id":"XQHvgxxQPOfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5n_e1FkbP66v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"GA5ibPfQTHoJ"}},{"cell_type":"markdown","source":["#7baidu 百度新闻 ： https://www.baidu.com/s?tn=news&word=2015&pn=100\n","\n","\n","\n","#查询： https://www.google.com/advanced_search?q=site:thepaper.cn+2015&client=firefox-b-d&sca_esv=4d1e52ecd0f51357&sxsrf=AE3TifOp3VsAjjic0H6D16Q26gmvMKT8dQ:1753550785198&tbas=0&biw=1232&bih=728&dpr=2\n"],"metadata":{"id":"yQfNst62wozZ"}},{"cell_type":"markdown","source":["# **Corpus en vietnamien**"],"metadata":{"id":"s894kKgpVOz0"}},{"cell_type":"markdown","source":["#1)https://kenh14.vn\n","\n","\n"],"metadata":{"id":"KAeS7LtzVpl0"}},{"cell_type":"code","source":["! pip install requests beautifulsoup4 pandas\n"],"metadata":{"id":"Wz8kum1kY5Zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# 目标网址\n","url = 'https://kenh14.vn/chum-anh-can-canh-rapper-binh-gold-thoi-diem-bi-csgt-truy-bat-tren-cao-toc-215250724113851073.chn'\n","\n","headers = {\n","    'User-Agent': 'Mozilla/5.0'\n","}\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","# 1. 标题\n","title = soup.find('h1', class_='kbwc-title')\n","title_text = title.get_text(strip=True) if title else 'N/A'\n","\n","# 2. 日期\n","date = soup.find('span', class_='kbwcm-time')\n","date_text = date.get_text(strip=True) if date else 'N/A'\n","\n","# 3. 作者\n","author = soup.find('span', class_='kbwcm-author')\n","author_text = author.get_text(strip=True).rstrip(',') if author else 'N/A'\n","\n","# 4. 正文内容（只取 <div class=\"detail-content\"> 下所有段落文字）\n","content_div = soup.find('div', class_='detail-content')\n","if content_div:\n","    paragraphs = content_div.find_all('p')\n","    content_text = '\\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n","else:\n","    content_text = 'N/A'\n","\n","# 5. 来源\n","source_text = url  # 原网页链接\n","\n","# 写入 CSV\n","with open('kenh2025_article1.csv', 'w', encoding='utf-8-sig', newline='') as f:\n","    writer = csv.DictWriter(f, fieldnames=['author', 'title', 'date', 'content', 'source'])\n","    writer.writeheader()\n","    writer.writerow({\n","        'author': author_text,\n","        'title': title_text,\n","        'date': date_text,\n","        'content': content_text,\n","        'source': source_text\n","    })\n","\n","print(\" 已保存为 kenh14_article.csv\")\n"],"metadata":{"id":"LH1dlMXRWluq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# -------------------------------------------------------\n","#  Extraction pour l'article que tu as indiqué\n","# -------------------------------------------------------\n","\n","url = \"https://kenh14.vn/hoc-duong/ky-thi-thpt-quoc-gia-2015-lich-thi-cua-dh-quoc-gia-ha-noi-20150314080244768.chn\"\n","headers = {'User-Agent': 'Mozilla/5.0'}\n","\n","response = requests.get(url, headers=headers)\n","response.encoding = 'utf-8'\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# 1. Auteur\n","author_tag = soup.find('span', class_='kbwcm-author')\n","author_text = author_tag.get_text(strip=True).rstrip(',') if author_tag else ''\n","\n","# 2. Titre\n","title_tag = soup.find('h1', class_='kbwc-title')\n","title_text = title_tag.get_text(strip=True) if title_tag else ''\n","\n","# 3. Date\n","date_tag = soup.find('span', class_='kbwcm-time')\n","date_text = date_tag.get_text(strip=True) if date_tag else ''\n","\n","# 4. Contenu principal\n","# On cible <div class=\"klw‑new‑content\"> puis on récupère\n","# le chapeau sapo (optionnel) + tous les paragraphes dans <div class=\"knc-content\">\n","content_list = []\n","sapo = soup.find('h2', class_='knc-sapo')\n","if sapo:\n","    sapo_text = sapo.get_text(strip=True)\n","    if sapo_text:\n","        content_list.append(sapo_text)\n","\n","main_div = soup.find('div', class_='knc-content')\n","if main_div:\n","    for p in main_div.find_all('p'):\n","        text = p.get_text(strip=True)\n","        if text:\n","            content_list.append(text)\n","\n","content_text = \"\\n\\n\".join(content_list)\n","\n","# 5. URL de la page\n","page_url = url\n","\n","# 6. Ecrire dans un CSV\n","with open('kenh2015_4.csv', 'w', encoding='utf-8-sig', newline='') as f:\n","    writer = csv.DictWriter(f, fieldnames=['author', 'title', 'date', 'content', 'url'])\n","    writer.writeheader()\n","    writer.writerow({\n","        'author': author_text,\n","        'title': title_text,\n","        'date': date_text,\n","        'content': content_text,\n","        'url': page_url\n","    })\n","\n","print(\" Fichier kenh2015_.csv créé avec succès\")\n"],"metadata":{"id":"biNEMk63Mihw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2016\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# -------------------------------------------------------\n","#  Extraction pour l'article que tu as indiqué\n","# -------------------------------------------------------\n","\n","url = \"https://kenh14.vn/top-3-hoa-hau-viet-nam-2022-nguoi-thang-giai-quoc-te-nguoi-hoc-len-thac-si-215241122144704936.chn\"\n","headers = {'User-Agent': 'Mozilla/5.0'}\n","\n","response = requests.get(url, headers=headers)\n","response.encoding = 'utf-8'\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# 1. Auteur\n","author_tag = soup.find('span', class_='kbwcm-author')\n","author_text = author_tag.get_text(strip=True).rstrip(',') if author_tag else ''\n","\n","# 2. Titre\n","title_tag = soup.find('h1', class_='kbwc-title')\n","title_text = title_tag.get_text(strip=True) if title_tag else ''\n","\n","# 3. Date\n","date_tag = soup.find('span', class_='kbwcm-time')\n","date_text = date_tag.get_text(strip=True) if date_tag else ''\n","\n","# 4. Contenu principal\n","# On cible <div class=\"klw‑new‑content\"> puis on récupère\n","# le chapeau sapo (optionnel) + tous les paragraphes dans <div class=\"knc-content\">\n","content_list = []\n","sapo = soup.find('h2', class_='knc-sapo')\n","if sapo:\n","    sapo_text = sapo.get_text(strip=True)\n","    if sapo_text:\n","        content_list.append(sapo_text)\n","\n","main_div = soup.find('div', class_='knc-content')\n","if main_div:\n","    for p in main_div.find_all('p'):\n","        text = p.get_text(strip=True)\n","        if text:\n","            content_list.append(text)\n","\n","content_text = \"\\n\\n\".join(content_list)\n","\n","# 5. URL de la page\n","page_url = url\n","\n","# 6. Ecrire dans un CSV\n","with open('kenh2024_22.csv', 'w', encoding='utf-8-sig', newline='') as f:\n","    writer = csv.DictWriter(f, fieldnames=['author', 'title', 'date', 'content', 'url'])\n","    writer.writeheader()\n","    writer.writerow({\n","        'author': author_text,\n","        'title': title_text,\n","        'date': date_text,\n","        'content': content_text,\n","        'url': page_url\n","    })\n","\n","print(\" Fichier kenh2015_.csv créé avec succès\")\n"],"metadata":{"id":"nDG6Iog8NDTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/kenh_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"kenh2024_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)"],"metadata":{"id":"c7hcNeMNn-W2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/kenh_all_csv.zip\")\n"],"metadata":{"id":"EVD5gViMn9Hx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2) https://cafebiz.vn\n"],"metadata":{"id":"0tv0EN_TWVOm"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网页\n","url = \"https://cafebiz.vn/quoc-hoi-se-bau-chu-tich-nuoc-vao-thang-10-2024-17624082621173561.chn\"\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","\n","# 请求网页内容\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# 1. 标题\n","title_tag = soup.find(\"h1\", class_=\"title\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 2. 日期\n","date_tag = soup.find(\"span\", class_=\"time\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 3. 作者（优先尝试作者栏，再尝试来源栏）\n","author = \"\"\n","author_byline = soup.find(\"strong\", class_=\"detail-author\")\n","if author_byline and author_byline.get_text(strip=True):\n","    author = author_byline.get_text(strip=True).replace(\"Theo\", \"\").strip()\n","else:\n","    author_tag = soup.find(\"span\", class_=\"link-source-text-name\")\n","    if author_tag:\n","        author = author_tag.get_text(strip=True)\n","\n","# 4. 正文内容（只提取<p>和<blockquote>中的纯文本）\n","content_div = soup.find(\"div\", class_=\"detail-content\")\n","content = \"\"\n","if content_div:\n","    for tag in content_div.find_all([\"p\", \"blockquote\"]):\n","        text = tag.get_text(strip=True)\n","        if text:\n","            content += text + \"\\n\"\n","\n","# 5. 来源（当前URL）\n","source = url\n","\n","# 6. 构建DataFrame并保存为CSV\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content.strip()],\n","    \"source\": [source]\n","}\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"cafebiz2024_25.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"抓取成功，已保存为 cafebiz2021_1.csv\")\n"],"metadata":{"id":"eeXLUn-HYVkT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/cafebiz_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"cafebiz2024_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)"],"metadata":{"id":"8bFZwjw49Rys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/cafebiz_all_csv.zip\")\n"],"metadata":{"id":"vQVmj6Is9U9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3) https://genk.vn\n","\n","\n","#查询https://www.google.com/advanced_search?q=site:thepaper.cn+2015&client=firefox-b-d&sca_esv=4d1e52ecd0f51357&sxsrf=AE3TifOp3VsAjjic0H6D16Q26gmvMKT8dQ:1753550785198&tbas=0&biw=1232&bih=728&dpr=2\n"],"metadata":{"id":"dRFAAueVWXMU"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","url = \"https://genk.vn/ro-ri-ve-khung-long-rtx-5090-32gb-vram-gddr7-cuc-khung-hieu-nang-vuot-troi-nhung-ngon-dien-kinh-hoang-20250107002116059.chn\"\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# 标题\n","title_tag = soup.find(\"h1\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 日期\n","date_tag = soup.find(\"span\", class_=\"kbwcm-time\")\n","date = date_tag.get(\"title\", \"\").strip() if date_tag else \"\"\n","\n","# 作者\n","author_tag = soup.find(\"span\", class_=\"kbwcm-author\")\n","author = author_tag.get_text(strip=True).replace(\",\", \"\").strip() if author_tag else \"\"\n","\n","# 正文内容\n","content = \"\"\n","# 第一段介绍：<h2 class=\"knc-sapo\">\n","sapo = soup.find(\"h2\", class_=\"knc-sapo\")\n","if sapo:\n","    content += sapo.get_text(strip=True) + \"\\n\"\n","\n","# 正文段落 <div id=\"ContentDetail\">\n","content_div = soup.find(\"div\", id=\"ContentDetail\")\n","if content_div:\n","    for tag in content_div.find_all(\"p\"):\n","        text = tag.get_text(strip=True)\n","        if text:\n","            content += text + \"\\n\"\n","\n","# 来源\n","source = url\n","\n","# 构建 DataFrame\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content.strip()],\n","    \"source\": [source]\n","}\n","\n","# 导出为 CSV\n","df = pd.DataFrame(data)\n","df.to_csv(\"genk2025_23.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"抓取成功，已保存为 genk_article.csv\")\n"],"metadata":{"id":"yfRT1RJtaZAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/genk_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"genk2025_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)"],"metadata":{"id":"M_zMuENDpE0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/genk_all_csv.zip\")\n"],"metadata":{"id":"UspHqsREpHOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#❌4) https://baomoi.com\n","\n","\n","\n","#查询https://www.google.com/advanced_search?q=site:thepaper.cn+2015&client=firefox-b-d&sca_esv=4d1e52ecd0f51357&sxsrf=AE3TifOp3VsAjjic0H6D16Q26gmvMKT8dQ:1753550785198&tbas=0&biw=1232&bih=728&dpr=2\n","\n"],"metadata":{"id":"P4Ley8ZXWaWY"}},{"cell_type":"code","source":["#2015\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 要抓取的网址\n","url = \"https://vtcnews.vn/ky-thi-thpt-quoc-gia-2015-cham-bai-thi-trac-nghiem-the-nao-ar189669.html\"\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# 标题（注意是 <h2 class=\"text-title\">）\n","title_tag = soup.find(\"h2\", class_=\"text-title\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 作者（注意是 <strong> 标签内，末尾可能重复一次）\n","author_tag = soup.find(\"strong\")\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","# 日期（<span class=\"time\">）\n","date_tag = soup.find(\"span\", class_=\"time\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 正文内容（在 <div class=\"detail-content afcbc-body\" data-role=\"content\">）\n","content = \"\"\n","content_div = soup.find(\"div\", class_=\"detail-content\")\n","if content_div:\n","    paragraphs = content_div.find_all(\"p\", class_=\"body-text\")\n","    for p in paragraphs:\n","        text = p.get_text(strip=True)\n","        if text:\n","            content += text + \"\\n\"\n","\n","# 构建 DataFrame\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content.strip()],\n","    \"source\": [url]\n","}\n","\n","# 导出为 CSV\n","df = pd.DataFrame(data)\n","df.to_csv(\"baomoi2015_2.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"抓取成功，已保存为 nguoiduatin_article.csv\")\n"],"metadata":{"id":"95ievOW956aG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#VTCNews\n","\n"],"metadata":{"id":"-RWnVj3A622H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2025\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","url = \"https://www.nguoiduatin.vn/dap-an-de-thi-mon-tieng-anh-vao-lop-10-nam-2015-tai-ha-noi-204193589.htm\"\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# 标题\n","title_tag = soup.find(\"h1\", class_=\"detail-title\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 作者\n","author_tag = soup.find(\"p\", class_=\"name\")\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","# 日期\n","date_tag = soup.find(\"div\", attrs={\"data-role\": \"publishdate\"})\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 正文内容\n","content_div = soup.find(\"div\", class_=\"detail-content\")\n","content = \"\"\n","\n","if content_div:\n","    paragraphs = content_div.find_all(\"p\")\n","    for p in paragraphs:\n","        text = p.get_text(strip=True)\n","        if text:\n","            content += text + \"\\n\"\n","\n","# 构建 DataFrame\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content.strip()],\n","    \"source\": [url]\n","}\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"baomoi2015_2.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"抓取成功，已保存为 nld_article_20250807.csv\")\n"],"metadata":{"id":"CdX74o2v19Vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/genk_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"genk2025_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)"],"metadata":{"id":"8i11S1HP10uJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/genk_all_csv.zip\")\n"],"metadata":{"id":"bF0V65v_119e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#❌ 5) https://vnexpress.net\n","\n","#查询https://www.google.com/advanced_search?q=site:thepaper.cn+2015&client=firefox-b-d&sca_esv=4d1e52ecd0f51357&sxsrf=AE3TifOp3VsAjjic0H6D16Q26gmvMKT8dQ:1753550785198&tbas=0&biw=1232&bih=728&dpr=2\n","\n","\n","\n"],"metadata":{"id":"cyz1rfqmWcsy"}},{"cell_type":"code","source":["#anti-scrapping\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 网址\n","url = \"https://vnexpress.net/khoanh-khac-buon-cua-messi-thang-giai-anh-bao-chi-the-gioi-2014-3147342.html\"\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","\n","# 发送请求\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# 抓取标题\n","title_tag = soup.find(\"h1\", class_=\"title-detail\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 抓取作者\n","author_tag = soup.find(\"strong\")\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","# 抓取时间\n","date_tag = soup.find(\"span\", class_=\"date\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 抓取正文内容（包括摘要 + 正文段落）\n","content = \"\"\n","\n","# 摘要段落\n","desc_tag = soup.find(\"p\", class_=\"description\")\n","if desc_tag:\n","    content += desc_tag.get_text(strip=True) + \"\\n\"\n","\n","# 主体段落\n","paragraphs = soup.find_all(\"p\", class_=\"Normal\")\n","for p in paragraphs:\n","    text = p.get_text(strip=True)\n","    if text:\n","        content += text + \"\\n\"\n","\n","# 构建 DataFrame\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content.strip()],\n","    \"source\": [url]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# 保存为 CSV 文件\n","df.to_csv(\"vnexpress_messi_2014.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"抓取成功，已保存为 vnexpress_messi_2014.csv\")\n"],"metadata":{"id":"R5uCOH-I7w-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 安装 Chromium 和 ChromeDriver\n","!apt-get update > /dev/null\n","!apt-get install -y chromium chromium-driver > /dev/null\n","\n","# 安装 Selenium\n","!pip install -q selenium\n"],"metadata":{"id":"I0uk_GeMCt3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.chrome.options import Options\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# 配置 Chrome 选项（Headless + Colab 兼容）\n","chrome_options = Options()\n","chrome_options.add_argument(\"--headless\")\n","chrome_options.add_argument(\"--no-sandbox\")\n","chrome_options.add_argument(\"--disable-dev-shm-usage\")\n","chrome_options.binary_location = \"/usr/bin/chromium\"\n","\n","# 启动浏览器\n","driver = webdriver.Chrome(service=Service(\"/usr/lib/chromium/chromedriver\"), options=chrome_options)\n","\n","# 抓取网页\n","url = \"https://vnexpress.net/khoanh-khac-buon-cua-messi-thang-giai-anh-bao-chi-the-gioi-2014-3147342.html\"\n","driver.get(url)\n","time.sleep(3)  # 等待页面加载完毕\n","\n","# 解析 HTML\n","soup = BeautifulSoup(driver.page_source, \"html.parser\")\n","driver.quit()\n","\n","# 提取信息\n","title_tag = soup.find(\"h1\", class_=\"title-detail\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","author_tag = soup.find(\"strong\")\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","date_tag = soup.find(\"span\", class_=\"date\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 抓取正文内容（包括摘要 + 段落）\n","content = \"\"\n","desc_tag = soup.find(\"p\", class_=\"description\")\n","if desc_tag:\n","    content += desc_tag.get_text(strip=True) + \"\\n\"\n","\n","for p in soup.find_all(\"p\", class_=\"Normal\"):\n","    text = p.get_text(strip=True)\n","    if text:\n","        content += text + \"\\n\"\n","\n","# 构建 DataFrame\n","df = pd.DataFrame([{\n","    \"author\": author,\n","    \"title\": title,\n","    \"date\": date,\n","    \"content\": content.strip(),\n","    \"source\": url\n","}])\n","\n","# 保存为 CSV\n","df.to_csv(\"vnexpress_messi_colab.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"抓取成功！文件名：vnexpress_messi_colab.csv\")\n"],"metadata":{"id":"g_bNAHzSCnRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#下载包\n","\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/vnexpress_all_csv.zip\"\n","with zipfile.ZipFile(zip_path, 'w') as zipf:\n","    for file in os.listdir(\"/content\"):\n","        if file.endswith(\".csv\") and file.startswith(\"vnexpress2015_\"):\n","            zipf.write(os.path.join(\"/content\", file), arcname=file)"],"metadata":{"id":"OjKCsbb814Hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/genk_all_csv.zip\")\n"],"metadata":{"id":"AFdjh0Pm12oy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#6) https://webtretho.com (forum)\n","\n","\n","#查询https://www.google.com/advanced_search?q=site:thepaper.cn+2015&client=firefox-b-d&sca_esv=4d1e52ecd0f51357&sxsrf=AE3TifOp3VsAjjic0H6D16Q26gmvMKT8dQ:1753550785198&tbas=0&biw=1232&bih=728&dpr=2\n","\n","\n","\n"],"metadata":{"id":"bh29fj5uWg5Z"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网址\n","url = \"https://www.webtretho.com/f/tam-su-chuyen-cong-viec-cong-so/4-2015-tap-doan-nam-long-cong-bo-mo-ban-dot-cuoi-3-lo-dep-nhat-c5-c6-va-c7-nhan-nha-nam-2015-2036813\"\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# --------------------------\n","# 提取标题\n","title_tag = soup.find(\"h1\", class_=\"text-wrap tw-font-bold text-primary tw-mt-2.5 tw-mb-1 tw-text-3xl\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 提取作者\n","author_tag = soup.find(\"a\", class_=\"tw-font-bold text-main tw-text-xs\")\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","# 提取日期（注：这里是“几个月前”的形式）\n","date_tag = soup.find(\"div\", class_=\"tw-text-xs text-gray-v2\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 提取正文内容：<div id=\"post-content\">\n","content = \"\"\n","content_div = soup.find(\"div\", id=\"post-content\")\n","if content_div:\n","    for p in content_div.find_all(\"p\", class_=\"editor-text-justify\"):\n","        text = p.get_text(strip=True)\n","        if text:\n","            content += text + \"\\n\"\n","\n","# 构建 dataframe\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content.strip()],\n","    \"source\": [url]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# 保存为 CSV\n","df.to_csv(\"webtretho2015_2.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"抓取成功，已保存为 webtretho_luat_2015.csv\")\n"],"metadata":{"id":"PzQp2va5FOt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 目标网址\n","url = \"https://www.webtretho.com/f/hoc-tap-va-nghien-cuu/chia-se-phan-mem-dong-goi-installshield-2015\"\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# --------------------------\n","# 提取标题\n","title_tag = soup.find(\"h1\", class_=\"text-wrap tw-font-bold text-primary tw-mt-2.5 tw-mb-1 tw-text-3xl\")\n","title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","# 提取作者\n","author_tag = soup.find(\"a\", class_=\"tw-font-bold text-main tw-text-xs\")\n","author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","# 提取日期\n","date_tag = soup.find(\"div\", class_=\"tw-text-xs text-gray-v2\")\n","date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","# 提取正文内容（注意：这段文字不完全在 <p> 中）\n","content = \"\"\n","content_div = soup.find(\"div\", id=\"post-content\")\n","if content_div:\n","    # 提取所有文字节点（不管什么标签）\n","    content = content_div.get_text(separator=\"\\n\", strip=True)\n","\n","# 构建 dataframe\n","data = {\n","    \"author\": [author],\n","    \"title\": [title],\n","    \"date\": [date],\n","    \"content\": [content],\n","    \"source\": [url]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# 保存为 CSV\n","df.to_csv(\"webtretho2015_5.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"抓取成功，已保存为 webtretho2015_2.csv\")\n"],"metadata":{"id":"eJGVDnZ3HHRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Forum\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# 标签页 URL\n","url = \"https://www.webtretho.com/tags/rong-bien\"\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# 初始化数据列表\n","posts = []\n","\n","# 遍历所有帖子容器\n","# 每个帖子是 <div role=\"article\">...</div>\n","for article in soup.find_all(\"div\", attrs={\"role\": \"article\"}):\n","    # 标题\n","    title_tag = article.find(\"h4\")\n","    title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","    # 作者\n","    author_tag = article.find(\"a\", class_=\"tw-font-bold text-main tw-text-xs\")\n","    author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","    # 日期\n","    date_tag = article.find(\"div\", class_=\"tw-text-xs text-gray-blue tw-mt-1.5\")\n","    date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","    # 内容（正文摘要）\n","    content_tag = article.find(\"div\", class_=\"tw-leading-5 tw-text-sm text-black tw-whitespace-normal tw-break-words w-100 max-l-2\")\n","    content = content_tag.get_text(separator=\"\\n\", strip=True) if content_tag else \"\"\n","\n","    # 链接\n","    link_tag = title_tag.find_parent(\"a\") if title_tag else None\n","    href = \"https://www.webtretho.com\" + link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else url\n","\n","    # 添加一行数据\n","    posts.append({\n","        \"title\": title,\n","        \"author\": author,\n","        \"date\": date,\n","        \"content\": content,\n","        \"link\": href\n","    })\n","\n","# 保存为 CSV\n","df = pd.DataFrame(posts)\n","df.to_csv(\"webtretho2015_6.csv\", index=False, encoding=\"utf-8-sig\")\n","print(\"已抓取\", len(posts), \"条帖子，保存为 webtretho_tag_rong_bien.csv\")\n"],"metadata":{"id":"w9JvA1gBJ2bU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get update > /dev/null\n","!apt-get install -y chromium chromium-driver > /dev/null\n","!pip install -q selenium\n"],"metadata":{"id":"311yZjPAKbMc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.chrome.options import Options\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# 配置 headless Chrome\n","chrome_options = Options()\n","chrome_options.add_argument(\"--headless\")\n","chrome_options.add_argument(\"--no-sandbox\")\n","chrome_options.add_argument(\"--disable-dev-shm-usage\")\n","chrome_options.binary_location = \"/usr/bin/chromium\"\n","\n","# 初始化浏览器\n","driver = webdriver.Chrome(service=Service(\"/usr/lib/chromium/chromedriver\"), options=chrome_options)\n","\n","# 打开标签页\n","url = \"https://www.webtretho.com/tags/rong-bien\"\n","driver.get(url)\n","time.sleep(5)  # ⏳ 等待加载完成\n","\n","soup = BeautifulSoup(driver.page_source, \"html.parser\")\n","driver.quit()\n","\n","# 提取帖子内容\n","posts = []\n","\n","for article in soup.find_all(\"div\", attrs={\"role\": \"article\"}):\n","    # 标题\n","    title_tag = article.find(\"h4\")\n","    title = title_tag.get_text(strip=True) if title_tag else \"\"\n","\n","    # 作者\n","    author_tag = article.find(\"a\", class_=\"tw-font-bold text-main tw-text-xs\")\n","    author = author_tag.get_text(strip=True) if author_tag else \"\"\n","\n","    # 日期\n","    date_tag = article.find(\"div\", class_=\"tw-text-xs text-gray-blue tw-mt-1.5\")\n","    date = date_tag.get_text(strip=True) if date_tag else \"\"\n","\n","    # 内容（正文摘要）\n","    content_tag = article.find(\"div\", class_=\"tw-leading-5 tw-text-sm text-black tw-whitespace-normal tw-break-words w-100 max-l-2\")\n","    content = content_tag.get_text(separator=\"\\n\", strip=True) if content_tag else \"\"\n","\n","    # 链接\n","    link_tag = title_tag.find_parent(\"a\") if title_tag else None\n","    href = \"https://www.webtretho.com\" + link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else url\n","\n","    posts.append({\n","        \"title\": title,\n","        \"author\": author,\n","        \"date\": date,\n","        \"content\": content,\n","        \"link\": href\n","    })\n","\n","# 保存为 CSV\n","df = pd.DataFrame(posts)\n","df.to_csv(\"webtretho_tag_rong_bien.csv\", index=False, encoding=\"utf-8-sig\")\n","print(f\"成功抓取 {len(posts)} 条内容，已保存为 webtretho_tag_rong_bien.csv\")\n"],"metadata":{"id":"gnaWNSDEKVcO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"XiJ4vyOCVOxq"}}]}